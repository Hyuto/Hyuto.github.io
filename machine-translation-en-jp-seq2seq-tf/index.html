<!DOCTYPE html><html lang="en"><head><meta charSet="utf-8"/><meta http-equiv="x-ua-compatible" content="ie=edge"/><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"/><style data-href="/styles.738d8045757e8cd34580.css" data-identity="gatsby-global-css">@font-face{font-display:swap;font-family:Montserrat;font-style:normal;font-weight:100;src:local("Montserrat Thin "),local("Montserrat-Thin"),url(/static/montserrat-latin-100-8d7d79679b70dbe27172b6460e7a7910.woff2) format("woff2"),url(/static/montserrat-latin-100-ec38980a9e0119a379e2a9b3dbb1901a.woff) format("woff")}@font-face{font-display:swap;font-family:Montserrat;font-style:italic;font-weight:100;src:local("Montserrat Thin italic"),local("Montserrat-Thinitalic"),url(/static/montserrat-latin-100italic-e279051046ba1286706adc886cf1c96b.woff2) format("woff2"),url(/static/montserrat-latin-100italic-3b325a3173c8207435cd1b76e19bf501.woff) format("woff")}@font-face{font-display:swap;font-family:Montserrat;font-style:normal;font-weight:200;src:local("Montserrat Extra Light "),local("Montserrat-Extra Light"),url(/static/montserrat-latin-200-9d266fbbfa6cab7009bd56003b1eeb67.woff2) format("woff2"),url(/static/montserrat-latin-200-2d8ba08717110d27122e54c34b8a5798.woff) format("woff")}@font-face{font-display:swap;font-family:Montserrat;font-style:italic;font-weight:200;src:local("Montserrat Extra Light italic"),local("Montserrat-Extra Lightitalic"),url(/static/montserrat-latin-200italic-6e5b3756583bb2263eb062eae992735e.woff2) format("woff2"),url(/static/montserrat-latin-200italic-a0d6f343e4b536c582926255367a57da.woff) format("woff")}@font-face{font-display:swap;font-family:Montserrat;font-style:normal;font-weight:300;src:local("Montserrat Light "),local("Montserrat-Light"),url(/static/montserrat-latin-300-00b3e893aab5a8fd632d6342eb72551a.woff2) format("woff2"),url(/static/montserrat-latin-300-ea303695ceab35f17e7d062f30e0173b.woff) format("woff")}@font-face{font-display:swap;font-family:Montserrat;font-style:italic;font-weight:300;src:local("Montserrat Light italic"),local("Montserrat-Lightitalic"),url(/static/montserrat-latin-300italic-56f34ea368f6aedf89583d444bbcb227.woff2) format("woff2"),url(/static/montserrat-latin-300italic-54b0bf2c8c4c12ffafd803be2466a790.woff) format("woff")}@font-face{font-display:swap;font-family:Montserrat;font-style:normal;font-weight:400;src:local("Montserrat Regular "),local("Montserrat-Regular"),url(/static/montserrat-latin-400-b71748ae4f80ec8c014def4c5fa8688b.woff2) format("woff2"),url(/static/montserrat-latin-400-0659a9f4e90db5cf51b50d005bff1e41.woff) format("woff")}@font-face{font-display:swap;font-family:Montserrat;font-style:italic;font-weight:400;src:local("Montserrat Regular italic"),local("Montserrat-Regularitalic"),url(/static/montserrat-latin-400italic-6eed6b4cbb809c6efc7aa7ddad6dbe3e.woff2) format("woff2"),url(/static/montserrat-latin-400italic-7583622cfde30ae49086d18447ab28e7.woff) format("woff")}@font-face{font-display:swap;font-family:Montserrat;font-style:normal;font-weight:500;src:local("Montserrat Medium "),local("Montserrat-Medium"),url(/static/montserrat-latin-500-091b209546e16313fd4f4fc36090c757.woff2) format("woff2"),url(/static/montserrat-latin-500-edd311588712a96bbf435fad264fff62.woff) format("woff")}@font-face{font-display:swap;font-family:Montserrat;font-style:italic;font-weight:500;src:local("Montserrat Medium italic"),local("Montserrat-Mediumitalic"),url(/static/montserrat-latin-500italic-c90ced68b46050061d1a41842d6dfb43.woff2) format("woff2"),url(/static/montserrat-latin-500italic-5146cbfe02b1deea5dffea27a5f2f998.woff) format("woff")}@font-face{font-display:swap;font-family:Montserrat;font-style:normal;font-weight:600;src:local("Montserrat SemiBold "),local("Montserrat-SemiBold"),url(/static/montserrat-latin-600-0480d2f8a71f38db8633b84d8722e0c2.woff2) format("woff2"),url(/static/montserrat-latin-600-b77863a375260a05dd13f86a1cee598f.woff) format("woff")}@font-face{font-display:swap;font-family:Montserrat;font-style:italic;font-weight:600;src:local("Montserrat SemiBold italic"),local("Montserrat-SemiBolditalic"),url(/static/montserrat-latin-600italic-cf46ffb11f3a60d7df0567f8851a1d00.woff2) format("woff2"),url(/static/montserrat-latin-600italic-c4fcfeeb057724724097167e57bd7801.woff) format("woff")}@font-face{font-display:swap;font-family:Montserrat;font-style:normal;font-weight:700;src:local("Montserrat Bold "),local("Montserrat-Bold"),url(/static/montserrat-latin-700-7dbcc8a5ea2289d83f657c25b4be6193.woff2) format("woff2"),url(/static/montserrat-latin-700-99271a835e1cae8c76ef8bba99a8cc4e.woff) format("woff")}@font-face{font-display:swap;font-family:Montserrat;font-style:italic;font-weight:700;src:local("Montserrat Bold italic"),local("Montserrat-Bolditalic"),url(/static/montserrat-latin-700italic-c41ad6bdb4bd504a843d546d0a47958d.woff2) format("woff2"),url(/static/montserrat-latin-700italic-6779372f04095051c62ed36bc1dcc142.woff) format("woff")}@font-face{font-display:swap;font-family:Montserrat;font-style:normal;font-weight:800;src:local("Montserrat ExtraBold "),local("Montserrat-ExtraBold"),url(/static/montserrat-latin-800-db9a3e0ba7eaea32e5f55328ace6cf23.woff2) format("woff2"),url(/static/montserrat-latin-800-4e3c615967a2360f5db87d2f0fd2456f.woff) format("woff")}@font-face{font-display:swap;font-family:Montserrat;font-style:italic;font-weight:800;src:local("Montserrat ExtraBold italic"),local("Montserrat-ExtraBolditalic"),url(/static/montserrat-latin-800italic-bf45bfa14805969eda318973947bc42b.woff2) format("woff2"),url(/static/montserrat-latin-800italic-fe82abb0bcede51bf724254878e0c374.woff) format("woff")}@font-face{font-display:swap;font-family:Montserrat;font-style:normal;font-weight:900;src:local("Montserrat Black "),local("Montserrat-Black"),url(/static/montserrat-latin-900-e66c7edc609e24bacbb705175669d814.woff2) format("woff2"),url(/static/montserrat-latin-900-8211f418baeb8ec880b80ba3c682f957.woff) format("woff")}@font-face{font-display:swap;font-family:Montserrat;font-style:italic;font-weight:900;src:local("Montserrat Black italic"),local("Montserrat-Blackitalic"),url(/static/montserrat-latin-900italic-4454c775e48152c1a72510ceed3603e2.woff2) format("woff2"),url(/static/montserrat-latin-900italic-efcaa0f6a82ee0640b83a0916e6e8d68.woff) format("woff")}@font-face{font-display:swap;font-family:Merriweather;font-style:normal;font-weight:300;src:local("Merriweather Light "),local("Merriweather-Light"),url(/static/merriweather-latin-300-fc117160c69a8ea0851b26dd14748ee4.woff2) format("woff2"),url(/static/merriweather-latin-300-58b18067ebbd21fda77b67e73c241d3b.woff) format("woff")}@font-face{font-display:swap;font-family:Merriweather;font-style:italic;font-weight:300;src:local("Merriweather Light italic"),local("Merriweather-Lightitalic"),url(/static/merriweather-latin-300italic-fe29961474f8dbf77c0aa7b9a629e4bc.woff2) format("woff2"),url(/static/merriweather-latin-300italic-23c3f1f88683618a4fb8d265d33d383a.woff) format("woff")}@font-face{font-display:swap;font-family:Merriweather;font-style:normal;font-weight:400;src:local("Merriweather Regular "),local("Merriweather-Regular"),url(/static/merriweather-latin-400-d9479e8023bef9cbd9bf8d6eabd6bf36.woff2) format("woff2"),url(/static/merriweather-latin-400-040426f99ff6e00b86506452e0d1f10b.woff) format("woff")}@font-face{font-display:swap;font-family:Merriweather;font-style:italic;font-weight:400;src:local("Merriweather Regular italic"),local("Merriweather-Regularitalic"),url(/static/merriweather-latin-400italic-2de7bfeaf08fb03d4315d49947f062f7.woff2) format("woff2"),url(/static/merriweather-latin-400italic-79db67aca65f5285964ab332bd65f451.woff) format("woff")}@font-face{font-display:swap;font-family:Merriweather;font-style:normal;font-weight:700;src:local("Merriweather Bold "),local("Merriweather-Bold"),url(/static/merriweather-latin-700-4b08e01d805fa35d7bf777f1b24314ae.woff2) format("woff2"),url(/static/merriweather-latin-700-22fb8afba4ab1f093b6ef9e28a9b6e92.woff) format("woff")}@font-face{font-display:swap;font-family:Merriweather;font-style:italic;font-weight:700;src:local("Merriweather Bold italic"),local("Merriweather-Bolditalic"),url(/static/merriweather-latin-700italic-cd92541b177652fffb6e3b952f1c33f1.woff2) format("woff2"),url(/static/merriweather-latin-700italic-f87f3d87cea0dd0979bfc8ac9ea90243.woff) format("woff")}@font-face{font-display:swap;font-family:Merriweather;font-style:normal;font-weight:900;src:local("Merriweather Black "),local("Merriweather-Black"),url(/static/merriweather-latin-900-f813fc6a4bee46eda5224ac7ebf1b7be.woff2) format("woff2"),url(/static/merriweather-latin-900-5d4e42cb44410674acd99153d57df032.woff) format("woff")}@font-face{font-display:swap;font-family:Merriweather;font-style:italic;font-weight:900;src:local("Merriweather Black italic"),local("Merriweather-Blackitalic"),url(/static/merriweather-latin-900italic-b7901d85486871c1779c0e93ddd85656.woff2) format("woff2"),url(/static/merriweather-latin-900italic-9647f9fdab98756989a8a5550eb205c3.woff) format("woff")}


/*! normalize.css v8.0.1 | MIT License | github.com/necolas/normalize.css */html{-webkit-text-size-adjust:100%;line-height:1.15}body{margin:0}main{display:block}h1{font-size:2em;margin:.67em 0}hr{box-sizing:content-box;height:0;overflow:visible}pre{font-family:monospace,monospace;font-size:1em}a{background-color:transparent}abbr[title]{border-bottom:none;text-decoration:underline;-webkit-text-decoration:underline dotted;text-decoration:underline dotted}b,strong{font-weight:bolder}code,kbd,samp{font-family:monospace,monospace;font-size:1em}small{font-size:80%}sub,sup{font-size:75%;line-height:0;position:relative;vertical-align:baseline}sub{bottom:-.25em}sup{top:-.5em}img{border-style:none}button,input,optgroup,select,textarea{font-family:inherit;font-size:100%;line-height:1.15;margin:0}button,input{overflow:visible}button,select{text-transform:none}[type=button],[type=reset],[type=submit],button{-webkit-appearance:button}[type=button]::-moz-focus-inner,[type=reset]::-moz-focus-inner,[type=submit]::-moz-focus-inner,button::-moz-focus-inner{border-style:none;padding:0}[type=button]:-moz-focusring,[type=reset]:-moz-focusring,[type=submit]:-moz-focusring,button:-moz-focusring{outline:1px dotted ButtonText}fieldset{padding:.35em .75em .625em}legend{box-sizing:border-box;color:inherit;display:table;max-width:100%;padding:0;white-space:normal}progress{vertical-align:baseline}textarea{overflow:auto}[type=checkbox],[type=radio]{box-sizing:border-box;padding:0}[type=number]::-webkit-inner-spin-button,[type=number]::-webkit-outer-spin-button{height:auto}[type=search]{-webkit-appearance:textfield;outline-offset:-2px}[type=search]::-webkit-search-decoration{-webkit-appearance:none}::-webkit-file-upload-button{-webkit-appearance:button;font:inherit}details{display:block}summary{display:list-item}[hidden]{display:none}:root{--maxWidth-none:"none";--maxWidth-xs:20rem;--maxWidth-sm:24rem;--maxWidth-md:28rem;--maxWidth-lg:32rem;--maxWidth-xl:36rem;--maxWidth-2xl:42rem;--maxWidth-3xl:48rem;--maxWidth-4xl:56rem;--maxWidth-full:"100%";--maxWidth-wrapper:var(--maxWidth-2xl);--spacing-px:"1px";--spacing-0:0;--spacing-1:0.25rem;--spacing-2:0.5rem;--spacing-3:0.75rem;--spacing-4:1rem;--spacing-5:1.25rem;--spacing-6:1.5rem;--spacing-8:2rem;--spacing-10:2.5rem;--spacing-12:3rem;--spacing-16:4rem;--spacing-20:5rem;--spacing-24:6rem;--spacing-32:8rem;--fontFamily-sans:Montserrat,system-ui,-apple-system,BlinkMacSystemFont,"Segoe UI",Roboto,"Helvetica Neue",Arial,"Noto Sans",sans-serif,"Apple Color Emoji","Segoe UI Emoji","Segoe UI Symbol","Noto Color Emoji";--fontFamily-serif:"Merriweather","Georgia",Cambria,"Times New Roman",Times,serif;--font-body:var(--fontFamily-serif);--font-heading:var(--fontFamily-sans);--fontWeight-normal:400;--fontWeight-medium:500;--fontWeight-semibold:600;--fontWeight-bold:700;--fontWeight-extrabold:800;--fontWeight-black:900;--fontSize-root:16px;--lineHeight-none:1;--lineHeight-tight:1.1;--lineHeight-normal:1.5;--lineHeight-relaxed:1.625;--fontSize-0:0.833rem;--fontSize-1:1rem;--fontSize-2:1.2rem;--fontSize-3:1.44rem;--fontSize-4:1.728rem;--fontSize-5:2.074rem;--fontSize-6:2.488rem;--fontSize-7:2.986rem;--color-primary:#005b99;--color-text:#2e353f;--color-text-light:#4f5969;--color-heading:#1a202c;--color-heading-black:#000;--color-accent:#d1dce5}*,:after,:before{box-sizing:border-box}html{-webkit-font-smoothing:antialiased;-moz-osx-font-smoothing:grayscale;font-size:var(--fontSize-root);line-height:var(--lineHeight-normal)}body{color:var(--color-text);font-family:var(--font-body);font-size:var(--fontSize-1)}footer{padding:var(--spacing-6) var(--spacing-0)}hr{background:var(--color-accent);border:0;height:1px}h1,h2,h3,h4,h5,h6{font-family:var(--font-heading);letter-spacing:-.025em;line-height:var(--lineHeight-tight);margin-bottom:var(--spacing-6);margin-top:var(--spacing-12)}h2,h3,h4,h5,h6{color:var(--color-heading);font-weight:var(--fontWeight-bold)}h1{color:var(--color-heading-black);font-size:var(--fontSize-6);font-weight:var(--fontWeight-black)}h2{font-size:var(--fontSize-5)}h3{font-size:var(--fontSize-4)}h4{font-size:var(--fontSize-3)}h5{font-size:var(--fontSize-2)}h6{font-size:var(--fontSize-1)}h1>a,h2>a,h3>a,h4>a,h5>a,h6>a{color:inherit;text-decoration:none}p{--baseline-multiplier:0.179;--x-height-multiplier:0.35;line-height:var(--lineHeight-relaxed);margin:var(--spacing-0) var(--spacing-0) var(--spacing-8) var(--spacing-0)}ol,p,ul{padding:var(--spacing-0)}ol,ul{list-style-image:none;list-style-position:outside;margin-bottom:var(--spacing-8);margin-left:var(--spacing-0);margin-right:var(--spacing-0)}ol li,ul li{padding-left:var(--spacing-0)}li>p,ol li,ul li{margin-bottom:calc(var(--spacing-8)/2)}li :last-child{margin-bottom:var(--spacing-0)}li>ul{margin-left:var(--spacing-8);margin-top:calc(var(--spacing-8)/2)}blockquote{border-left:var(--spacing-1) solid var(--color-primary);color:var(--color-text-light);font-size:var(--fontSize-2);font-style:italic;margin-bottom:var(--spacing-8);margin-left:calc(var(--spacing-6)*-1);margin-right:var(--spacing-8);padding:var(--spacing-0) var(--spacing-0) var(--spacing-0) var(--spacing-6)}blockquote>:last-child{margin-bottom:var(--spacing-0)}blockquote>ol,blockquote>ul{list-style-position:inside}table{border-collapse:collapse;border-spacing:.25rem;margin-bottom:var(--spacing-8);width:100%}table thead tr th{border-bottom:1px solid var(--color-accent)}a{color:var(--color-primary);text-decoration:none}a:focus,a:hover{text-decoration:underline}.global-wrapper{margin:var(--spacing-0) auto;max-width:var(--maxWidth-wrapper);padding:var(--spacing-10) var(--spacing-5)}.global-header{margin-bottom:var(--spacing-12)}.main-heading{font-size:var(--fontSize-7);margin:0}.post-list-item{margin-bottom:var(--spacing-8);margin-top:var(--spacing-8)}.post-list-item p{margin-bottom:var(--spacing-0)}.post-list-item h2{color:var(--color-primary);font-size:var(--fontSize-4);margin-bottom:var(--spacing-2);margin-top:var(--spacing-0)}.post-list-item header{margin-bottom:var(--spacing-4)}.header-link-home{font-family:var(--font-heading);font-size:var(--fontSize-2);font-weight:var(--fontWeight-bold);text-decoration:none}.bio{align-items:center;display:flex;justify-content:center}.bio-avatar,.bio p{margin-bottom:var(--spacing-0)}.bio-avatar{border-radius:100%;margin-right:var(--spacing-4);min-width:50px}.blog-post header h1{margin:var(--spacing-0) var(--spacing-0) var(--spacing-4) var(--spacing-0)}.blog-post header p{font-family:var(--font-heading);font-size:var(--fontSize-2)}.blog-post-nav ul{margin:var(--spacing-0)}.gatsby-highlight{margin-bottom:var(--spacing-8)}@media (max-width:42rem){blockquote{margin-left:var(--spacing-0);padding:var(--spacing-0) var(--spacing-0) var(--spacing-0) var(--spacing-4)}ol,ul{list-style-position:inside}}code[class*=language-],pre[class*=language-]{word-wrap:normal;background:none;color:#000;font-family:Consolas,Monaco,Andale Mono,Ubuntu Mono,monospace;font-size:1em;-webkit-hyphens:none;-ms-hyphens:none;hyphens:none;line-height:1.5;-moz-tab-size:4;-o-tab-size:4;tab-size:4;text-align:left;text-shadow:0 1px #fff;white-space:pre;word-break:normal;word-spacing:normal}code[class*=language-]::selection,code[class*=language-] ::selection,pre[class*=language-]::selection,pre[class*=language-] ::selection{background:#b3d4fc;text-shadow:none}@media print{code[class*=language-],pre[class*=language-]{text-shadow:none}}pre[class*=language-]{margin:.5em 0;overflow:auto;padding:1em}:not(pre)>code[class*=language-],pre[class*=language-]{background:#f5f2f0}:not(pre)>code[class*=language-]{border-radius:.3em;padding:.1em;white-space:normal}.token.cdata,.token.comment,.token.doctype,.token.prolog{color:#708090}.token.punctuation{color:#999}.token.namespace{opacity:.7}.token.boolean,.token.constant,.token.deleted,.token.number,.token.property,.token.symbol,.token.tag{color:#905}.token.attr-name,.token.builtin,.token.char,.token.inserted,.token.selector,.token.string{color:#690}.language-css .token.string,.style .token.string,.token.entity,.token.operator,.token.url{background:hsla(0,0%,100%,.5);color:#9a6e3a}.token.atrule,.token.attr-value,.token.keyword{color:#07a}.token.class-name,.token.function{color:#dd4a68}.token.important,.token.regex,.token.variable{color:#e90}.token.bold,.token.important{font-weight:700}.token.italic{font-style:italic}.token.entity{cursor:help}.about-title{margin:0}.about .header{align-items:center;display:flex;flex-direction:column;height:300px;justify-content:center;margin-bottom:5px;max-width:100%;padding:0}.about .header .profile-pic{border-radius:50%;margin:5px;padding:0}.about .header h3{margin:10px}.about .desc{margin:0}.about .desc p{text-align:justify}.about .social{margin:0 0 20px}</style><meta name="generator" content="Gatsby 3.11.1"/><style>.gatsby-image-wrapper{position:relative;overflow:hidden}.gatsby-image-wrapper img{bottom:0;height:100%;left:0;margin:0;max-width:none;padding:0;position:absolute;right:0;top:0;width:100%;object-fit:cover}.gatsby-image-wrapper [data-main-image]{opacity:0;transform:translateZ(0);transition:opacity .25s linear;will-change:opacity}.gatsby-image-wrapper-constrained{display:inline-block;vertical-align:top}</style><noscript><style>.gatsby-image-wrapper noscript [data-main-image]{opacity:1!important}.gatsby-image-wrapper [data-placeholder-image]{opacity:0!important}</style></noscript><script type="module">const e="undefined"!=typeof HTMLImageElement&&"loading"in HTMLImageElement.prototype;e&&document.body.addEventListener("load",(function(e){if(void 0===e.target.dataset.mainImage)return;if(void 0===e.target.dataset.gatsbyImageSsr)return;const t=e.target;let a=null,n=t;for(;null===a&&n;)void 0!==n.parentNode.dataset.gatsbyImageWrapper&&(a=n.parentNode),n=n.parentNode;const o=a.querySelector("[data-placeholder-image]"),r=new Image;r.src=t.currentSrc,r.decode().catch((()=>{})).then((()=>{t.style.opacity=1,o&&(o.style.opacity=0,o.style.transition="opacity 500ms linear")}))}),!0);</script><link rel="preconnect" href="https://www.google-analytics.com"/><link rel="dns-prefetch" href="https://www.google-analytics.com"/><link rel="alternate" type="application/rss+xml" href="/rss.xml"/><link rel="icon" href="/favicon-32x32.png?v=8239e45d9e9b363c649caaa3e24a0221" type="image/png"/><link rel="manifest" href="/manifest.webmanifest" crossorigin="anonymous"/><meta name="theme-color" content="#663399"/><link rel="apple-touch-icon" sizes="48x48" href="/icons/icon-48x48.png?v=8239e45d9e9b363c649caaa3e24a0221"/><link rel="apple-touch-icon" sizes="72x72" href="/icons/icon-72x72.png?v=8239e45d9e9b363c649caaa3e24a0221"/><link rel="apple-touch-icon" sizes="96x96" href="/icons/icon-96x96.png?v=8239e45d9e9b363c649caaa3e24a0221"/><link rel="apple-touch-icon" sizes="144x144" href="/icons/icon-144x144.png?v=8239e45d9e9b363c649caaa3e24a0221"/><link rel="apple-touch-icon" sizes="192x192" href="/icons/icon-192x192.png?v=8239e45d9e9b363c649caaa3e24a0221"/><link rel="apple-touch-icon" sizes="256x256" href="/icons/icon-256x256.png?v=8239e45d9e9b363c649caaa3e24a0221"/><link rel="apple-touch-icon" sizes="384x384" href="/icons/icon-384x384.png?v=8239e45d9e9b363c649caaa3e24a0221"/><link rel="apple-touch-icon" sizes="512x512" href="/icons/icon-512x512.png?v=8239e45d9e9b363c649caaa3e24a0221"/><title data-react-helmet="true">Machine Translation EN-JP Seq2Seq Tensorflow | Hyuto&#x27;s Blog</title><meta data-react-helmet="true" name="description" content="Machine Translation EN-JP Seq2Seq using Tensorflow 2."/><meta data-react-helmet="true" property="og:title" content="Machine Translation EN-JP Seq2Seq Tensorflow"/><meta data-react-helmet="true" property="og:description" content="Machine Translation EN-JP Seq2Seq using Tensorflow 2."/><meta data-react-helmet="true" property="og:type" content="website"/><link as="script" rel="preload" href="/webpack-runtime-25b9e67a831a950b2ccf.js"/><link as="script" rel="preload" href="/framework-cf9a832c62e1a97a7e95.js"/><link as="script" rel="preload" href="/app-56f5b02a7eae32f29a0e.js"/><link as="script" rel="preload" href="/commons-4425428cc7103c2b9c46.js"/><link as="script" rel="preload" href="/component---src-templates-blog-post-js-3484340e7d6fb1e13f5f.js"/><link as="fetch" rel="preload" href="/page-data/machine-translation-en-jp-seq2seq-tf/page-data.json" crossorigin="anonymous"/><link as="fetch" rel="preload" href="/page-data/sq/d/3000541721.json" crossorigin="anonymous"/><link as="fetch" rel="preload" href="/page-data/sq/d/3274528899.json" crossorigin="anonymous"/><link as="fetch" rel="preload" href="/page-data/app-data.json" crossorigin="anonymous"/></head><body><div id="___gatsby"><div style="outline:none" tabindex="-1" id="gatsby-focus-wrapper"><div class="global-wrapper" data-is-root-path="false"><header class="global-header"><a class="header-link-home" href="/">Hyuto&#x27;s Blog</a></header><main><article class="blog-post" itemscope="" itemType="http://schema.org/Article"><header><h1 itemProp="headline">Machine Translation EN-JP Seq2Seq Tensorflow</h1><p>August 19, 2020</p></header><section itemProp="articleBody"><p>Hello guys, lately i‚Äôve been studying about machine translation and give it a try.</p>
<p>Most of code in this notebook is based on tensorflow tutorial on their website
<a href="https://www.tensorflow.org/addons/tutorials/networks_seq2seq_nmt">TensorFlow Addons Networks : Sequence-to-Sequence NMT with Attention Mechanism</a>.</p>
<p>This notebook is basically my notebook run on <a href="https://www.kaggle.com/">kaggle</a> so if you want to try
and run the code with same environment as mine go to link bellow.</p>
<p>Kaggle Notebook : <a href="https://www.kaggle.com/wahyusetianto/machine-translation-en-jp-seq2seq-tf">Machine Translation EN-JP Seq2Seq Tensorflow</a></p>
<p>P.S. Don‚Äôt forget to <em>upvote</em> if you like it üòä.</p>
<h1>English - Japanese Machine Translation</h1>
<p>So in this notebook we‚Äôre going to build English to Japanese machine translation, Japanese text contains lots of unique words because they have 3 type of it:</p>
<ol>
<li>Kanji</li>
<li>Katakana</li>
<li>Hiragana</li>
</ol>
<p>that‚Äôs the interesting part of it and so it‚Äôll be little complicated to process. So let‚Äôs get started.</p>
<h1>Table Of Content</h1>
<ol>
<li>Load Dataset</li>
<li>Text Preprocessing
<ul>
<li>English misspell handling</li>
<li>Segmenting Japanese words</li>
<li>Add BOS and EOS to train sentences</li>
</ul>
</li>
<li>Word Tokenizing
<ul>
<li>Word Cloud</li>
</ul>
</li>
<li>Build &#x26; Train Model</li>
<li>Scoring Bleu</li>
<li>Test with Some Raw Input</li>
</ol>
<h2>Install some tools</h2>
<ol>
<li>Sacreblue for calculate BLEU score</li>
<li>Googletrans => Google Translate for testing some sentences later</li>
</ol>
<p>Note : You can use NLTK for calculating BLEU score <a href="https://www.nltk.org/_modules/nltk/translate/bleu_score.html">documentation</a></p>
<div class="gatsby-highlight" data-language="python"><pre class="language-python"><code class="language-python">!pip <span class="token operator">-</span>q install sacrebleu
!pip <span class="token operator">-</span>q install googletrans
!pip <span class="token operator">-</span>q install tensorflow<span class="token operator">-</span>addons <span class="token operator">-</span><span class="token operator">-</span>upgrade</code></pre></div>
<div class="gatsby-highlight" data-language="python"><pre class="language-python"><code class="language-python"><span class="token keyword">import</span> random<span class="token punctuation">,</span> re<span class="token punctuation">,</span> string<span class="token punctuation">,</span> itertools<span class="token punctuation">,</span> timeit<span class="token punctuation">,</span> sacrebleu
<span class="token keyword">import</span> numpy <span class="token keyword">as</span> np
<span class="token keyword">import</span> pandas <span class="token keyword">as</span> pd
<span class="token keyword">import</span> matplotlib<span class="token punctuation">.</span>pyplot <span class="token keyword">as</span> plt
<span class="token keyword">import</span> seaborn <span class="token keyword">as</span> sns
<span class="token keyword">from</span> tqdm<span class="token punctuation">.</span>notebook <span class="token keyword">import</span> tqdm
<span class="token keyword">from</span> IPython<span class="token punctuation">.</span>display <span class="token keyword">import</span> display<span class="token punctuation">,</span> clear_output
<span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>model_selection <span class="token keyword">import</span> train_test_split

<span class="token comment"># Tensorflow &amp; Keras</span>
<span class="token keyword">import</span> tensorflow <span class="token keyword">as</span> tf
<span class="token keyword">import</span> tensorflow_addons <span class="token keyword">as</span> tfa
<span class="token keyword">import</span> tensorflow<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>backend <span class="token keyword">as</span> K
<span class="token keyword">from</span> tensorflow<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>layers <span class="token keyword">import</span> Input<span class="token punctuation">,</span> Dense<span class="token punctuation">,</span> LSTM<span class="token punctuation">,</span> LSTMCell
<span class="token keyword">from</span> tensorflow<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>layers <span class="token keyword">import</span> Embedding<span class="token punctuation">,</span> Bidirectional
<span class="token keyword">from</span> tensorflow<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>models <span class="token keyword">import</span> Model
<span class="token keyword">from</span> tensorflow<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>preprocessing<span class="token punctuation">.</span>text <span class="token keyword">import</span> Tokenizer
<span class="token keyword">from</span> tensorflow<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>preprocessing<span class="token punctuation">.</span>sequence <span class="token keyword">import</span> pad_sequences
<span class="token keyword">from</span> tensorflow<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>losses <span class="token keyword">import</span> SparseCategoricalCrossentropy
<span class="token keyword">from</span> tensorflow<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>optimizers <span class="token keyword">import</span> Adam

<span class="token comment"># Japanese Word Tokenizer</span>
<span class="token keyword">from</span> janome<span class="token punctuation">.</span>tokenizer <span class="token keyword">import</span> Tokenizer <span class="token keyword">as</span> janome_tokenizer

plt<span class="token punctuation">.</span>style<span class="token punctuation">.</span>use<span class="token punctuation">(</span><span class="token string">'seaborn-pastel'</span><span class="token punctuation">)</span></code></pre></div>
<h1>Dataset</h1>
<p>Here we use 55463 en-jp corpus from <a href="http://www.manythings.org/bilingual/">ManyThings.org Bilingual Sentence Pairs</a></p>
<div class="gatsby-highlight" data-language="python"><pre class="language-python"><code class="language-python"><span class="token comment"># Download Data &amp; Unzip Data</span>
!wget http<span class="token punctuation">:</span><span class="token operator">//</span>www<span class="token punctuation">.</span>manythings<span class="token punctuation">.</span>org<span class="token operator">/</span>anki<span class="token operator">/</span>jpn<span class="token operator">-</span>eng<span class="token punctuation">.</span><span class="token builtin">zip</span>
!unzip jpn<span class="token operator">-</span>eng<span class="token punctuation">.</span><span class="token builtin">zip</span></code></pre></div>
<div class="gatsby-highlight" data-language="text"><pre class="language-text"><code class="language-text">--2020-08-19 01:52:31--  http://www.manythings.org/anki/jpn-eng.zip
Resolving www.manythings.org (www.manythings.org)... 104.24.109.196, 104.24.108.196, 172.67.173.198, ...
Connecting to www.manythings.org (www.manythings.org)|104.24.109.196|:80... connected.
HTTP request sent, awaiting response... 200 OK
Length: 2303148 (2.2M) [application/zip]
Saving to: ‚Äòjpn-eng.zip‚Äô

jpn-eng.zip         100%[===================>]   2.20M  9.70MB/s    in 0.2s

2020-08-19 01:52:32 (9.70 MB/s) - ‚Äòjpn-eng.zip‚Äô saved [2303148/2303148]

Archive:  jpn-eng.zip
  inflating: jpn.txt
  inflating: _about.txt</code></pre></div>
<p>Load data to memory.</p>
<div class="gatsby-highlight" data-language="python"><pre class="language-python"><code class="language-python">data <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>

f1 <span class="token operator">=</span> <span class="token builtin">open</span><span class="token punctuation">(</span><span class="token string">'./jpn.txt'</span><span class="token punctuation">,</span> <span class="token string">'r'</span><span class="token punctuation">)</span>
data <span class="token operator">+=</span> <span class="token punctuation">[</span>x<span class="token punctuation">.</span>rstrip<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>lower<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>split<span class="token punctuation">(</span><span class="token string">'\t'</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token number">2</span><span class="token punctuation">]</span> <span class="token keyword">for</span> x <span class="token keyword">in</span> tqdm<span class="token punctuation">(</span>f1<span class="token punctuation">.</span>readlines<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">]</span>
f1<span class="token punctuation">.</span>close<span class="token punctuation">(</span><span class="token punctuation">)</span>

<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f'Loaded </span><span class="token interpolation"><span class="token punctuation">{</span><span class="token builtin">len</span><span class="token punctuation">(</span>data<span class="token punctuation">)</span><span class="token punctuation">}</span></span><span class="token string"> Sentences'</span></span><span class="token punctuation">)</span></code></pre></div>
<div class="gatsby-highlight" data-language="text"><pre class="language-text"><code class="language-text">Loaded 53594 Sentences</code></pre></div>
<h1>Text Preprocessing</h1>
<h3>Handling misspell words &#x26; Clearing Punctuation</h3>
<p>we‚Äôre gonna change the misspell words in english sentences and clearing punctuation from text.</p>
<p>‚Äúaren‚Äôt my english bad?‚Äù -> ‚Äúare not my english bad‚Äù</p>
<div class="gatsby-highlight" data-language="python"><pre class="language-python"><code class="language-python">mispell_dict <span class="token operator">=</span> <span class="token punctuation">{</span>
    <span class="token string">"aren't"</span> <span class="token punctuation">:</span> <span class="token string">"are not"</span><span class="token punctuation">,</span>
    <span class="token string">"can't"</span> <span class="token punctuation">:</span> <span class="token string">"cannot"</span><span class="token punctuation">,</span>
    <span class="token string">"couldn't"</span> <span class="token punctuation">:</span> <span class="token string">"could not"</span><span class="token punctuation">,</span>
    <span class="token string">"didn't"</span> <span class="token punctuation">:</span> <span class="token string">"did not"</span><span class="token punctuation">,</span>
    <span class="token string">"doesn't"</span> <span class="token punctuation">:</span> <span class="token string">"does not"</span><span class="token punctuation">,</span>
    <span class="token string">"don't"</span> <span class="token punctuation">:</span> <span class="token string">"do not"</span><span class="token punctuation">,</span>
    <span class="token string">"hadn't"</span> <span class="token punctuation">:</span> <span class="token string">"had not"</span><span class="token punctuation">,</span>
    <span class="token string">"hasn't"</span> <span class="token punctuation">:</span> <span class="token string">"has not"</span><span class="token punctuation">,</span>
    <span class="token string">"haven't"</span> <span class="token punctuation">:</span> <span class="token string">"have not"</span><span class="token punctuation">,</span>
    <span class="token string">"he'd"</span> <span class="token punctuation">:</span> <span class="token string">"he would"</span><span class="token punctuation">,</span>
    <span class="token string">"he'll"</span> <span class="token punctuation">:</span> <span class="token string">"he will"</span><span class="token punctuation">,</span>
    <span class="token string">"he's"</span> <span class="token punctuation">:</span> <span class="token string">"he is"</span><span class="token punctuation">,</span>
    <span class="token string">"i'd"</span> <span class="token punctuation">:</span> <span class="token string">"i would"</span><span class="token punctuation">,</span>
    <span class="token string">"i'd"</span> <span class="token punctuation">:</span> <span class="token string">"i had"</span><span class="token punctuation">,</span>
    <span class="token string">"i'll"</span> <span class="token punctuation">:</span> <span class="token string">"i will"</span><span class="token punctuation">,</span>
    <span class="token string">"i'm"</span> <span class="token punctuation">:</span> <span class="token string">"i am"</span><span class="token punctuation">,</span>
    <span class="token string">"isn't"</span> <span class="token punctuation">:</span> <span class="token string">"is not"</span><span class="token punctuation">,</span>
    <span class="token string">"it's"</span> <span class="token punctuation">:</span> <span class="token string">"it is"</span><span class="token punctuation">,</span>
    <span class="token string">"it'll"</span><span class="token punctuation">:</span><span class="token string">"it will"</span><span class="token punctuation">,</span>
    <span class="token string">"i've"</span> <span class="token punctuation">:</span> <span class="token string">"i have"</span><span class="token punctuation">,</span>
    <span class="token string">"let's"</span> <span class="token punctuation">:</span> <span class="token string">"let us"</span><span class="token punctuation">,</span>
    <span class="token string">"mightn't"</span> <span class="token punctuation">:</span> <span class="token string">"might not"</span><span class="token punctuation">,</span>
    <span class="token string">"mustn't"</span> <span class="token punctuation">:</span> <span class="token string">"must not"</span><span class="token punctuation">,</span>
    <span class="token string">"shan't"</span> <span class="token punctuation">:</span> <span class="token string">"shall not"</span><span class="token punctuation">,</span>
    <span class="token string">"she'd"</span> <span class="token punctuation">:</span> <span class="token string">"she would"</span><span class="token punctuation">,</span>
    <span class="token string">"she'll"</span> <span class="token punctuation">:</span> <span class="token string">"she will"</span><span class="token punctuation">,</span>
    <span class="token string">"she's"</span> <span class="token punctuation">:</span> <span class="token string">"she is"</span><span class="token punctuation">,</span>
    <span class="token string">"shouldn't"</span> <span class="token punctuation">:</span> <span class="token string">"should not"</span><span class="token punctuation">,</span>
    <span class="token string">"that's"</span> <span class="token punctuation">:</span> <span class="token string">"that is"</span><span class="token punctuation">,</span>
    <span class="token string">"there's"</span> <span class="token punctuation">:</span> <span class="token string">"there is"</span><span class="token punctuation">,</span>
    <span class="token string">"they'd"</span> <span class="token punctuation">:</span> <span class="token string">"they would"</span><span class="token punctuation">,</span>
    <span class="token string">"they'll"</span> <span class="token punctuation">:</span> <span class="token string">"they will"</span><span class="token punctuation">,</span>
    <span class="token string">"they're"</span> <span class="token punctuation">:</span> <span class="token string">"they are"</span><span class="token punctuation">,</span>
    <span class="token string">"they've"</span> <span class="token punctuation">:</span> <span class="token string">"they have"</span><span class="token punctuation">,</span>
    <span class="token string">"we'd"</span> <span class="token punctuation">:</span> <span class="token string">"we would"</span><span class="token punctuation">,</span>
    <span class="token string">"we're"</span> <span class="token punctuation">:</span> <span class="token string">"we are"</span><span class="token punctuation">,</span>
    <span class="token string">"weren't"</span> <span class="token punctuation">:</span> <span class="token string">"were not"</span><span class="token punctuation">,</span>
    <span class="token string">"we've"</span> <span class="token punctuation">:</span> <span class="token string">"we have"</span><span class="token punctuation">,</span>
    <span class="token string">"what'll"</span> <span class="token punctuation">:</span> <span class="token string">"what will"</span><span class="token punctuation">,</span>
    <span class="token string">"what're"</span> <span class="token punctuation">:</span> <span class="token string">"what are"</span><span class="token punctuation">,</span>
    <span class="token string">"what's"</span> <span class="token punctuation">:</span> <span class="token string">"what is"</span><span class="token punctuation">,</span>
    <span class="token string">"what've"</span> <span class="token punctuation">:</span> <span class="token string">"what have"</span><span class="token punctuation">,</span>
    <span class="token string">"where's"</span> <span class="token punctuation">:</span> <span class="token string">"where is"</span><span class="token punctuation">,</span>
    <span class="token string">"who'd"</span> <span class="token punctuation">:</span> <span class="token string">"who would"</span><span class="token punctuation">,</span>
    <span class="token string">"who'll"</span> <span class="token punctuation">:</span> <span class="token string">"who will"</span><span class="token punctuation">,</span>
    <span class="token string">"who're"</span> <span class="token punctuation">:</span> <span class="token string">"who are"</span><span class="token punctuation">,</span>
    <span class="token string">"who's"</span> <span class="token punctuation">:</span> <span class="token string">"who is"</span><span class="token punctuation">,</span>
    <span class="token string">"who've"</span> <span class="token punctuation">:</span> <span class="token string">"who have"</span><span class="token punctuation">,</span>
    <span class="token string">"won't"</span> <span class="token punctuation">:</span> <span class="token string">"will not"</span><span class="token punctuation">,</span>
    <span class="token string">"wouldn't"</span> <span class="token punctuation">:</span> <span class="token string">"would not"</span><span class="token punctuation">,</span>
    <span class="token string">"you'd"</span> <span class="token punctuation">:</span> <span class="token string">"you would"</span><span class="token punctuation">,</span>
    <span class="token string">"you'll"</span> <span class="token punctuation">:</span> <span class="token string">"you will"</span><span class="token punctuation">,</span>
    <span class="token string">"you're"</span> <span class="token punctuation">:</span> <span class="token string">"you are"</span><span class="token punctuation">,</span>
    <span class="token string">"you've"</span> <span class="token punctuation">:</span> <span class="token string">"you have"</span><span class="token punctuation">,</span>
    <span class="token string">"'re"</span><span class="token punctuation">:</span> <span class="token string">" are"</span><span class="token punctuation">,</span>
    <span class="token string">"wasn't"</span><span class="token punctuation">:</span> <span class="token string">"was not"</span><span class="token punctuation">,</span>
    <span class="token string">"we'll"</span><span class="token punctuation">:</span><span class="token string">" will"</span><span class="token punctuation">,</span>
    <span class="token string">"didn't"</span><span class="token punctuation">:</span> <span class="token string">"did not"</span><span class="token punctuation">,</span>
    <span class="token string">"tryin'"</span><span class="token punctuation">:</span><span class="token string">"trying"</span>
<span class="token punctuation">}</span>

mispell_re <span class="token operator">=</span> re<span class="token punctuation">.</span><span class="token builtin">compile</span><span class="token punctuation">(</span><span class="token string">'(%s)'</span> <span class="token operator">%</span> <span class="token string">'|'</span><span class="token punctuation">.</span>join<span class="token punctuation">(</span>mispell_dict<span class="token punctuation">.</span>keys<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>

<span class="token keyword">def</span> <span class="token function">preprocess</span><span class="token punctuation">(</span>text<span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">></span> <span class="token builtin">str</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">replace</span><span class="token punctuation">(</span>match<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">return</span> mispell_dict<span class="token punctuation">[</span>match<span class="token punctuation">.</span>group<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">]</span>

    text <span class="token operator">=</span> mispell_re<span class="token punctuation">.</span>sub<span class="token punctuation">(</span>replace<span class="token punctuation">,</span> text<span class="token punctuation">)</span>
    <span class="token keyword">return</span> text</code></pre></div>
<p>Japanese words have their own punctuation like „Äêthis„Äë</p>
<div class="gatsby-highlight" data-language="python"><pre class="language-python"><code class="language-python"><span class="token comment"># Adding Japanese Punctuation</span>
string<span class="token punctuation">.</span>punctuation <span class="token operator">+=</span> <span class="token string">'„ÄÅ„ÄÇ„Äê„Äë„Äå„Äç„Äé„Äè‚Ä¶„Éª„ÄΩÔºàÔºâ„ÄúÔºüÔºÅÔΩ°ÔºöÔΩ§ÔºõÔΩ•'</span>

CP <span class="token operator">=</span> <span class="token keyword">lambda</span> x <span class="token punctuation">:</span> x<span class="token punctuation">.</span>translate<span class="token punctuation">(</span><span class="token builtin">str</span><span class="token punctuation">.</span>maketrans<span class="token punctuation">(</span><span class="token string">''</span><span class="token punctuation">,</span> <span class="token string">''</span><span class="token punctuation">,</span> string<span class="token punctuation">.</span>punctuation<span class="token punctuation">)</span><span class="token punctuation">)</span></code></pre></div>
<div class="gatsby-highlight" data-language="python"><pre class="language-python"><code class="language-python">data <span class="token operator">=</span> <span class="token punctuation">[</span>x <span class="token keyword">for</span> x <span class="token keyword">in</span> data <span class="token keyword">if</span> <span class="token builtin">len</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span> <span class="token operator">==</span> <span class="token number">2</span><span class="token punctuation">]</span>

eng_data <span class="token operator">=</span> <span class="token punctuation">[</span>CP<span class="token punctuation">(</span>preprocess<span class="token punctuation">(</span>x<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token keyword">for</span> x <span class="token keyword">in</span> data<span class="token punctuation">]</span>
jpn_data <span class="token operator">=</span> <span class="token punctuation">[</span>CP<span class="token punctuation">(</span>x<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token keyword">for</span> x <span class="token keyword">in</span> data<span class="token punctuation">]</span></code></pre></div>
<h3>Segmenting Japanese Sentences</h3>
<p>Unlike english sentence we can tokenize it by splitting words with space just like this,</p>
<div class="gatsby-highlight" data-language="text"><pre class="language-text"><code class="language-text">'This is english or i think so'.split()

Output:
['This', 'is', 'english', 'or', 'i', 'think', 'so']</code></pre></div>
<p>but in Japanese we can‚Äôt do it that way. Here we gonna use Janome Tokenizer to segmenting Japanese sentence and adding space to it so Keras Tokenizer can handle it.</p>
<div class="gatsby-highlight" data-language="python"><pre class="language-python"><code class="language-python"><span class="token comment"># Initialize Janome Tokenizer</span>
token_jp <span class="token operator">=</span> janome_tokenizer<span class="token punctuation">(</span><span class="token punctuation">)</span></code></pre></div>
<div class="gatsby-highlight" data-language="python"><pre class="language-python"><code class="language-python">sample_text <span class="token operator">=</span> <span class="token string">'„Åì„Åì„ÅßÁßÅ„ÅØËã±Ë™û„ÅßË©±„Åó„Å¶„ÅÑ„Çã'</span>
<span class="token string">' '</span><span class="token punctuation">.</span>join<span class="token punctuation">(</span><span class="token punctuation">[</span>word <span class="token keyword">for</span> word <span class="token keyword">in</span> token_jp<span class="token punctuation">.</span>tokenize<span class="token punctuation">(</span>sample_text<span class="token punctuation">,</span> wakati<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span> \
          <span class="token keyword">if</span> word <span class="token operator">!=</span> <span class="token string">' '</span><span class="token punctuation">]</span><span class="token punctuation">)</span></code></pre></div>
<div class="gatsby-highlight" data-language="text"><pre class="language-text"><code class="language-text">'„Åì„Åì „Åß ÁßÅ „ÅØ Ëã±Ë™û „Åß Ë©±„Åó „Å¶ „ÅÑ„Çã'</code></pre></div>
<div class="gatsby-highlight" data-language="python"><pre class="language-python"><code class="language-python"><span class="token comment"># Apply to Japanese Sentences</span>
jpn_data <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token string">' '</span><span class="token punctuation">.</span>join<span class="token punctuation">(</span><span class="token punctuation">[</span>word <span class="token keyword">for</span> word <span class="token keyword">in</span> token_jp<span class="token punctuation">.</span>tokenize<span class="token punctuation">(</span>x<span class="token punctuation">,</span> wakati<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span> \
                      <span class="token keyword">if</span> word <span class="token operator">!=</span> <span class="token string">' '</span><span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token keyword">for</span> x <span class="token keyword">in</span> tqdm<span class="token punctuation">(</span>jpn_data<span class="token punctuation">)</span><span class="token punctuation">]</span></code></pre></div>
<p>For evaluating our model let‚Äôs split our data.</p>
<div class="gatsby-highlight" data-language="python"><pre class="language-python"><code class="language-python">eng_train<span class="token punctuation">,</span> eng_test<span class="token punctuation">,</span> jpn_train<span class="token punctuation">,</span> jpn_test <span class="token operator">=</span> \
train_test_split<span class="token punctuation">(</span>eng_data<span class="token punctuation">,</span> jpn_data<span class="token punctuation">,</span> test_size <span class="token operator">=</span> <span class="token number">0.04</span><span class="token punctuation">,</span> random_state <span class="token operator">=</span> <span class="token number">42</span><span class="token punctuation">)</span>

<span class="token keyword">print</span><span class="token punctuation">(</span>f"Splitting to <span class="token punctuation">{</span><span class="token builtin">len</span><span class="token punctuation">(</span>eng_train<span class="token punctuation">)</span><span class="token punctuation">}</span> Train data <span class="token keyword">and</span> \
<span class="token punctuation">{</span><span class="token builtin">len</span><span class="token punctuation">(</span>eng_test<span class="token punctuation">)</span><span class="token punctuation">}</span> Test data"<span class="token punctuation">)</span></code></pre></div>
<div class="gatsby-highlight" data-language="text"><pre class="language-text"><code class="language-text">Splitting to 51450 Train data and 2144 Test data</code></pre></div>
<h3>Add BOS and EOS</h3>
<p>We put BOS ‚ÄúBegin of Sequence‚Äù and EOS ‚ÄúEnd of Sequence‚Äù to help our decoder recognize begin and end of a sequence.</p>
<div class="gatsby-highlight" data-language="python"><pre class="language-python"><code class="language-python">eng_train <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token string">'bos '</span><span class="token operator">+</span> x <span class="token operator">+</span> <span class="token string">' eos'</span> <span class="token keyword">for</span> x <span class="token keyword">in</span> eng_train <span class="token operator">+</span> <span class="token punctuation">[</span><span class="token string">'unk unk unk'</span><span class="token punctuation">]</span><span class="token punctuation">]</span>
jpn_train <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token string">'bos '</span><span class="token operator">+</span> x <span class="token operator">+</span> <span class="token string">' eos'</span> <span class="token keyword">for</span> x <span class="token keyword">in</span> jpn_train <span class="token operator">+</span> <span class="token punctuation">[</span><span class="token string">'unk unk unk'</span><span class="token punctuation">]</span><span class="token punctuation">]</span>

eng_val <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token string">'bos '</span><span class="token operator">+</span> x <span class="token operator">+</span> <span class="token string">' eos'</span> <span class="token keyword">for</span> x <span class="token keyword">in</span> eng_test<span class="token punctuation">]</span>
jpn_val <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token string">'bos '</span><span class="token operator">+</span> x <span class="token operator">+</span> <span class="token string">' eos'</span> <span class="token keyword">for</span> x <span class="token keyword">in</span> jpn_test<span class="token punctuation">]</span></code></pre></div>
<h1>Word Tokenizing</h1>
<p>Here we use Tokenizer API from Keras to make vocabulary and tokenizing our data</p>
<div class="gatsby-highlight" data-language="python"><pre class="language-python"><code class="language-python"><span class="token comment"># English Tokenizer</span>
en_tokenizer <span class="token operator">=</span> Tokenizer<span class="token punctuation">(</span>filters<span class="token operator">=</span><span class="token string">''</span><span class="token punctuation">)</span>
en_tokenizer<span class="token punctuation">.</span>fit_on_texts<span class="token punctuation">(</span>eng_train<span class="token punctuation">)</span>

<span class="token comment"># Japannese Tokenizer</span>
jp_tokenizer <span class="token operator">=</span> Tokenizer<span class="token punctuation">(</span>filters<span class="token operator">=</span><span class="token string">''</span><span class="token punctuation">)</span>
jp_tokenizer<span class="token punctuation">.</span>fit_on_texts<span class="token punctuation">(</span>jpn_train<span class="token punctuation">)</span></code></pre></div>
<div class="gatsby-highlight" data-language="python"><pre class="language-python"><code class="language-python"><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f'English vocab size   :'</span></span><span class="token punctuation">,</span> <span class="token builtin">len</span><span class="token punctuation">(</span>en_tokenizer<span class="token punctuation">.</span>word_index<span class="token punctuation">)</span> <span class="token operator">-</span> <span class="token number">3</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f'Japanese vocab size  :'</span></span><span class="token punctuation">,</span> <span class="token builtin">len</span><span class="token punctuation">(</span>jp_tokenizer<span class="token punctuation">.</span>word_index<span class="token punctuation">)</span> <span class="token operator">-</span> <span class="token number">3</span><span class="token punctuation">)</span></code></pre></div>
<div class="gatsby-highlight" data-language="text"><pre class="language-text"><code class="language-text">English vocab size   : 9646
Japanese vocab size  : 14403</code></pre></div>
<h2>Word Cloud</h2>
<p>What comes when doing NLP? It‚Äôs Word Cloud. Let‚Äôs do it for our vocab.</p>
<p>Font : <a href="https://www.google.com/get/noto/">Google Noto Fonts</a> -> Noto Sans CJK JP</p>
<div class="gatsby-highlight" data-language="python"><pre class="language-python"><code class="language-python">!wget https<span class="token punctuation">:</span><span class="token operator">//</span>noto<span class="token operator">-</span>website<span class="token operator">-</span><span class="token number">2.</span>storage<span class="token punctuation">.</span>googleapis<span class="token punctuation">.</span>com<span class="token operator">/</span>pkgs<span class="token operator">/</span>NotoSansCJKjp<span class="token operator">-</span>hinted<span class="token punctuation">.</span><span class="token builtin">zip</span>
!wget https<span class="token punctuation">:</span><span class="token operator">//</span>raw<span class="token punctuation">.</span>githubusercontent<span class="token punctuation">.</span>com<span class="token operator">/</span>Hyuto<span class="token operator">/</span>NMT<span class="token operator">-</span>TF<span class="token operator">-</span>Seq2seq<span class="token operator">-</span>EN<span class="token operator">-</span>JP<span class="token operator">/</span>master<span class="token operator">/</span>Japan<span class="token punctuation">.</span>jpg
!wget https<span class="token punctuation">:</span><span class="token operator">//</span>raw<span class="token punctuation">.</span>githubusercontent<span class="token punctuation">.</span>com<span class="token operator">/</span>Hyuto<span class="token operator">/</span>NMT<span class="token operator">-</span>TF<span class="token operator">-</span>Seq2seq<span class="token operator">-</span>EN<span class="token operator">-</span>JP<span class="token operator">/</span>master<span class="token operator">/</span>English<span class="token punctuation">.</span>png
!mkdir font
!unzip NotoSansCJKjp<span class="token operator">-</span>hinted<span class="token punctuation">.</span><span class="token builtin">zip</span> <span class="token operator">-</span>d <span class="token punctuation">.</span><span class="token operator">/</span>font</code></pre></div>
<div class="gatsby-highlight" data-language="text"><pre class="language-text"><code class="language-text">--2020-08-19 01:53:52--  https://noto-website-2.storage.googleapis.com/pkgs/NotoSansCJKjp-hinted.zip
Resolving noto-website-2.storage.googleapis.com (noto-website-2.storage.googleapis.com)... 172.217.204.128, 2607:f8b0:400c:c15::80
Connecting to noto-website-2.storage.googleapis.com (noto-website-2.storage.googleapis.com)|172.217.204.128|:443... connected.
HTTP request sent, awaiting response... 200 OK
Length: 121096772 (115M) [application/zip]
Saving to: ‚ÄòNotoSansCJKjp-hinted.zip‚Äô

NotoSansCJKjp-hinte 100%[===================>] 115.49M  53.3MB/s    in 2.2s

2020-08-19 01:53:54 (53.3 MB/s) - ‚ÄòNotoSansCJKjp-hinted.zip‚Äô saved [121096772/121096772]

--2020-08-19 01:53:55--  https://raw.githubusercontent.com/Hyuto/NMT-TF-Seq2seq-EN-JP/master/Japan.jpg
Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.200.133
Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.200.133|:443... connected.
HTTP request sent, awaiting response... 200 OK
Length: 3976 (3.9K) [image/jpeg]
Saving to: ‚ÄòJapan.jpg‚Äô

Japan.jpg           100%[===================>]   3.88K  --.-KB/s    in 0s

2020-08-19 01:53:55 (46.7 MB/s) - ‚ÄòJapan.jpg‚Äô saved [3976/3976]

--2020-08-19 01:53:56--  https://raw.githubusercontent.com/Hyuto/NMT-TF-Seq2seq-EN-JP/master/English.png
Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.200.133
Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.200.133|:443... connected.
HTTP request sent, awaiting response... 200 OK
Length: 303684 (297K) [image/png]
Saving to: ‚ÄòEnglish.png‚Äô

English.png         100%[===================>] 296.57K  --.-KB/s    in 0.1s

2020-08-19 01:53:56 (2.18 MB/s) - ‚ÄòEnglish.png‚Äô saved [303684/303684]

Archive:  NotoSansCJKjp-hinted.zip
  inflating: ./font/LICENSE_OFL.txt
  inflating: ./font/NotoSansCJKjp-Black.otf
  inflating: ./font/NotoSansCJKjp-Bold.otf
  inflating: ./font/NotoSansCJKjp-DemiLight.otf
  inflating: ./font/NotoSansCJKjp-Light.otf
  inflating: ./font/NotoSansCJKjp-Medium.otf
  inflating: ./font/NotoSansCJKjp-Regular.otf
  inflating: ./font/NotoSansCJKjp-Thin.otf
  inflating: ./font/NotoSansMonoCJKjp-Bold.otf
  inflating: ./font/NotoSansMonoCJKjp-Regular.otf
  inflating: ./font/README</code></pre></div>
<div class="gatsby-highlight" data-language="python"><pre class="language-python"><code class="language-python"><span class="token keyword">from</span> wordcloud <span class="token keyword">import</span> WordCloud<span class="token punctuation">,</span> ImageColorGenerator
<span class="token keyword">from</span> PIL <span class="token keyword">import</span> Image

<span class="token keyword">def</span> <span class="token function">get_words</span><span class="token punctuation">(</span>arr<span class="token punctuation">)</span><span class="token punctuation">:</span>
    keys <span class="token operator">=</span> <span class="token builtin">list</span><span class="token punctuation">(</span>arr<span class="token punctuation">.</span>keys<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
    count <span class="token operator">=</span> <span class="token builtin">list</span><span class="token punctuation">(</span>arr<span class="token punctuation">.</span>values<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
    <span class="token keyword">return</span> <span class="token string">' '</span><span class="token punctuation">.</span>join<span class="token punctuation">(</span><span class="token punctuation">[</span>x <span class="token keyword">for</span> _<span class="token punctuation">,</span>x <span class="token keyword">in</span> <span class="token builtin">sorted</span><span class="token punctuation">(</span><span class="token builtin">zip</span><span class="token punctuation">(</span>count<span class="token punctuation">,</span> keys<span class="token punctuation">)</span><span class="token punctuation">,</span> reverse <span class="token operator">=</span> <span class="token boolean">True</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token punctuation">)</span>

<span class="token keyword">def</span> <span class="token function">transform</span><span class="token punctuation">(</span>arr<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token builtin">len</span><span class="token punctuation">(</span>arr<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">for</span> j <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token builtin">len</span><span class="token punctuation">(</span>arr<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
            <span class="token keyword">if</span> <span class="token keyword">not</span> <span class="token builtin">any</span><span class="token punctuation">(</span>arr<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">[</span>j<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
                arr<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">[</span>j<span class="token punctuation">]</span> <span class="token operator">=</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">225</span><span class="token punctuation">,</span> <span class="token number">225</span><span class="token punctuation">,</span> <span class="token number">225</span><span class="token punctuation">,</span> <span class="token number">225</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
    <span class="token keyword">return</span> arr

font_path <span class="token operator">=</span> <span class="token string">'./font/NotoSansCJKjp-Light.otf'</span>


mask <span class="token operator">=</span> <span class="token string">'./English.png'</span>
mask <span class="token operator">=</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span>Image<span class="token punctuation">.</span><span class="token builtin">open</span><span class="token punctuation">(</span>mask<span class="token punctuation">)</span><span class="token punctuation">)</span>
mask <span class="token operator">=</span> transform<span class="token punctuation">(</span>mask<span class="token punctuation">)</span>
image_colors <span class="token operator">=</span> ImageColorGenerator<span class="token punctuation">(</span>mask<span class="token punctuation">)</span>
words <span class="token operator">=</span> get_words<span class="token punctuation">(</span>en_tokenizer<span class="token punctuation">.</span>word_counts<span class="token punctuation">)</span><span class="token punctuation">.</span>title<span class="token punctuation">(</span><span class="token punctuation">)</span>
wc <span class="token operator">=</span> WordCloud<span class="token punctuation">(</span>background_color<span class="token operator">=</span><span class="token string">"white"</span><span class="token punctuation">,</span> max_words<span class="token operator">=</span><span class="token number">2000</span><span class="token punctuation">,</span> random_state<span class="token operator">=</span><span class="token number">42</span><span class="token punctuation">,</span>
               width<span class="token operator">=</span>mask<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span> height<span class="token operator">=</span>mask<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
wc <span class="token operator">=</span> wc<span class="token punctuation">.</span>generate<span class="token punctuation">(</span>words<span class="token punctuation">)</span>
fig1<span class="token punctuation">,</span> ax1 <span class="token operator">=</span> plt<span class="token punctuation">.</span>subplots<span class="token punctuation">(</span>figsize<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">20</span><span class="token punctuation">,</span><span class="token number">15</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
ax1<span class="token punctuation">.</span>imshow<span class="token punctuation">(</span>wc<span class="token punctuation">.</span>recolor<span class="token punctuation">(</span>color_func<span class="token operator">=</span>image_colors<span class="token punctuation">)</span><span class="token punctuation">,</span> interpolation<span class="token operator">=</span><span class="token string">'bilinear'</span><span class="token punctuation">)</span>
ax1<span class="token punctuation">.</span>axis<span class="token punctuation">(</span><span class="token string">"off"</span><span class="token punctuation">)</span>

mask <span class="token operator">=</span> <span class="token string">'./Japan.jpg'</span>
mask <span class="token operator">=</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span>Image<span class="token punctuation">.</span><span class="token builtin">open</span><span class="token punctuation">(</span>mask<span class="token punctuation">)</span><span class="token punctuation">)</span>
image_colors <span class="token operator">=</span> ImageColorGenerator<span class="token punctuation">(</span>mask<span class="token punctuation">)</span>
words <span class="token operator">=</span> get_words<span class="token punctuation">(</span>jp_tokenizer<span class="token punctuation">.</span>word_counts<span class="token punctuation">)</span><span class="token punctuation">.</span>title<span class="token punctuation">(</span><span class="token punctuation">)</span>
wc <span class="token operator">=</span> WordCloud<span class="token punctuation">(</span>collocations<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span> background_color<span class="token operator">=</span><span class="token string">"white"</span><span class="token punctuation">,</span> mode<span class="token operator">=</span><span class="token string">"RGBA"</span><span class="token punctuation">,</span>
               max_words<span class="token operator">=</span><span class="token number">6000</span><span class="token punctuation">,</span> font_path<span class="token operator">=</span>font_path<span class="token punctuation">,</span> contour_width<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span>
               scale<span class="token operator">=</span><span class="token number">5</span><span class="token punctuation">,</span> max_font_size <span class="token operator">=</span> <span class="token number">50</span><span class="token punctuation">,</span> relative_scaling<span class="token operator">=</span><span class="token number">0.5</span><span class="token punctuation">,</span>
               random_state<span class="token operator">=</span><span class="token number">42</span><span class="token punctuation">,</span> width<span class="token operator">=</span>mask<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span> height<span class="token operator">=</span>mask<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
wc <span class="token operator">=</span> wc<span class="token punctuation">.</span>generate<span class="token punctuation">(</span>words<span class="token punctuation">)</span>
fig2<span class="token punctuation">,</span> ax2 <span class="token operator">=</span> plt<span class="token punctuation">.</span>subplots<span class="token punctuation">(</span>figsize<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">20</span><span class="token punctuation">,</span><span class="token number">15</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
ax2<span class="token punctuation">.</span>imshow<span class="token punctuation">(</span>wc<span class="token punctuation">.</span>recolor<span class="token punctuation">(</span>color_func<span class="token operator">=</span>image_colors<span class="token punctuation">)</span><span class="token punctuation">,</span> interpolation<span class="token operator">=</span><span class="token string">'bilinear'</span><span class="token punctuation">)</span>
ax2<span class="token punctuation">.</span>axis<span class="token punctuation">(</span><span class="token string">"off"</span><span class="token punctuation">)</span>

fig1<span class="token punctuation">.</span>savefig<span class="token punctuation">(</span><span class="token string">'WC_English.png'</span><span class="token punctuation">)</span>
fig2<span class="token punctuation">.</span>savefig<span class="token punctuation">(</span><span class="token string">'WC_Japanese.png'</span><span class="token punctuation">)</span>
plt<span class="token punctuation">.</span>close<span class="token punctuation">(</span>fig1<span class="token punctuation">)</span>
plt<span class="token punctuation">.</span>close<span class="token punctuation">(</span>fig2<span class="token punctuation">)</span>

!rm <span class="token operator">-</span>rf <span class="token punctuation">.</span><span class="token operator">/</span>font</code></pre></div>
<h2>WordCloud</h2>
<table>
<thead>
<tr>
<th align="center">English</th>
<th align="center">Japanesse</th>
</tr>
</thead>
<tbody>
<tr>
<td align="center"><span
      class="gatsby-resp-image-wrapper"
      style="position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 630px; "
    >
      <a
    class="gatsby-resp-image-link"
    href="/static/925d8b9db09014ceb7735de6722223fc/07a9c/WC_English.png"
    style="display: block"
    target="_blank"
    rel="noopener"
  >
    <span
    class="gatsby-resp-image-background-image"
    style="padding-bottom: 75.31645569620254%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAPCAYAAADkmO9VAAAACXBIWXMAAAsSAAALEgHS3X78AAACg0lEQVQ4y3WUe1PaUBDF/f5fojPtdOpM31qtDxQReQfQJAQSEhIICAQIT5WQhF8HrA62sH/cvbsze2bvPbtnj7+2XC75z7akdtlL/d5msFj4zD2PMAzx/YCQJU+TGaNml8Dz8D0P7/GR+fQB31sQLBYEfvBav/J7q+MlMRpPqVlNalYL+75Lr9Ul8/WcXCSNKWu0iiWMeA5XNbAydyjnSSYd5xVsDbjZYdsZYDXuKWsWomYhRFLkLrPkErfoQonCp2MK+7+pZ0TUsySp71fULYcw3NFhtzek1uhyq1hkZZPEWZqb4wSxwziVwyiZ9weopzd0FAMzXyZ/LWK2x+vv2QroDkbUzBaKZiOW60iKxfX+KZfvftLNSpg3ecq/rhjIGj3FQDmK0Tdbb5+8Ceg4LpJUparbFAsVBNEgmy2TjwrYxQpWPE/16IpZUWYmV3goyPTkCsFuUmbcNx3seoe61SZ9kSN+lECNCXRiWTo5kV6pSldSsW417EKZe6mynoqtgEN3REfRGZhNnHYfJZpDu0jzpFvoHw+QTxKMW11aJZ22VqcqlKnlxN1j8+AOcSSVodFgvGI8r9BMFQlVHf3DD5InWe4EjXHbwRCr2GqdiqjhBxuksAH4NJkyUHT6moXbaNOIC6T3zximb+l8OyEVKRCLyYjXRUoFDUPSMXSbxS6WZ30XRzMZmk06RpPI5yhnX2IYgkI9mka+SHOXlJHzJWqqQdOyMavmesO2Ak6cAbZSxVAMZMXk/EYikdcQFYuWM+Dh6ZHpbMZoPGYynTCdTXHd4RZSnqO1X+2vt1gwn88JA58g8J8LlkvCIFjf1w0s3wrKG3F4FZblboX5V402md1k+Q9clGHGV8oF9gAAAABJRU5ErkJggg=='); background-size: cover; display: block;"
  ></span>
  <img
        class="gatsby-resp-image-image"
        alt="wc-english"
        title="wc-english"
        src="/static/925d8b9db09014ceb7735de6722223fc/f058b/WC_English.png"
        srcset="/static/925d8b9db09014ceb7735de6722223fc/c26ae/WC_English.png 158w,
/static/925d8b9db09014ceb7735de6722223fc/6bdcf/WC_English.png 315w,
/static/925d8b9db09014ceb7735de6722223fc/f058b/WC_English.png 630w,
/static/925d8b9db09014ceb7735de6722223fc/40601/WC_English.png 945w,
/static/925d8b9db09014ceb7735de6722223fc/78612/WC_English.png 1260w,
/static/925d8b9db09014ceb7735de6722223fc/07a9c/WC_English.png 1440w"
        sizes="(max-width: 630px) 100vw, 630px"
        style="width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;"
        loading="lazy"
        decoding="async"
      />
  </a>
    </span></td>
<td align="center"><span
      class="gatsby-resp-image-wrapper"
      style="position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 630px; "
    >
      <a
    class="gatsby-resp-image-link"
    href="/static/97526e703f2d47eceebc07ee03968a2e/07a9c/WC_Japanese.png"
    style="display: block"
    target="_blank"
    rel="noopener"
  >
    <span
    class="gatsby-resp-image-background-image"
    style="padding-bottom: 75.31645569620254%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAPCAYAAADkmO9VAAAACXBIWXMAAAsSAAALEgHS3X78AAAA70lEQVQ4y62UDQuCMBiE/f+/LaikKCvsO/qiaE6duu1iA0Pi3VzQCzIQ9+yOuxmBGK31Z22f7nvfRKFAEk4cEPlgXahL8bfqfoVKkRtd9p3AdoPMS4jzHeX2ZNcmy0m1/ZalRMM4sukKbDjDazBFNllCHK+QpQi33H6gRIVicwSLE2TxwoL5fA02mqO6Pcmg/MC6QZHuweIFeLIBn6UWzMYJ6sfrd4UmDHG5W5Cxa5QZYJEeoKqabIQT+FHZSAswMJ6ska92NhiqTsEpa6lsOPWTQRals6+9ln1do8ofrtBz9XTI1etQSWDfTyLCn+cNu5Sfjc744QQAAAAASUVORK5CYII='); background-size: cover; display: block;"
  ></span>
  <img
        class="gatsby-resp-image-image"
        alt="wc-japanesse"
        title="wc-japanesse"
        src="/static/97526e703f2d47eceebc07ee03968a2e/f058b/WC_Japanese.png"
        srcset="/static/97526e703f2d47eceebc07ee03968a2e/c26ae/WC_Japanese.png 158w,
/static/97526e703f2d47eceebc07ee03968a2e/6bdcf/WC_Japanese.png 315w,
/static/97526e703f2d47eceebc07ee03968a2e/f058b/WC_Japanese.png 630w,
/static/97526e703f2d47eceebc07ee03968a2e/40601/WC_Japanese.png 945w,
/static/97526e703f2d47eceebc07ee03968a2e/78612/WC_Japanese.png 1260w,
/static/97526e703f2d47eceebc07ee03968a2e/07a9c/WC_Japanese.png 1440w"
        sizes="(max-width: 630px) 100vw, 630px"
        style="width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;"
        loading="lazy"
        decoding="async"
      />
  </a>
    </span></td>
</tr>
</tbody>
</table>
<p>now let‚Äôs transform our train sentences to sequences.</p>
<div class="gatsby-highlight" data-language="python"><pre class="language-python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">Sequences</span><span class="token punctuation">(</span>texts<span class="token punctuation">,</span> tokenizer<span class="token punctuation">)</span><span class="token punctuation">:</span>
    res <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
    <span class="token keyword">for</span> text <span class="token keyword">in</span> texts<span class="token punctuation">:</span>
        seq <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
        <span class="token keyword">for</span> w <span class="token keyword">in</span> text<span class="token punctuation">.</span>split<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
            <span class="token keyword">try</span><span class="token punctuation">:</span>
                seq<span class="token punctuation">.</span>append<span class="token punctuation">(</span>tokenizer<span class="token punctuation">.</span>word_index<span class="token punctuation">[</span>w<span class="token punctuation">]</span><span class="token punctuation">)</span>
            <span class="token keyword">except</span><span class="token punctuation">:</span>
                seq<span class="token punctuation">.</span>append<span class="token punctuation">(</span>tokenizer<span class="token punctuation">.</span>word_index<span class="token punctuation">[</span><span class="token string">'unk'</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
        res<span class="token punctuation">.</span>append<span class="token punctuation">(</span>seq<span class="token punctuation">)</span>
    <span class="token keyword">return</span> res</code></pre></div>
<div class="gatsby-highlight" data-language="python"><pre class="language-python"><code class="language-python"><span class="token comment"># Transform Sentences to Sequences</span>
data_en <span class="token operator">=</span> en_tokenizer<span class="token punctuation">.</span>texts_to_sequences<span class="token punctuation">(</span>eng_train<span class="token punctuation">)</span>
data_jp <span class="token operator">=</span> jp_tokenizer<span class="token punctuation">.</span>texts_to_sequences<span class="token punctuation">(</span>jpn_train<span class="token punctuation">)</span>

val_en <span class="token operator">=</span> Sequences<span class="token punctuation">(</span>eng_val<span class="token punctuation">,</span> en_tokenizer<span class="token punctuation">)</span>
val_jp <span class="token operator">=</span> Sequences<span class="token punctuation">(</span>jpn_val<span class="token punctuation">,</span> jp_tokenizer<span class="token punctuation">)</span></code></pre></div>
<div class="gatsby-highlight" data-language="python"><pre class="language-python"><code class="language-python">plt<span class="token punctuation">.</span>figure<span class="token punctuation">(</span>figsize <span class="token operator">=</span> <span class="token punctuation">(</span><span class="token number">8</span><span class="token punctuation">,</span><span class="token number">8</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
sns<span class="token punctuation">.</span>distplot<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token builtin">len</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span> <span class="token keyword">for</span> x <span class="token keyword">in</span> data_en<span class="token punctuation">]</span><span class="token punctuation">,</span> label<span class="token operator">=</span><span class="token string">'English'</span><span class="token punctuation">)</span>
sns<span class="token punctuation">.</span>distplot<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token builtin">len</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span> <span class="token keyword">for</span> x <span class="token keyword">in</span> data_jp<span class="token punctuation">]</span><span class="token punctuation">,</span> label<span class="token operator">=</span><span class="token string">'Japanese'</span><span class="token punctuation">)</span>
plt<span class="token punctuation">.</span>title<span class="token punctuation">(</span><span class="token string">'Distribution of Sentences Length'</span><span class="token punctuation">)</span>
plt<span class="token punctuation">.</span>legend<span class="token punctuation">(</span><span class="token punctuation">)</span>
plt<span class="token punctuation">.</span>show<span class="token punctuation">(</span><span class="token punctuation">)</span></code></pre></div>
<p><span
      class="gatsby-resp-image-wrapper"
      style="position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 490px; "
    >
      <a
    class="gatsby-resp-image-link"
    href="/static/8ce63a24a5b8ffbe5487aa1074fa8a20/41d3c/output_32_0.png"
    style="display: block"
    target="_blank"
    rel="noopener"
  >
    <span
    class="gatsby-resp-image-background-image"
    style="padding-bottom: 98.10126582278481%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAUCAYAAACNiR0NAAAACXBIWXMAAAsSAAALEgHS3X78AAACwklEQVQ4y6WUW08TQRTH91UTL/XViES8pRD7RELDEwnhkY/gix/AJx8MGm+BGCWR1jbGxAh4ibEQiAoJIBcxxkBSwYhEpCz0Srud3XZ3u213ZnbnmG5boSGGFs/m7Ewyk9/+/2d2DudwODiv13PY5XKf8HoeH3e7+mwDg4PHOjs765ubmx1Op7Opo6OjoaWlpam1tdXe3t5+zul0Nra1tZ33+XxHHz3os3lcHtvAQL/N6/Ue4QCAEwThpWEYlFKKKKWSYRgSxiSt67qSz+dVjLFcnu8aZdMwJCWHU/EkQps8ryOE5iygLMujAADEBCAG1BR5E4OkpCEWjQJC6DsXCAQ4NZN5C8BAp0DylJkAzGTMyv3CBAYmAFBLECH+ssLhAjBPgKZyjBUWC+/ibG8wK4sbrIcxy5eu698sIEKiZVnOM4q0/YHFdfZ3rABGIhFOVTO+wjejCtCwvAOsNiqAsViMU0rAtSTQVaEMZFVD/2n5N2J0KWYVu8JWzcB0unAoACsJRr+GypwDKuR5vmQZwL+N6VxY+3+FqXTaUriU0OloKMpyJtl9grUDkwhZNVyVsnQkFGYyze0AoUZgMBjkFFW1LP9UEB2J8mxdUQ6uMJtROVVVhgoXblEU6LvtDeYXJQAwSz/3/tA9liVJHKUmwLwYouOJDfZJiANh1ddxD1BVlCFsmDAvBsmEsMEm4mGGaNq6stV0CMYYLQH9XGwzyOnZ3LBKMUwmeTYt8jApbMGKtg0AtfUyjPFysduIqTdbmggf4r+UycS6NpUIaFPxrcwPJaJJuqpliZ7BFGuYEo0YRCO0MjElKjUMls1lv3Acxx3qutF15vn0WMOT2ff1V3u7G+8Oui+89s/UXe7pbrxyv9f+bGHh1DXvHfvtpz32j2uf63pfuC7ect9rGl+eOt0//urs9Yc3L40tTpyZmZ89+Qdd/6OoUfpEpAAAAABJRU5ErkJggg=='); background-size: cover; display: block;"
  ></span>
  <img
        class="gatsby-resp-image-image"
        alt="png"
        title="png"
        src="/static/8ce63a24a5b8ffbe5487aa1074fa8a20/41d3c/output_32_0.png"
        srcset="/static/8ce63a24a5b8ffbe5487aa1074fa8a20/c26ae/output_32_0.png 158w,
/static/8ce63a24a5b8ffbe5487aa1074fa8a20/6bdcf/output_32_0.png 315w,
/static/8ce63a24a5b8ffbe5487aa1074fa8a20/41d3c/output_32_0.png 490w"
        sizes="(max-width: 490px) 100vw, 490px"
        style="width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;"
        loading="lazy"
        decoding="async"
      />
  </a>
    </span></p>
<p>based on the distplot English sentences contains about 20 - 40 words while Japanese have more wider range.</p>
<p>Let‚Äôs check their max length</p>
<div class="gatsby-highlight" data-language="python"><pre class="language-python"><code class="language-python">max_en <span class="token operator">=</span> <span class="token builtin">max</span><span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token builtin">len</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span> <span class="token keyword">for</span> x <span class="token keyword">in</span> data_en<span class="token punctuation">]</span> <span class="token operator">+</span> <span class="token punctuation">[</span><span class="token builtin">len</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span> <span class="token keyword">for</span> x <span class="token keyword">in</span> val_en<span class="token punctuation">]</span><span class="token punctuation">)</span>
max_jp <span class="token operator">=</span> <span class="token builtin">max</span><span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token builtin">len</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span> <span class="token keyword">for</span> x <span class="token keyword">in</span> data_jp<span class="token punctuation">]</span> <span class="token operator">+</span> <span class="token punctuation">[</span><span class="token builtin">len</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span> <span class="token keyword">for</span> x <span class="token keyword">in</span> val_jp<span class="token punctuation">]</span><span class="token punctuation">)</span>

<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f'Maximum length of English sequences is  </span><span class="token interpolation"><span class="token punctuation">{</span>max_en<span class="token punctuation">}</span></span><span class="token string">'</span></span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f'Maximum length of Japanese sequences is </span><span class="token interpolation"><span class="token punctuation">{</span>max_jp<span class="token punctuation">}</span></span><span class="token string">'</span></span><span class="token punctuation">)</span></code></pre></div>
<div class="gatsby-highlight" data-language="text"><pre class="language-text"><code class="language-text">Maximum length of English sequences is  49
Maximum length of Japanese sequences is 54</code></pre></div>
<div class="gatsby-highlight" data-language="python"><pre class="language-python"><code class="language-python"><span class="token comment"># Padding Sequences</span>
data_en <span class="token operator">=</span> pad_sequences<span class="token punctuation">(</span>data_en<span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token string">'post'</span><span class="token punctuation">,</span> maxlen <span class="token operator">=</span> max_en<span class="token punctuation">)</span>
data_jp <span class="token operator">=</span> pad_sequences<span class="token punctuation">(</span>data_jp<span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token string">'post'</span><span class="token punctuation">,</span> maxlen <span class="token operator">=</span> max_jp<span class="token punctuation">)</span>

val_en <span class="token operator">=</span> pad_sequences<span class="token punctuation">(</span>val_en<span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token string">'post'</span><span class="token punctuation">,</span> maxlen <span class="token operator">=</span> max_en<span class="token punctuation">)</span>
val_jp <span class="token operator">=</span> pad_sequences<span class="token punctuation">(</span>val_jp<span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token string">'post'</span><span class="token punctuation">,</span> maxlen <span class="token operator">=</span> max_jp<span class="token punctuation">)</span></code></pre></div>
<h1>Build &#x26; Train Model</h1>
<p>Now it‚Äôs the time brace yourself.</p>
<p>We‚Äôll build model based on Seq2seq approaches with Attention optimization.</p>
<blockquote>
<p>Seq2Seq is a method of encoder-decoder based machine translation that maps an input of sequence to an output of sequence with a tag and attention value. The idea is to use 2 RNN that will work together with a special token and trying to predict the next state sequence from the previous sequence.</p>
</blockquote>
<p><span
      class="gatsby-resp-image-wrapper"
      style="position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 630px; "
    >
      <a
    class="gatsby-resp-image-link"
    href="/static/a5a04f4dc28fd14e1a9c5e883fda9bf5/5a190/seq2seq.png"
    style="display: block"
    target="_blank"
    rel="noopener"
  >
    <span
    class="gatsby-resp-image-background-image"
    style="padding-bottom: 50.632911392405056%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAKCAIAAAA7N+mxAAAACXBIWXMAAAsTAAALEwEAmpwYAAABeElEQVQoz22R2W7bMBBF/f/fE+SlD0WA5K1tHFuKbS22ZWqjJNJcRdLm0ipKUBvtATHALBd3wFmEe5xzWZZRRkMIPvi56P38fJJkjPGrdeZiQwiL8A/GGGvtbeViOCdgFDBPl+xcNtWW4PL/4nsmc85AWz6t3x6T7bf18gHWzwTHzrnF/dhNvEk73P1cv5awXm3X2SmP0vf0mM/OU59JUrRpj9uyO3S4rvqi7k8dagDMjdFEqAPE7YD3LWoGfGj6lohJ7P0k7s4wLn5lRZI28b5KdyDagTgDyaZcEYorRHZIRvkxxeOmqN7rYY+4s/bTWY2kKlejKE/Fsu9SAKIe7jg51lXsvdXmAvtByBEOSEjZDUgq9Xdto3qBXzWPweEF7F+yzXfU/tAs4ujNXrX3nlIy73jL54dZ61qIrAuYsKu1Uiou9OXqhdTz8b337gP7xSQ2xgAAKKWMETXK8xkLwTljlJJRCsqotVZrzRj7iFRKKYRQSv051W8r8je8hZRFFgAAAABJRU5ErkJggg=='); background-size: cover; display: block;"
  ></span>
  <img
        class="gatsby-resp-image-image"
        alt="Seq2Seq"
        title="Seq2Seq"
        src="/static/a5a04f4dc28fd14e1a9c5e883fda9bf5/f058b/seq2seq.png"
        srcset="/static/a5a04f4dc28fd14e1a9c5e883fda9bf5/c26ae/seq2seq.png 158w,
/static/a5a04f4dc28fd14e1a9c5e883fda9bf5/6bdcf/seq2seq.png 315w,
/static/a5a04f4dc28fd14e1a9c5e883fda9bf5/f058b/seq2seq.png 630w,
/static/a5a04f4dc28fd14e1a9c5e883fda9bf5/5a190/seq2seq.png 800w"
        sizes="(max-width: 630px) 100vw, 630px"
        style="width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;"
        loading="lazy"
        decoding="async"
      />
  </a>
    </span></p>
<div class="gatsby-highlight" data-language="python"><pre class="language-python"><code class="language-python"><span class="token comment"># Config</span>
epochs <span class="token operator">=</span> <span class="token number">7</span>
BATCH_SIZE <span class="token operator">=</span> <span class="token number">64</span>
BUFFER_SIZE <span class="token operator">=</span> <span class="token builtin">len</span><span class="token punctuation">(</span>data_jp<span class="token punctuation">)</span>
steps_per_epoch <span class="token operator">=</span> BUFFER_SIZE<span class="token operator">//</span>BATCH_SIZE
val_steps_per_epoch <span class="token operator">=</span> <span class="token builtin">len</span><span class="token punctuation">(</span>val_jp<span class="token punctuation">)</span> <span class="token operator">//</span> BATCH_SIZE
embedding_dims <span class="token operator">=</span> <span class="token number">256</span>
rnn_units <span class="token operator">=</span> <span class="token number">1024</span>
dense_units <span class="token operator">=</span> <span class="token number">1024</span>
Dtype <span class="token operator">=</span> tf<span class="token punctuation">.</span>float32</code></pre></div>
<div class="gatsby-highlight" data-language="python"><pre class="language-python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">max_len</span><span class="token punctuation">(</span>tensor<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">"""
    Get max len in Sequences
    """</span>
    <span class="token keyword">return</span> <span class="token builtin">max</span><span class="token punctuation">(</span> <span class="token builtin">len</span><span class="token punctuation">(</span>t<span class="token punctuation">)</span> <span class="token keyword">for</span> t <span class="token keyword">in</span> tensor<span class="token punctuation">)</span></code></pre></div>
<div class="gatsby-highlight" data-language="python"><pre class="language-python"><code class="language-python"><span class="token comment"># Max Len</span>
Tx <span class="token operator">=</span> max_len<span class="token punctuation">(</span>data_en<span class="token punctuation">)</span>
Ty <span class="token operator">=</span> max_len<span class="token punctuation">(</span>data_jp<span class="token punctuation">)</span>

<span class="token comment"># Vocab</span>
input_vocab_size <span class="token operator">=</span> <span class="token builtin">len</span><span class="token punctuation">(</span>en_tokenizer<span class="token punctuation">.</span>word_index<span class="token punctuation">)</span> <span class="token operator">+</span> <span class="token number">1</span>   <span class="token comment"># English</span>
output_vocab_size <span class="token operator">=</span> <span class="token builtin">len</span><span class="token punctuation">(</span>jp_tokenizer<span class="token punctuation">.</span>word_index<span class="token punctuation">)</span> <span class="token operator">+</span> <span class="token number">1</span>  <span class="token comment"># Japanese</span>

<span class="token comment"># Changing to TF data</span>
dataset <span class="token operator">=</span> <span class="token punctuation">(</span>tf<span class="token punctuation">.</span>data<span class="token punctuation">.</span>Dataset<span class="token punctuation">.</span>from_tensor_slices<span class="token punctuation">(</span><span class="token punctuation">(</span>data_en<span class="token punctuation">,</span> data_jp<span class="token punctuation">)</span><span class="token punctuation">)</span>
           <span class="token punctuation">.</span>shuffle<span class="token punctuation">(</span>BUFFER_SIZE<span class="token punctuation">)</span>
           <span class="token punctuation">.</span>batch<span class="token punctuation">(</span>BATCH_SIZE<span class="token punctuation">,</span> drop_remainder<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
          <span class="token punctuation">)</span>

val_dataset <span class="token operator">=</span> <span class="token punctuation">(</span>tf<span class="token punctuation">.</span>data<span class="token punctuation">.</span>Dataset<span class="token punctuation">.</span>from_tensor_slices<span class="token punctuation">(</span><span class="token punctuation">(</span>val_en<span class="token punctuation">,</span> val_jp<span class="token punctuation">)</span><span class="token punctuation">)</span>
               <span class="token punctuation">.</span>batch<span class="token punctuation">(</span>BATCH_SIZE<span class="token punctuation">)</span>
              <span class="token punctuation">)</span></code></pre></div>
<p>Let‚Äôs define our based Seq2Seq Model</p>
<div class="gatsby-highlight" data-language="python"><pre class="language-python"><code class="language-python"><span class="token comment">#ENCODER</span>
<span class="token keyword">class</span> <span class="token class-name">EncoderNetwork</span><span class="token punctuation">(</span>tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>Model<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span>input_vocab_size<span class="token punctuation">,</span>embedding_dims<span class="token punctuation">,</span> rnn_units <span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>encoder_embedding <span class="token operator">=</span> Embedding<span class="token punctuation">(</span>input_dim<span class="token operator">=</span>input_vocab_size<span class="token punctuation">,</span>
                                           output_dim<span class="token operator">=</span>embedding_dims<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>encoder_rnnlayer <span class="token operator">=</span> LSTM<span class="token punctuation">(</span>rnn_units<span class="token punctuation">,</span>return_sequences<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span>
                                     return_state<span class="token operator">=</span><span class="token boolean">True</span> <span class="token punctuation">)</span>

<span class="token comment">#DECODER</span>
<span class="token keyword">class</span> <span class="token class-name">DecoderNetwork</span><span class="token punctuation">(</span>tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>Model<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span>output_vocab_size<span class="token punctuation">,</span> embedding_dims<span class="token punctuation">,</span> rnn_units<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>decoder_embedding <span class="token operator">=</span> Embedding<span class="token punctuation">(</span>input_dim<span class="token operator">=</span>output_vocab_size<span class="token punctuation">,</span>
                                           output_dim<span class="token operator">=</span>embedding_dims<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>dense_layer <span class="token operator">=</span> Dense<span class="token punctuation">(</span>output_vocab_size<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>decoder_rnncell <span class="token operator">=</span> LSTMCell<span class="token punctuation">(</span>rnn_units<span class="token punctuation">)</span>
        <span class="token comment"># Sampler</span>
        self<span class="token punctuation">.</span>sampler <span class="token operator">=</span> tfa<span class="token punctuation">.</span>seq2seq<span class="token punctuation">.</span>sampler<span class="token punctuation">.</span>TrainingSampler<span class="token punctuation">(</span><span class="token punctuation">)</span>
        <span class="token comment"># Create attention mechanism with memory = None</span>
        self<span class="token punctuation">.</span>attention_mechanism <span class="token operator">=</span> \
            self<span class="token punctuation">.</span>build_attention_mechanism<span class="token punctuation">(</span>dense_units<span class="token punctuation">,</span><span class="token boolean">None</span><span class="token punctuation">,</span>BATCH_SIZE<span class="token operator">*</span><span class="token punctuation">[</span>Tx<span class="token punctuation">]</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>rnn_cell <span class="token operator">=</span> self<span class="token punctuation">.</span>build_rnn_cell<span class="token punctuation">(</span>BATCH_SIZE<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>decoder <span class="token operator">=</span> tfa<span class="token punctuation">.</span>seq2seq<span class="token punctuation">.</span>BasicDecoder<span class="token punctuation">(</span>self<span class="token punctuation">.</span>rnn_cell<span class="token punctuation">,</span>
                                                sampler<span class="token operator">=</span> self<span class="token punctuation">.</span>sampler<span class="token punctuation">,</span>
                                                output_layer <span class="token operator">=</span> self<span class="token punctuation">.</span>dense_layer
                                               <span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">build_attention_mechanism</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> units<span class="token punctuation">,</span> memory<span class="token punctuation">,</span> MSL<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token triple-quoted-string string">"""
        MSL : Memory Sequence Length
        """</span>
        <span class="token comment">#return tfa.seq2seq.LuongAttention(units, memory = memory,</span>
        <span class="token comment">#                                  memory_sequence_length = MSL)</span>
        <span class="token keyword">return</span> tfa<span class="token punctuation">.</span>seq2seq<span class="token punctuation">.</span>BahdanauAttention<span class="token punctuation">(</span>units<span class="token punctuation">,</span> memory <span class="token operator">=</span> memory<span class="token punctuation">,</span>
                                             memory_sequence_length <span class="token operator">=</span> MSL<span class="token punctuation">)</span>

    <span class="token comment"># wrap decodernn cell</span>
    <span class="token keyword">def</span> <span class="token function">build_rnn_cell</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> batch_size<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">return</span> tfa<span class="token punctuation">.</span>seq2seq<span class="token punctuation">.</span>AttentionWrapper<span class="token punctuation">(</span>self<span class="token punctuation">.</span>decoder_rnncell<span class="token punctuation">,</span>
                                            self<span class="token punctuation">.</span>attention_mechanism<span class="token punctuation">,</span>
                                            attention_layer_size<span class="token operator">=</span>dense_units<span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">build_decoder_initial_state</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> batch_size<span class="token punctuation">,</span> encoder_state<span class="token punctuation">,</span> Dtype<span class="token punctuation">)</span><span class="token punctuation">:</span>
        decoder_initial_state <span class="token operator">=</span> self<span class="token punctuation">.</span>rnn_cell<span class="token punctuation">.</span>get_initial_state<span class="token punctuation">(</span>batch_size <span class="token operator">=</span> batch_size<span class="token punctuation">,</span>
                                                                dtype <span class="token operator">=</span> Dtype<span class="token punctuation">)</span>
        decoder_initial_state <span class="token operator">=</span> decoder_initial_state<span class="token punctuation">.</span>clone<span class="token punctuation">(</span>cell_state <span class="token operator">=</span> encoder_state<span class="token punctuation">)</span>
        <span class="token keyword">return</span> decoder_initial_state</code></pre></div>
<div class="gatsby-highlight" data-language="python"><pre class="language-python"><code class="language-python"><span class="token comment"># Build Model</span>
encoderNetwork <span class="token operator">=</span> EncoderNetwork<span class="token punctuation">(</span>input_vocab_size<span class="token punctuation">,</span> embedding_dims<span class="token punctuation">,</span> rnn_units<span class="token punctuation">)</span>
decoderNetwork <span class="token operator">=</span> DecoderNetwork<span class="token punctuation">(</span>output_vocab_size<span class="token punctuation">,</span> embedding_dims<span class="token punctuation">,</span> rnn_units<span class="token punctuation">)</span>

<span class="token comment"># Optimizer</span>
optimizer <span class="token operator">=</span> tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>optimizers<span class="token punctuation">.</span>Adam<span class="token punctuation">(</span><span class="token punctuation">)</span></code></pre></div>
<p>Make custom training loop</p>
<div class="gatsby-highlight" data-language="python"><pre class="language-python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">loss_function</span><span class="token punctuation">(</span>y_pred<span class="token punctuation">,</span> y<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token comment">#shape of y [batch_size, ty]</span>
    <span class="token comment">#shape of y_pred [batch_size, Ty, output_vocab_size]</span>
    sparsecategoricalcrossentropy <span class="token operator">=</span> SparseCategoricalCrossentropy<span class="token punctuation">(</span>from_logits<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span>
                                                                  reduction<span class="token operator">=</span><span class="token string">'none'</span><span class="token punctuation">)</span>
    loss <span class="token operator">=</span> sparsecategoricalcrossentropy<span class="token punctuation">(</span>y_true<span class="token operator">=</span>y<span class="token punctuation">,</span> y_pred<span class="token operator">=</span>y_pred<span class="token punctuation">)</span>
    mask <span class="token operator">=</span> tf<span class="token punctuation">.</span>logical_not<span class="token punctuation">(</span>tf<span class="token punctuation">.</span>math<span class="token punctuation">.</span>equal<span class="token punctuation">(</span>y<span class="token punctuation">,</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">)</span>   <span class="token comment">#output 0 for y=0 else output 1</span>
    mask <span class="token operator">=</span> tf<span class="token punctuation">.</span>cast<span class="token punctuation">(</span>mask<span class="token punctuation">,</span> dtype<span class="token operator">=</span>loss<span class="token punctuation">.</span>dtype<span class="token punctuation">)</span>
    loss <span class="token operator">=</span> mask <span class="token operator">*</span> loss
    loss <span class="token operator">=</span> tf<span class="token punctuation">.</span>reduce_mean<span class="token punctuation">(</span>loss<span class="token punctuation">)</span>
    <span class="token keyword">return</span> loss

<span class="token decorator annotation punctuation">@tf<span class="token punctuation">.</span>function</span>
<span class="token keyword">def</span> <span class="token function">train_step</span><span class="token punctuation">(</span>input_batch<span class="token punctuation">,</span> output_batch<span class="token punctuation">,</span> encoder_initial_cell_state<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token comment"># initialize loss = 0</span>
    loss <span class="token operator">=</span> <span class="token number">0</span>
    <span class="token keyword">with</span> tf<span class="token punctuation">.</span>GradientTape<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">as</span> tape<span class="token punctuation">:</span>
        encoder_emb_inp <span class="token operator">=</span> encoderNetwork<span class="token punctuation">.</span>encoder_embedding<span class="token punctuation">(</span>input_batch<span class="token punctuation">)</span>
        a<span class="token punctuation">,</span> a_tx<span class="token punctuation">,</span> c_tx <span class="token operator">=</span> encoderNetwork<span class="token punctuation">.</span>encoder_rnnlayer<span class="token punctuation">(</span>encoder_emb_inp<span class="token punctuation">,</span>
                                                        initial_state <span class="token operator">=</span> encoder_initial_cell_state<span class="token punctuation">)</span>

        <span class="token comment"># [last step activations,last memory_state] of</span>
        <span class="token comment"># encoder passed as input to decoder Network</span>

        <span class="token comment"># Prepare correct Decoder input &amp; output sequence data</span>
        decoder_input <span class="token operator">=</span> output_batch<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span><span class="token punctuation">:</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span> <span class="token comment"># ignore eos</span>
        <span class="token comment"># compare logits with timestepped +1 version of decoder_input</span>
        decoder_output <span class="token operator">=</span> output_batch<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">:</span><span class="token punctuation">]</span> <span class="token comment">#ignore bos</span>

        <span class="token comment"># Decoder Embeddings</span>
        decoder_emb_inp <span class="token operator">=</span> decoderNetwork<span class="token punctuation">.</span>decoder_embedding<span class="token punctuation">(</span>decoder_input<span class="token punctuation">)</span>

        <span class="token comment"># Setting up decoder memory from encoder output</span>
        <span class="token comment"># and Zero State for AttentionWrapperState</span>
        decoderNetwork<span class="token punctuation">.</span>attention_mechanism<span class="token punctuation">.</span>setup_memory<span class="token punctuation">(</span>a<span class="token punctuation">)</span>
        decoder_initial_state <span class="token operator">=</span> decoderNetwork<span class="token punctuation">.</span>build_decoder_initial_state<span class="token punctuation">(</span>BATCH_SIZE<span class="token punctuation">,</span>
                                                                           encoder_state<span class="token operator">=</span><span class="token punctuation">[</span>a_tx<span class="token punctuation">,</span> c_tx<span class="token punctuation">]</span><span class="token punctuation">,</span>
                                                                           Dtype<span class="token operator">=</span>tf<span class="token punctuation">.</span>float32<span class="token punctuation">)</span>

        <span class="token comment"># BasicDecoderOutput</span>
        outputs<span class="token punctuation">,</span> _<span class="token punctuation">,</span> _ <span class="token operator">=</span> decoderNetwork<span class="token punctuation">.</span>decoder<span class="token punctuation">(</span>decoder_emb_inp<span class="token punctuation">,</span>initial_state<span class="token operator">=</span>decoder_initial_state<span class="token punctuation">,</span>
                                               sequence_length<span class="token operator">=</span>BATCH_SIZE<span class="token operator">*</span><span class="token punctuation">[</span>Ty<span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span>

        logits <span class="token operator">=</span> outputs<span class="token punctuation">.</span>rnn_output

        <span class="token comment"># Calculate loss</span>
        loss <span class="token operator">=</span> loss_function<span class="token punctuation">(</span>logits<span class="token punctuation">,</span> decoder_output<span class="token punctuation">)</span>

    <span class="token comment"># Returns the list of all layer variables / weights.</span>
    variables <span class="token operator">=</span> encoderNetwork<span class="token punctuation">.</span>trainable_variables <span class="token operator">+</span> decoderNetwork<span class="token punctuation">.</span>trainable_variables
    <span class="token comment"># differentiate loss wrt variables</span>
    gradients <span class="token operator">=</span> tape<span class="token punctuation">.</span>gradient<span class="token punctuation">(</span>loss<span class="token punctuation">,</span> variables<span class="token punctuation">)</span>

    <span class="token comment"># grads_and_vars ‚Äì List of(gradient, variable) pairs.</span>
    grads_and_vars <span class="token operator">=</span> <span class="token builtin">zip</span><span class="token punctuation">(</span>gradients<span class="token punctuation">,</span>variables<span class="token punctuation">)</span>
    optimizer<span class="token punctuation">.</span>apply_gradients<span class="token punctuation">(</span>grads_and_vars<span class="token punctuation">)</span>
    <span class="token keyword">return</span> loss

<span class="token decorator annotation punctuation">@tf<span class="token punctuation">.</span>function</span>
<span class="token keyword">def</span> <span class="token function">evaluate</span><span class="token punctuation">(</span>input_batch<span class="token punctuation">,</span> output_batch<span class="token punctuation">,</span> encoder_initial_cell_state<span class="token punctuation">)</span><span class="token punctuation">:</span>
    loss <span class="token operator">=</span> <span class="token number">0</span>
    encoder_emb_inp <span class="token operator">=</span> encoderNetwork<span class="token punctuation">.</span>encoder_embedding<span class="token punctuation">(</span>input_batch<span class="token punctuation">)</span>
    a<span class="token punctuation">,</span> a_tx<span class="token punctuation">,</span> c_tx <span class="token operator">=</span> encoderNetwork<span class="token punctuation">.</span>encoder_rnnlayer<span class="token punctuation">(</span>encoder_emb_inp<span class="token punctuation">,</span>
                                                    initial_state <span class="token operator">=</span>encoder_initial_cell_state<span class="token punctuation">)</span>
    decoder_input <span class="token operator">=</span> output_batch<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span><span class="token punctuation">:</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span>
    decoder_output <span class="token operator">=</span> output_batch<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">:</span><span class="token punctuation">]</span>
    decoder_emb_inp <span class="token operator">=</span> decoderNetwork<span class="token punctuation">.</span>decoder_embedding<span class="token punctuation">(</span>decoder_input<span class="token punctuation">)</span>
    decoderNetwork<span class="token punctuation">.</span>attention_mechanism<span class="token punctuation">.</span>setup_memory<span class="token punctuation">(</span>a<span class="token punctuation">)</span>
    decoder_initial_state <span class="token operator">=</span> decoderNetwork<span class="token punctuation">.</span>build_decoder_initial_state<span class="token punctuation">(</span>BATCH_SIZE<span class="token punctuation">,</span>
                                                                       encoder_state<span class="token operator">=</span><span class="token punctuation">[</span>a_tx<span class="token punctuation">,</span> c_tx<span class="token punctuation">]</span><span class="token punctuation">,</span>
                                                                       Dtype<span class="token operator">=</span>tf<span class="token punctuation">.</span>float32<span class="token punctuation">)</span>
    outputs<span class="token punctuation">,</span> _<span class="token punctuation">,</span> _ <span class="token operator">=</span> decoderNetwork<span class="token punctuation">.</span>decoder<span class="token punctuation">(</span>decoder_emb_inp<span class="token punctuation">,</span>initial_state<span class="token operator">=</span>decoder_initial_state<span class="token punctuation">,</span>
                                           sequence_length<span class="token operator">=</span>BATCH_SIZE<span class="token operator">*</span><span class="token punctuation">[</span>Ty<span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
    logits <span class="token operator">=</span> outputs<span class="token punctuation">.</span>rnn_output
    loss <span class="token operator">=</span> loss_function<span class="token punctuation">(</span>logits<span class="token punctuation">,</span> decoder_output<span class="token punctuation">)</span>
    <span class="token keyword">return</span> loss</code></pre></div>
<div class="gatsby-highlight" data-language="python"><pre class="language-python"><code class="language-python"><span class="token comment"># RNN LSTM hidden and memory state initializer</span>
<span class="token keyword">def</span> <span class="token function">initialize_initial_state</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">return</span> <span class="token punctuation">[</span>tf<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span><span class="token punctuation">(</span>BATCH_SIZE<span class="token punctuation">,</span> rnn_units<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span> tf<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span><span class="token punctuation">(</span>BATCH_SIZE<span class="token punctuation">,</span> rnn_units<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">]</span></code></pre></div>
<h1>Calculating BLEU Score</h1>
<blockquote>
<p>BLEU (bilingual evaluation understudy) is an algorithm for evaluating the quality of text which has been machine-translated from one natural language to another. Quality is considered to be the correspondence between a machine‚Äôs output and that of a human: ‚Äúthe closer a machine translation is to a professional human translation, the better it is‚Äù. - Wikipedia</p>
</blockquote>
<p>BLEU is a metric for evaluating a generated sentence to a reference sentence.
A perfect match results in a score of 1.0, whereas a perfect mismatch results in a score of 0.0.</p>
<p>So now we‚Äôre going to define our translation function &#x26; calculate the BLEU score for test data at the end of every epoch while training. Note that test data is sentences that our tokenizer didn‚Äôt train with, so there must be some words that our tokenizer didn‚Äôt know.
I‚Äôm currently working to fix this issue. Based on keras Tokenizer API it have <code class="language-text">oov_token</code> for handling this but i‚Äôm not sure.</p>
<p>For now i‚Äôm handling this by adding <code class="language-text">unk</code> in train dataset so the tokenizer can read it, and then when coming to translation if there is word that our tokenizer don‚Äôt know i‚Äôll set it by index of <code class="language-text">unk</code> not very eficient but it works.</p>
<div class="gatsby-highlight" data-language="python"><pre class="language-python"><code class="language-python"><span class="token comment"># Translate</span>
<span class="token keyword">def</span> <span class="token function">Translate</span><span class="token punctuation">(</span>input_raw<span class="token punctuation">)</span><span class="token punctuation">:</span>
    input_raw <span class="token operator">=</span> CP<span class="token punctuation">(</span>preprocess<span class="token punctuation">(</span>input_raw<span class="token punctuation">)</span><span class="token punctuation">)</span>
    input_lines <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token string">'bos '</span><span class="token operator">+</span> input_raw <span class="token operator">+</span> <span class="token string">''</span><span class="token punctuation">]</span>

    input_sequences<span class="token punctuation">,</span> unique <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
    <span class="token keyword">for</span> line <span class="token keyword">in</span> input_lines<span class="token punctuation">:</span>
        temp <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
        <span class="token keyword">for</span> w <span class="token keyword">in</span> line<span class="token punctuation">.</span>split<span class="token punctuation">(</span><span class="token string">' '</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
            <span class="token keyword">try</span><span class="token punctuation">:</span>
                temp<span class="token punctuation">.</span>append<span class="token punctuation">(</span>en_tokenizer<span class="token punctuation">.</span>word_index<span class="token punctuation">[</span>w<span class="token punctuation">]</span><span class="token punctuation">)</span>
            <span class="token keyword">except</span><span class="token punctuation">:</span> <span class="token comment"># Avoid Error</span>
                unique<span class="token punctuation">.</span>append<span class="token punctuation">(</span>w<span class="token punctuation">)</span>
                temp<span class="token punctuation">.</span>append<span class="token punctuation">(</span>en_tokenizer<span class="token punctuation">.</span>word_index<span class="token punctuation">[</span><span class="token string">'unk'</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
        input_sequences<span class="token punctuation">.</span>append<span class="token punctuation">(</span>temp<span class="token punctuation">)</span>

    input_sequences <span class="token operator">=</span> pad_sequences<span class="token punctuation">(</span>input_sequences<span class="token punctuation">,</span> maxlen<span class="token operator">=</span>Tx<span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token string">'post'</span><span class="token punctuation">)</span>
    inp <span class="token operator">=</span> tf<span class="token punctuation">.</span>convert_to_tensor<span class="token punctuation">(</span>input_sequences<span class="token punctuation">)</span>
    inference_batch_size <span class="token operator">=</span> input_sequences<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>
    encoder_initial_cell_state <span class="token operator">=</span> <span class="token punctuation">[</span>tf<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span><span class="token punctuation">(</span>inference_batch_size<span class="token punctuation">,</span> rnn_units<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
                                  tf<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span><span class="token punctuation">(</span>inference_batch_size<span class="token punctuation">,</span> rnn_units<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">]</span>
    encoder_emb_inp <span class="token operator">=</span> encoderNetwork<span class="token punctuation">.</span>encoder_embedding<span class="token punctuation">(</span>inp<span class="token punctuation">)</span>
    a<span class="token punctuation">,</span> a_tx<span class="token punctuation">,</span> c_tx <span class="token operator">=</span> encoderNetwork<span class="token punctuation">.</span>encoder_rnnlayer<span class="token punctuation">(</span>encoder_emb_inp<span class="token punctuation">,</span>
                                                    initial_state <span class="token operator">=</span> encoder_initial_cell_state<span class="token punctuation">)</span>

    start_tokens <span class="token operator">=</span> tf<span class="token punctuation">.</span>fill<span class="token punctuation">(</span><span class="token punctuation">[</span>inference_batch_size<span class="token punctuation">]</span><span class="token punctuation">,</span> jp_tokenizer<span class="token punctuation">.</span>word_index<span class="token punctuation">[</span><span class="token string">'bos'</span><span class="token punctuation">]</span><span class="token punctuation">)</span>

    end_token <span class="token operator">=</span> jp_tokenizer<span class="token punctuation">.</span>word_index<span class="token punctuation">[</span><span class="token string">'eos'</span><span class="token punctuation">]</span>

    greedy_sampler <span class="token operator">=</span> tfa<span class="token punctuation">.</span>seq2seq<span class="token punctuation">.</span>GreedyEmbeddingSampler<span class="token punctuation">(</span><span class="token punctuation">)</span>

    decoder_input <span class="token operator">=</span> tf<span class="token punctuation">.</span>expand_dims<span class="token punctuation">(</span><span class="token punctuation">[</span>jp_tokenizer<span class="token punctuation">.</span>word_index<span class="token punctuation">[</span><span class="token string">'bos'</span><span class="token punctuation">]</span><span class="token punctuation">]</span> <span class="token operator">*</span> inference_batch_size<span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">)</span>
    decoder_emb_inp <span class="token operator">=</span> decoderNetwork<span class="token punctuation">.</span>decoder_embedding<span class="token punctuation">(</span>decoder_input<span class="token punctuation">)</span>

    decoder_instance <span class="token operator">=</span> tfa<span class="token punctuation">.</span>seq2seq<span class="token punctuation">.</span>BasicDecoder<span class="token punctuation">(</span>cell <span class="token operator">=</span> decoderNetwork<span class="token punctuation">.</span>rnn_cell<span class="token punctuation">,</span>
                                                sampler <span class="token operator">=</span> greedy_sampler<span class="token punctuation">,</span>
                                                output_layer <span class="token operator">=</span> decoderNetwork<span class="token punctuation">.</span>dense_layer<span class="token punctuation">)</span>
    decoderNetwork<span class="token punctuation">.</span>attention_mechanism<span class="token punctuation">.</span>setup_memory<span class="token punctuation">(</span>a<span class="token punctuation">)</span>

    decoder_initial_state <span class="token operator">=</span> decoderNetwork<span class="token punctuation">.</span>build_decoder_initial_state<span class="token punctuation">(</span>
        inference_batch_size<span class="token punctuation">,</span> encoder_state<span class="token operator">=</span><span class="token punctuation">[</span>a_tx<span class="token punctuation">,</span> c_tx<span class="token punctuation">]</span><span class="token punctuation">,</span> Dtype<span class="token operator">=</span>tf<span class="token punctuation">.</span>float32<span class="token punctuation">)</span>

    maximum_iterations <span class="token operator">=</span> tf<span class="token punctuation">.</span><span class="token builtin">round</span><span class="token punctuation">(</span>tf<span class="token punctuation">.</span>reduce_max<span class="token punctuation">(</span>Tx<span class="token punctuation">)</span> <span class="token operator">*</span> <span class="token number">2</span><span class="token punctuation">)</span>

    decoder_embedding_matrix <span class="token operator">=</span> decoderNetwork<span class="token punctuation">.</span>decoder_embedding<span class="token punctuation">.</span>variables<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>
    <span class="token punctuation">(</span>first_finished<span class="token punctuation">,</span> first_inputs<span class="token punctuation">,</span>first_state<span class="token punctuation">)</span> <span class="token operator">=</span> decoder_instance<span class="token punctuation">.</span>initialize<span class="token punctuation">(</span>
        decoder_embedding_matrix<span class="token punctuation">,</span> start_tokens <span class="token operator">=</span> start_tokens<span class="token punctuation">,</span>
        end_token <span class="token operator">=</span> end_token<span class="token punctuation">,</span> initial_state <span class="token operator">=</span> decoder_initial_state<span class="token punctuation">)</span>

    inputs <span class="token operator">=</span> first_inputs
    state <span class="token operator">=</span> first_state
    predictions <span class="token operator">=</span> np<span class="token punctuation">.</span>empty<span class="token punctuation">(</span><span class="token punctuation">(</span>inference_batch_size<span class="token punctuation">,</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">,</span> dtype <span class="token operator">=</span> np<span class="token punctuation">.</span>int32<span class="token punctuation">)</span>
    <span class="token keyword">for</span> j <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>maximum_iterations<span class="token punctuation">)</span><span class="token punctuation">:</span>
        outputs<span class="token punctuation">,</span> next_state<span class="token punctuation">,</span> next_inputs<span class="token punctuation">,</span> finished <span class="token operator">=</span> decoder_instance<span class="token punctuation">.</span>step<span class="token punctuation">(</span>j<span class="token punctuation">,</span> inputs<span class="token punctuation">,</span>state<span class="token punctuation">)</span>
        inputs <span class="token operator">=</span> next_inputs
        state <span class="token operator">=</span> next_state
        outputs <span class="token operator">=</span> np<span class="token punctuation">.</span>expand_dims<span class="token punctuation">(</span>outputs<span class="token punctuation">.</span>sample_id<span class="token punctuation">,</span>axis <span class="token operator">=</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>
        predictions <span class="token operator">=</span> np<span class="token punctuation">.</span>append<span class="token punctuation">(</span>predictions<span class="token punctuation">,</span> outputs<span class="token punctuation">,</span> axis <span class="token operator">=</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>

    res <span class="token operator">=</span> <span class="token string">''</span>
    <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token builtin">len</span><span class="token punctuation">(</span>predictions<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        line <span class="token operator">=</span> predictions<span class="token punctuation">[</span>i<span class="token punctuation">,</span><span class="token punctuation">:</span><span class="token punctuation">]</span>
        seq <span class="token operator">=</span> <span class="token builtin">list</span><span class="token punctuation">(</span>itertools<span class="token punctuation">.</span>takewhile<span class="token punctuation">(</span><span class="token keyword">lambda</span> index<span class="token punctuation">:</span> index <span class="token operator">!=</span><span class="token number">2</span><span class="token punctuation">,</span> line<span class="token punctuation">)</span><span class="token punctuation">)</span>
        res <span class="token operator">+=</span> <span class="token string">" "</span><span class="token punctuation">.</span>join<span class="token punctuation">(</span> <span class="token punctuation">[</span>jp_tokenizer<span class="token punctuation">.</span>index_word<span class="token punctuation">[</span>w<span class="token punctuation">]</span> <span class="token keyword">for</span> w <span class="token keyword">in</span> seq<span class="token punctuation">]</span><span class="token punctuation">)</span>
    res <span class="token operator">=</span> res<span class="token punctuation">.</span>split<span class="token punctuation">(</span><span class="token punctuation">)</span>

    <span class="token comment"># Return back Unique words</span>
    <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token builtin">len</span><span class="token punctuation">(</span>res<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">if</span> res<span class="token punctuation">[</span>i<span class="token punctuation">]</span> <span class="token operator">==</span> <span class="token string">'unk'</span> <span class="token keyword">and</span> unique <span class="token operator">!=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span><span class="token punctuation">:</span>
            res<span class="token punctuation">[</span>i<span class="token punctuation">]</span> <span class="token operator">=</span> unique<span class="token punctuation">.</span>pop<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span>

    <span class="token keyword">return</span> <span class="token string">' '</span><span class="token punctuation">.</span>join<span class="token punctuation">(</span>res<span class="token punctuation">)</span>

<span class="token comment"># Calculate BLEU</span>
<span class="token keyword">def</span> <span class="token function">BLEU</span><span class="token punctuation">(</span>X<span class="token punctuation">,</span> y<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token comment"># Prediction</span>
    pred <span class="token operator">=</span> <span class="token punctuation">[</span>Translate<span class="token punctuation">(</span>w<span class="token punctuation">)</span> <span class="token keyword">for</span> w <span class="token keyword">in</span> tqdm<span class="token punctuation">(</span>X<span class="token punctuation">)</span><span class="token punctuation">]</span>
    <span class="token comment"># Calculate BLEU</span>
    score <span class="token operator">=</span> sacrebleu<span class="token punctuation">.</span>corpus_bleu<span class="token punctuation">(</span>pred<span class="token punctuation">,</span> <span class="token punctuation">[</span>y<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">.</span>score <span class="token operator">/</span> <span class="token number">100</span>
    <span class="token keyword">return</span> score<span class="token punctuation">,</span> pred</code></pre></div>
<div class="gatsby-highlight" data-language="python"><pre class="language-python"><code class="language-python"><span class="token comment"># Custom Train Progress</span>
<span class="token keyword">class</span> <span class="token class-name">Progress</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
        self<span class="token punctuation">.</span>fig <span class="token operator">=</span> plt<span class="token punctuation">.</span>figure<span class="token punctuation">(</span>figsize <span class="token operator">=</span> <span class="token punctuation">(</span><span class="token number">8</span><span class="token punctuation">,</span><span class="token number">6</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>ax <span class="token operator">=</span> self<span class="token punctuation">.</span>fig<span class="token punctuation">.</span>add_subplot<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>loss<span class="token punctuation">,</span> self<span class="token punctuation">.</span>val_loss<span class="token punctuation">,</span> self<span class="token punctuation">.</span>BLEU <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
        self<span class="token punctuation">.</span>epoch_loss <span class="token operator">=</span> <span class="token number">0</span>

    <span class="token keyword">def</span> <span class="token function">get_val_loss</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">return</span> <span class="token punctuation">[</span>x<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span> <span class="token keyword">for</span> x <span class="token keyword">in</span> self<span class="token punctuation">.</span>val_loss<span class="token punctuation">]</span>

    <span class="token comment"># Plot</span>
    <span class="token keyword">def</span> <span class="token function">dynamic_plot</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
        self<span class="token punctuation">.</span>ax<span class="token punctuation">.</span>cla<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>ax<span class="token punctuation">.</span>plot<span class="token punctuation">(</span><span class="token builtin">range</span><span class="token punctuation">(</span><span class="token builtin">len</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>loss<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span> self<span class="token punctuation">.</span>loss<span class="token punctuation">,</span> label<span class="token operator">=</span><span class="token string">'loss'</span><span class="token punctuation">)</span>
        <span class="token keyword">if</span> <span class="token builtin">len</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>val_loss<span class="token punctuation">)</span> <span class="token operator">>=</span> <span class="token number">1</span><span class="token punctuation">:</span>
            x <span class="token operator">=</span> <span class="token punctuation">[</span>l<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span> <span class="token keyword">for</span> l <span class="token keyword">in</span> self<span class="token punctuation">.</span>val_loss<span class="token punctuation">]</span>
            y <span class="token operator">=</span> <span class="token punctuation">[</span>l<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span> <span class="token keyword">for</span> l <span class="token keyword">in</span> self<span class="token punctuation">.</span>val_loss<span class="token punctuation">]</span>
            self<span class="token punctuation">.</span>ax<span class="token punctuation">.</span>plot<span class="token punctuation">(</span>x<span class="token punctuation">,</span> y<span class="token punctuation">,</span> color <span class="token operator">=</span> <span class="token string">'r'</span><span class="token punctuation">,</span> label<span class="token operator">=</span><span class="token string">'val_loss'</span><span class="token punctuation">)</span>
            self<span class="token punctuation">.</span>ax<span class="token punctuation">.</span>plot<span class="token punctuation">(</span>x<span class="token punctuation">,</span> self<span class="token punctuation">.</span>BLEU<span class="token punctuation">,</span> color <span class="token operator">=</span> <span class="token string">'purple'</span><span class="token punctuation">,</span> label<span class="token operator">=</span><span class="token string">'BLEU'</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>ax<span class="token punctuation">.</span>set_ylim<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>ax<span class="token punctuation">.</span>legend<span class="token punctuation">(</span>loc <span class="token operator">=</span> <span class="token number">1</span><span class="token punctuation">)</span>
        display<span class="token punctuation">(</span>self<span class="token punctuation">.</span>fig<span class="token punctuation">)</span>

    <span class="token comment"># Train step progress</span>
    <span class="token keyword">def</span> <span class="token function">train_progress</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> epoch<span class="token punctuation">,</span> step<span class="token punctuation">,</span> steps_per_epoch<span class="token punctuation">,</span> start<span class="token punctuation">)</span><span class="token punctuation">:</span>
        self<span class="token punctuation">.</span>dynamic_plot<span class="token punctuation">(</span><span class="token punctuation">)</span>
        <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f'Working on Epoch </span><span class="token interpolation"><span class="token punctuation">{</span>epoch<span class="token punctuation">}</span></span><span class="token string">'</span></span><span class="token punctuation">)</span>
        <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'['</span> <span class="token operator">+</span> <span class="token punctuation">(</span><span class="token string">'='</span> <span class="token operator">*</span> <span class="token builtin">int</span><span class="token punctuation">(</span><span class="token punctuation">(</span>step <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">)</span> <span class="token operator">/</span> steps_per_epoch <span class="token operator">*</span> <span class="token number">60</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">.</span>ljust<span class="token punctuation">(</span><span class="token number">61</span><span class="token punctuation">,</span> <span class="token string">' '</span><span class="token punctuation">)</span>
              <span class="token operator">+</span> <span class="token string-interpolation"><span class="token string">f']  </span><span class="token interpolation"><span class="token punctuation">{</span>step <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">}</span></span><span class="token string">/</span><span class="token interpolation"><span class="token punctuation">{</span>steps_per_epoch<span class="token punctuation">}</span></span><span class="token string"> - loss : </span><span class="token interpolation"><span class="token punctuation">{</span><span class="token builtin">round</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>epoch_loss <span class="token operator">/</span> step<span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">)</span><span class="token punctuation">}</span></span><span class="token string">'</span></span><span class="token punctuation">)</span>
        <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f'Time per Step </span><span class="token interpolation"><span class="token punctuation">{</span><span class="token builtin">round</span><span class="token punctuation">(</span>timeit<span class="token punctuation">.</span>default_timer<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">-</span> start<span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">}</span></span><span class="token string"> s'</span></span><span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">summary</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
        loss <span class="token operator">=</span> np<span class="token punctuation">.</span>array_split<span class="token punctuation">(</span>np<span class="token punctuation">.</span>array<span class="token punctuation">(</span>self<span class="token punctuation">.</span>loss<span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token builtin">len</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>val_loss<span class="token punctuation">)</span><span class="token punctuation">)</span>
        loss <span class="token operator">=</span> <span class="token punctuation">[</span>np<span class="token punctuation">.</span>mean<span class="token punctuation">(</span>x<span class="token punctuation">)</span> <span class="token keyword">for</span> x <span class="token keyword">in</span> loss<span class="token punctuation">]</span>
        val_loss <span class="token operator">=</span> <span class="token punctuation">[</span>x<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span> <span class="token keyword">for</span> x <span class="token keyword">in</span> self<span class="token punctuation">.</span>val_loss<span class="token punctuation">]</span>
        df <span class="token operator">=</span> pd<span class="token punctuation">.</span>DataFrame<span class="token punctuation">(</span><span class="token punctuation">{</span><span class="token string">'Epochs'</span> <span class="token punctuation">:</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token builtin">len</span><span class="token punctuation">(</span>val_loss<span class="token punctuation">)</span> <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token string">'loss'</span> <span class="token punctuation">:</span> loss<span class="token punctuation">,</span>
                           <span class="token string">'val loss'</span> <span class="token punctuation">:</span> val_loss<span class="token punctuation">,</span> <span class="token string">'BLEU'</span> <span class="token punctuation">:</span> self<span class="token punctuation">.</span>BLEU<span class="token punctuation">}</span><span class="token punctuation">)</span>

        self<span class="token punctuation">.</span>dynamic_plot<span class="token punctuation">(</span><span class="token punctuation">)</span>
        clear_output<span class="token punctuation">(</span>wait <span class="token operator">=</span> <span class="token boolean">True</span><span class="token punctuation">)</span>
        display<span class="token punctuation">(</span>df<span class="token punctuation">)</span></code></pre></div>
<div class="gatsby-highlight" data-language="python"><pre class="language-python"><code class="language-python"><span class="token comment"># Initialize Train Progress</span>
TP <span class="token operator">=</span> Progress<span class="token punctuation">(</span><span class="token punctuation">)</span>
best_prediction <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>

<span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> epochs <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">:</span>

    encoder_initial_cell_state <span class="token operator">=</span> initialize_initial_state<span class="token punctuation">(</span><span class="token punctuation">)</span>
    total_loss <span class="token operator">=</span> <span class="token number">0.0</span>
    <span class="token comment"># Train Loss</span>
    TP<span class="token punctuation">.</span>epoch_loss <span class="token operator">=</span> <span class="token number">0</span>

    <span class="token comment"># Train</span>
    <span class="token keyword">for</span> <span class="token punctuation">(</span>batch <span class="token punctuation">,</span> <span class="token punctuation">(</span>input_batch<span class="token punctuation">,</span> output_batch<span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token keyword">in</span> <span class="token builtin">enumerate</span><span class="token punctuation">(</span>dataset<span class="token punctuation">.</span>take<span class="token punctuation">(</span>steps_per_epoch<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        start <span class="token operator">=</span> timeit<span class="token punctuation">.</span>default_timer<span class="token punctuation">(</span><span class="token punctuation">)</span>
        batch_loss <span class="token operator">=</span> train_step<span class="token punctuation">(</span>input_batch<span class="token punctuation">,</span> output_batch<span class="token punctuation">,</span> encoder_initial_cell_state<span class="token punctuation">)</span>
        total_loss <span class="token operator">+=</span> batch_loss
        TP<span class="token punctuation">.</span>loss<span class="token punctuation">.</span>append<span class="token punctuation">(</span>batch_loss<span class="token punctuation">.</span>numpy<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
        TP<span class="token punctuation">.</span>epoch_loss <span class="token operator">+=</span> batch_loss<span class="token punctuation">.</span>numpy<span class="token punctuation">(</span><span class="token punctuation">)</span>

        <span class="token keyword">if</span> <span class="token punctuation">(</span>batch<span class="token operator">+</span><span class="token number">1</span><span class="token punctuation">)</span> <span class="token operator">%</span> <span class="token number">30</span> <span class="token operator">==</span> <span class="token number">0</span><span class="token punctuation">:</span>
            TP<span class="token punctuation">.</span>train_progress<span class="token punctuation">(</span>i<span class="token punctuation">,</span> batch<span class="token punctuation">,</span> steps_per_epoch<span class="token punctuation">,</span> start<span class="token punctuation">)</span>
            clear_output<span class="token punctuation">(</span>wait <span class="token operator">=</span> <span class="token boolean">True</span><span class="token punctuation">)</span>

    <span class="token comment"># Validitate</span>
    encoderNetwork<span class="token punctuation">.</span>trainable <span class="token operator">=</span> <span class="token boolean">False</span>  <span class="token comment"># Freeze our model layer to make sure</span>
    decoderNetwork<span class="token punctuation">.</span>trainable <span class="token operator">=</span> <span class="token boolean">False</span>  <span class="token comment"># it didn't learn anything from val_data</span>

    <span class="token comment"># Valid loss</span>
    val_loss <span class="token operator">=</span> <span class="token number">0</span>
    <span class="token keyword">for</span> <span class="token punctuation">(</span>batch<span class="token punctuation">,</span> <span class="token punctuation">(</span>input_batch<span class="token punctuation">,</span> output_batch<span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token keyword">in</span> <span class="token builtin">enumerate</span><span class="token punctuation">(</span>val_dataset<span class="token punctuation">.</span>take<span class="token punctuation">(</span>val_steps_per_epoch<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        batch_loss <span class="token operator">=</span> evaluate<span class="token punctuation">(</span>input_batch<span class="token punctuation">,</span> output_batch<span class="token punctuation">,</span> encoder_initial_cell_state<span class="token punctuation">)</span>
        val_loss <span class="token operator">+=</span> batch_loss<span class="token punctuation">.</span>numpy<span class="token punctuation">(</span><span class="token punctuation">)</span>
    val_loss <span class="token operator">/=</span> val_steps_per_epoch

    TP<span class="token punctuation">.</span>val_loss<span class="token punctuation">.</span>append<span class="token punctuation">(</span><span class="token punctuation">(</span>i <span class="token operator">*</span> steps_per_epoch <span class="token operator">-</span> <span class="token number">1</span><span class="token punctuation">,</span> val_loss<span class="token punctuation">)</span><span class="token punctuation">)</span>

    <span class="token comment"># Bleu Score</span>
    bleu_score<span class="token punctuation">,</span> pred <span class="token operator">=</span> BLEU<span class="token punctuation">(</span>eng_test<span class="token punctuation">,</span> jpn_test<span class="token punctuation">)</span>
    TP<span class="token punctuation">.</span>BLEU<span class="token punctuation">.</span>append<span class="token punctuation">(</span>bleu_score<span class="token punctuation">)</span>

    encoderNetwork<span class="token punctuation">.</span>trainable <span class="token operator">=</span> <span class="token boolean">True</span>  <span class="token comment"># Unfreeze layer for next epoch</span>
    decoderNetwork<span class="token punctuation">.</span>trainable <span class="token operator">=</span> <span class="token boolean">True</span>

    <span class="token comment"># Save best model</span>
    <span class="token keyword">if</span> bleu_score <span class="token operator">==</span> <span class="token builtin">max</span><span class="token punctuation">(</span>TP<span class="token punctuation">.</span>BLEU<span class="token punctuation">)</span> <span class="token keyword">and</span> val_loss <span class="token operator">==</span> <span class="token builtin">min</span><span class="token punctuation">(</span>TP<span class="token punctuation">.</span>get_val_loss<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        best_prediction <span class="token operator">=</span> pred
        encoderNetwork<span class="token punctuation">.</span>save_weights<span class="token punctuation">(</span><span class="token string">'encoderNetwork'</span><span class="token punctuation">)</span>
        decoderNetwork<span class="token punctuation">.</span>save_weights<span class="token punctuation">(</span><span class="token string">'decoderNetwork'</span><span class="token punctuation">)</span>

TP<span class="token punctuation">.</span>summary<span class="token punctuation">(</span><span class="token punctuation">)</span></code></pre></div>
<table>
<thead>
<tr>
<th align="center">Epochs</th>
<th align="center">loss</th>
<th align="center">val loss</th>
<th align="center">BLEU</th>
</tr>
</thead>
<tbody>
<tr>
<td align="center">1</td>
<td align="center">0.755007</td>
<td align="center">0.617767</td>
<td align="center">0.033657</td>
</tr>
<tr>
<td align="center">2</td>
<td align="center">0.507803</td>
<td align="center">0.467358</td>
<td align="center">0.100527</td>
</tr>
<tr>
<td align="center">3</td>
<td align="center">0.366943</td>
<td align="center">0.404535</td>
<td align="center">0.169416</td>
</tr>
<tr>
<td align="center">4</td>
<td align="center">0.278531</td>
<td align="center">0.382699</td>
<td align="center">0.206622</td>
</tr>
<tr>
<td align="center">5</td>
<td align="center">0.220114</td>
<td align="center">0.379538</td>
<td align="center">0.223136</td>
</tr>
<tr>
<td align="center">6</td>
<td align="center">0.180076</td>
<td align="center">0.384533</td>
<td align="center">0.240543</td>
</tr>
<tr>
<td align="center">7</td>
<td align="center">0.152284</td>
<td align="center">0.390775</td>
<td align="center">0.256055</td>
</tr>
</tbody>
</table>
<p><span
      class="gatsby-resp-image-wrapper"
      style="position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 490px; "
    >
      <a
    class="gatsby-resp-image-link"
    href="/static/4968cd1de5dc4c8cb3cbadd3b3801588/41d3c/output_49_1.png"
    style="display: block"
    target="_blank"
    rel="noopener"
  >
    <span
    class="gatsby-resp-image-background-image"
    style="padding-bottom: 72.78481012658227%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAPCAYAAADkmO9VAAAACXBIWXMAAAsSAAALEgHS3X78AAACXklEQVQ4y52RT08TQRjG9zt48SuYgISEePBuIn6HBoMaE60XPXKRqjF69WI8ATcPRk0jCUYM8k+FQOgChZZu6F/ows7u0m53d3Zn3sdsWy0NJlIneTIzyby/93mfUTKZzFXDMN6apjlpmuZUL2KMTR0Y1lShWJrQdf29pmnXlFxu7y4X+K9FAKRdR7lYRKlcBmPsoVIsFmIuFyACJ6KwF0kpQ243wqOq7hmGEQHvK9ns7qjLKQIKgIjonO46D6NDEN0ZY3GlWMiPuFz2BPz9KNqbFYQgDEOYphlX8vvaTceXkKeA/4KeAQKBEKLlMK9lRku2hJAQRB2HPYzeDazp+ZHUgUTJbjmUktqdW/Gcw203sJBL3zqqEybWIPIW6PRYv4V2g865ncvfRi6WKjFAIlWFeL0c0o90HbYTdrmQ1FKTgTPqBqqqeicaM6rT66B3qy7ezJhY2zmBrjvwTxwgSgOyrRamE01zBWEoYJosrlQqlZiUf365WVGugZIqp8nFBn1YdymVOaGVbYusQ4saVUZulRFsRjAZwYpkcDh1GLYdV9Lp9O3213blLwTgB4BWCTG9EeCzyjGT4vi07uNLysf3HR9fVR/b+z5WM77QDjm8hvVA0TQtFgQBhBBewAPumq5XNxzuGA73bdeTns8R+Bwi8Fwv4DwUvMbJK9nSNxzBNUZe5lg6xzUOix3fUxKJxMVkMnllc2uzb2lhaWDs0dj1+dn5ge3cTv+T50+H55aXBtXsXt/YeOLGys/Fwd0ttf/ls/HhjYXpocAsXXr14vGw+u3jkFHKXp6dW7jwC/SSU1VUgwWcAAAAAElFTkSuQmCC'); background-size: cover; display: block;"
  ></span>
  <img
        class="gatsby-resp-image-image"
        alt="png"
        title="png"
        src="/static/4968cd1de5dc4c8cb3cbadd3b3801588/41d3c/output_49_1.png"
        srcset="/static/4968cd1de5dc4c8cb3cbadd3b3801588/c26ae/output_49_1.png 158w,
/static/4968cd1de5dc4c8cb3cbadd3b3801588/6bdcf/output_49_1.png 315w,
/static/4968cd1de5dc4c8cb3cbadd3b3801588/41d3c/output_49_1.png 490w"
        sizes="(max-width: 490px) 100vw, 490px"
        style="width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;"
        loading="lazy"
        decoding="async"
      />
  </a>
    </span></p>
<div class="gatsby-highlight" data-language="python"><pre class="language-python"><code class="language-python"><span class="token comment"># Load best weights</span>
encoderNetwork<span class="token punctuation">.</span>load_weights<span class="token punctuation">(</span><span class="token string">'encoderNetwork'</span><span class="token punctuation">)</span>
decoderNetwork<span class="token punctuation">.</span>load_weights<span class="token punctuation">(</span><span class="token string">'decoderNetwork'</span><span class="token punctuation">)</span></code></pre></div>
<div class="gatsby-highlight" data-language="text"><pre class="language-text"><code class="language-text">&lt;tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7fb3601083d0></code></pre></div>
<p>Let‚Äôs check the best prediction of our model.</p>
<div class="gatsby-highlight" data-language="python"><pre class="language-python"><code class="language-python"><span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">7</span><span class="token punctuation">,</span><span class="token number">16</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"English Sentence:"</span><span class="token punctuation">)</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span>eng_test<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">)</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"\nJapanese Translation:"</span><span class="token punctuation">)</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span>best_prediction<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">)</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"\nJapanese Reference:"</span><span class="token punctuation">)</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span>jpn_test<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">)</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">''</span><span class="token punctuation">.</span>ljust<span class="token punctuation">(</span><span class="token number">60</span><span class="token punctuation">,</span> <span class="token string">'-'</span><span class="token punctuation">)</span><span class="token punctuation">)</span></code></pre></div>
<div class="gatsby-highlight" data-language="text"><pre class="language-text"><code class="language-text">English Sentence:
in september there are just a few people here and there on the beach

Japanese Translation:
Ôºô ÊôÇ „Å´ „ÅØ „Åì„Åì „Å´ ‰ªñ „ÅÆ ‰∫∫ „Åå ‰Ωè„Çì „Åß „ÅÑ „Å™„ÅÑ ‰∫∫ „Åå „Åù„ÅÆ „Éì„Éº„ÉÅ „Åß ‰ªñ „ÅÆ ‰∫∫ÈÅî „Åå ÊÆã„Çä „ÅÆ „ÇÇ„Å® „Çí „ÅÇ„Å°„Çâ „Å´ Âª∫Ë®≠ Ë¶ã „Åü

Japanese Reference:
ÔºôÊúà „ÅÆ Êµ∑ „ÅØ ‰∫∫ „Åå „Åæ„Å∞„Çâ „Å† „Å≠
------------------------------------------------------------
English Sentence:
while you are young you should read a lot

Japanese Translation:
Ëã•„ÅÑ È†É „ÅØ Ë™≠„Åø ÁµÇ„Çè„Å£ „Åü„Çâ „ÅÑ„ÅÑ „Çà

Japanese Reference:
Ëã•„ÅÑ „ÅÜ„Å° „Å´ „Åü„Åè„Åï„Çì „ÅÆ Êú¨ „Çí Ë™≠„ÇÄ „Åπ„Åç „Å†
------------------------------------------------------------
English Sentence:
here i come

Japanese Translation:
„Åì„Åì „Å´ Êù• „Åü „ÅÆ

Japanese Reference:
„ÅÑ„Åæ Ë°å„Åç „Åæ„Åô
------------------------------------------------------------
English Sentence:
once you have decided when you will be coming let me know

Japanese Translation:
Êù•„Çã „Åã Âêõ „Å´ „ÅØ ÈÄ£Áµ° „Çí Ë®Ä„Å£ „Å¶ „Åç „Åü „Çà

Japanese Reference:
„ÅÑ„Å§ Êù•„Çã „Åã Ê±∫„Åæ„Å£ „Åü„Çâ Êïô„Åà „Å¶
------------------------------------------------------------
English Sentence:
he jumped on the train

Japanese Translation:
ÂΩº „ÅØ ÈõªËªä „Å´ Êóó „Çí È£õ„Å≥Ë∂ä„Åà „Åü

Japanese Reference:
ÂΩº „ÅØ ÈõªËªä „Å´ È£õ„Å≥‰πó„Å£ „Åü
------------------------------------------------------------
English Sentence:
he passed away yesterday

Japanese Translation:
ÂΩº „ÅØ Êò®Êó• ‰∫°„Åè„Å™„Å£ „Åü

Japanese Reference:
ÂΩº „ÅØ Êò®Êó• „Åä ‰∫°„Åè„Å™„Çä „Å´ „Å™„Çä „Åæ„Åó „Åü
------------------------------------------------------------
English Sentence:
i had no other choice

Japanese Translation:
‰ªñ „Å´ ÈÅ∏ÊäûËÇ¢ „Åå „Å™„Åã„Å£ „Åü

Japanese Reference:
‰ªñ „Å´ Êâã „Åå „Å™„Åã„Å£ „Åü „ÅÆ „Å†
------------------------------------------------------------
English Sentence:
are you good at bowling

Japanese Translation:
„Éú„Éº„É™„É≥„Ç∞ „ÅØ ÂæóÊÑè „Åß„Åô „Åã

Japanese Reference:
„Éú„Ç¶„É™„É≥„Ç∞ „ÅØ ÂæóÊÑè
------------------------------------------------------------
English Sentence:
it is strange that you do not know anything about that matter

Japanese Translation:
„Åù„Çå „Å´„Å§„ÅÑ„Å¶ ‰Ωï „ÇÇ Áü•„Çâ „Å™„ÅÑ „Åì„Å® „Åå „Å™„ÅÑ „Å®„ÅÑ„ÅÜ „Åì„Å® „ÅØ Â§â „Å†

Japanese Reference:
„ÅÇ„Å™„Åü „Åå „Åù„ÅÆ „Åì„Å® „Å´„Å§„ÅÑ„Å¶ ‰Ωï „ÇÇ Áü•„Çâ „Å™„ÅÑ „ÅÆ „ÅØ Â§â „Å†
------------------------------------------------------------</code></pre></div>
<h1>Test with Some Raw Input</h1>
<p>Yeay now let‚Äôs play with <strong>our</strong> Machine Translation with some raw input. We‚Äôll cross check the prediction from MT with Google Translate API to translate it back to english and see how bad <strong>our</strong> MT is :).</p>
<div class="gatsby-highlight" data-language="python"><pre class="language-python"><code class="language-python"><span class="token keyword">from</span> googletrans <span class="token keyword">import</span> Translator
<span class="token comment"># Google Translate</span>
translator <span class="token operator">=</span> Translator<span class="token punctuation">(</span><span class="token punctuation">)</span></code></pre></div>
<div class="gatsby-highlight" data-language="python"><pre class="language-python"><code class="language-python"><span class="token builtin">raw_input</span> <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token string">'i love you'</span><span class="token punctuation">,</span> <span class="token string">'i am sorry'</span><span class="token punctuation">,</span> <span class="token string">'hello'</span><span class="token punctuation">,</span> <span class="token string">'thank you'</span><span class="token punctuation">,</span>
             <span class="token string">'is there something i can help?'</span><span class="token punctuation">]</span>

<span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token builtin">len</span><span class="token punctuation">(</span><span class="token builtin">raw_input</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    prediction <span class="token operator">=</span> Translate<span class="token punctuation">(</span><span class="token builtin">raw_input</span><span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">)</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"English Sentence:"</span><span class="token punctuation">)</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token builtin">raw_input</span><span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">)</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"\nJapanese Translation:"</span><span class="token punctuation">)</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span>prediction<span class="token punctuation">)</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"\nEnglish Translation from prediction [GoogleTranslate]:"</span><span class="token punctuation">)</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span>translator<span class="token punctuation">.</span>translate<span class="token punctuation">(</span>prediction<span class="token punctuation">)</span><span class="token punctuation">.</span>text<span class="token punctuation">)</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">''</span><span class="token punctuation">.</span>ljust<span class="token punctuation">(</span><span class="token number">60</span><span class="token punctuation">,</span> <span class="token string">'-'</span><span class="token punctuation">)</span><span class="token punctuation">)</span></code></pre></div>
<div class="gatsby-highlight" data-language="text"><pre class="language-text"><code class="language-text">English Sentence:
i love you

Japanese Translation:
ÊÑõ„Åó „Å¶„Çã „Çà

English Translation from prediction [GoogleTranslate]:
I love you
------------------------------------------------------------
English Sentence:
i am sorry

Japanese Translation:
„Åô„Åø„Åæ„Åõ„Çì

English Translation from prediction [GoogleTranslate]:
Excuse me
------------------------------------------------------------
English Sentence:
hello

Japanese Translation:
„ÇÇ„Åó„ÇÇ„Åó

English Translation from prediction [GoogleTranslate]:
Hello
------------------------------------------------------------
English Sentence:
thank you

Japanese Translation:
„ÅÇ„Çä„Åå„Å®„ÅÜ „Åî„Åñ„ÅÑ „Åæ„Åô

English Translation from prediction [GoogleTranslate]:
Thank you
------------------------------------------------------------
English Sentence:
is there something i can help?

Japanese Translation:
‰Ωï „Åã Êâã‰ºù„Åà„Çã „ÇÇ„ÅÆ „Åå „ÅÇ„Çã „ÅÆ

English Translation from prediction [GoogleTranslate]:
There is something to help
------------------------------------------------------------</code></pre></div>
<div class="gatsby-highlight" data-language="python"><pre class="language-python"><code class="language-python"><span class="token keyword">import</span> pickle

<span class="token keyword">with</span> <span class="token builtin">open</span><span class="token punctuation">(</span><span class="token string">'en_tokenizer.pickle'</span><span class="token punctuation">,</span> <span class="token string">'wb'</span><span class="token punctuation">)</span> <span class="token keyword">as</span> handle<span class="token punctuation">:</span>
    pickle<span class="token punctuation">.</span>dump<span class="token punctuation">(</span>en_tokenizer<span class="token punctuation">,</span> handle<span class="token punctuation">,</span> protocol<span class="token operator">=</span>pickle<span class="token punctuation">.</span>HIGHEST_PROTOCOL<span class="token punctuation">)</span>
handle<span class="token punctuation">.</span>close<span class="token punctuation">(</span><span class="token punctuation">)</span>

<span class="token keyword">with</span> <span class="token builtin">open</span><span class="token punctuation">(</span><span class="token string">'jp_tokenizer.pickle'</span><span class="token punctuation">,</span> <span class="token string">'wb'</span><span class="token punctuation">)</span> <span class="token keyword">as</span> handle<span class="token punctuation">:</span>
    pickle<span class="token punctuation">.</span>dump<span class="token punctuation">(</span>jp_tokenizer<span class="token punctuation">,</span> handle<span class="token punctuation">,</span> protocol<span class="token operator">=</span>pickle<span class="token punctuation">.</span>HIGHEST_PROTOCOL<span class="token punctuation">)</span>
handle<span class="token punctuation">.</span>close<span class="token punctuation">(</span><span class="token punctuation">)</span></code></pre></div>
<h1>Reference</h1>
<ol>
<li>TensorFlow Addons Networks : Sequence-to-Sequence NMT with Attention Mechanism <a href="https://www.tensorflow.org/addons/tutorials/networks_seq2seq_nmt">Link</a></li>
<li>seq2seq (Sequence to Sequence) Model for Deep Learning with PyTorch <a href="https://www.guru99.com/seq2seq-model.html">Link</a></li>
</ol></section><hr/><footer><div class="bio"><a href="/about"><div data-gatsby-image-wrapper="" style="width:50px;height:50px" class="gatsby-image-wrapper bio-avatar"><div aria-hidden="true" data-placeholder-image="" style="opacity:1;transition:opacity 500ms linear;background-color:#f8f8e8;width:50px;height:50px;position:relative"></div><picture><source type="image/avif" data-srcset="/static/8c047f82dd390bcb065fe98e0afcc519/d4bf4/profile-pic.avif 50w,/static/8c047f82dd390bcb065fe98e0afcc519/ee81f/profile-pic.avif 100w" sizes="50px"/><source type="image/webp" data-srcset="/static/8c047f82dd390bcb065fe98e0afcc519/3faea/profile-pic.webp 50w,/static/8c047f82dd390bcb065fe98e0afcc519/6a679/profile-pic.webp 100w" sizes="50px"/><img data-gatsby-image-ssr="" layout="fixed" data-main-image="" style="opacity:0" sizes="50px" decoding="async" loading="lazy" data-src="/static/8c047f82dd390bcb065fe98e0afcc519/e5610/profile-pic.png" data-srcset="/static/8c047f82dd390bcb065fe98e0afcc519/e5610/profile-pic.png 50w,/static/8c047f82dd390bcb065fe98e0afcc519/e9b55/profile-pic.png 100w" alt="Profile picture"/></picture><noscript><picture><source type="image/avif" srcSet="/static/8c047f82dd390bcb065fe98e0afcc519/d4bf4/profile-pic.avif 50w,/static/8c047f82dd390bcb065fe98e0afcc519/ee81f/profile-pic.avif 100w" sizes="50px"/><source type="image/webp" srcSet="/static/8c047f82dd390bcb065fe98e0afcc519/3faea/profile-pic.webp 50w,/static/8c047f82dd390bcb065fe98e0afcc519/6a679/profile-pic.webp 100w" sizes="50px"/><img data-gatsby-image-ssr="" layout="fixed" data-main-image="" style="opacity:0" sizes="50px" decoding="async" loading="lazy" src="/static/8c047f82dd390bcb065fe98e0afcc519/e5610/profile-pic.png" srcSet="/static/8c047f82dd390bcb065fe98e0afcc519/e5610/profile-pic.png 50w,/static/8c047f82dd390bcb065fe98e0afcc519/e9b55/profile-pic.png 100w" alt="Profile picture"/></picture></noscript><script type="module">const t="undefined"!=typeof HTMLImageElement&&"loading"in HTMLImageElement.prototype;if(t){const t=document.querySelectorAll("img[data-main-image]");for(let e of t){e.dataset.src&&(e.setAttribute("src",e.dataset.src),e.removeAttribute("data-src")),e.dataset.srcset&&(e.setAttribute("srcset",e.dataset.srcset),e.removeAttribute("data-srcset"));const t=e.parentNode.querySelectorAll("source[data-srcset]");for(let e of t)e.setAttribute("srcset",e.dataset.srcset),e.removeAttribute("data-srcset");e.complete&&(e.style.opacity=1)}}</script></div></a><p>Written by <strong>Wahyu Setianto</strong> <!-- -->a Data Enthusiast who love building a website.<!-- --> <a href="/about">About Me</a></p></div></footer></article><nav class="blog-post-nav"><ul style="display:flex;flex-wrap:wrap;justify-content:space-between;list-style:none;padding:0"><li></li><li><a rel="next" href="/cnn-keras-cv-0-996-tpu/">MNIST Digit Classifier using Keras &amp; TPU<!-- --> ‚Üí</a></li></ul><a href="/">Back to Home</a></nav></main><footer>¬© <!-- -->2021<!-- -->, Built with<!-- --> <a href="https://www.gatsbyjs.com">Gatsby </a>and <!-- -->‚ù§</footer></div></div><div id="gatsby-announcer" style="position:absolute;top:0;width:1px;height:1px;padding:0;overflow:hidden;clip:rect(0, 0, 0, 0);white-space:nowrap;border:0" aria-live="assertive" aria-atomic="true"></div></div><script>
  
  
  if(true) {
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
  }
  if (typeof ga === "function") {
    ga('create', '238057269', 'auto', {});
      
      
      
      
      
      }</script><script id="gatsby-script-loader">/*<![CDATA[*/window.pagePath="/machine-translation-en-jp-seq2seq-tf/";/*]]>*/</script><script id="gatsby-chunk-mapping">/*<![CDATA[*/window.___chunkMapping={"polyfill":["/polyfill-e519fe25106c9a3ec3a7.js"],"app":["/app-56f5b02a7eae32f29a0e.js"],"component---src-pages-404-js":["/component---src-pages-404-js-7d7e4be9ae31018f7e40.js"],"component---src-pages-about-tsx":["/component---src-pages-about-tsx-e88704afca9d91d27621.js"],"component---src-pages-index-js":["/component---src-pages-index-js-5ed9ff86b285f095695e.js"],"component---src-templates-blog-post-js":["/component---src-templates-blog-post-js-3484340e7d6fb1e13f5f.js"]};/*]]>*/</script><script src="/polyfill-e519fe25106c9a3ec3a7.js" nomodule=""></script><script src="/component---src-templates-blog-post-js-3484340e7d6fb1e13f5f.js" async=""></script><script src="/commons-4425428cc7103c2b9c46.js" async=""></script><script src="/app-56f5b02a7eae32f29a0e.js" async=""></script><script src="/framework-cf9a832c62e1a97a7e95.js" async=""></script><script src="/webpack-runtime-25b9e67a831a950b2ccf.js" async=""></script></body></html>