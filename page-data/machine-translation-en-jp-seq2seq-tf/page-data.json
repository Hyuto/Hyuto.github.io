{"componentChunkName":"component---src-templates-blog-post-js","path":"/machine-translation-en-jp-seq2seq-tf/","result":{"data":{"site":{"siteMetadata":{"title":"Hyuto's Blog"}},"markdownRemark":{"id":"461be79d-3ea2-5bb6-b8f1-c4460049fd52","excerpt":"Hello guys, lately i’ve been studying about machine translation and give it a try. Most of code in this notebook is based on tensorflow tutorial on their…","html":"<p>Hello guys, lately i’ve been studying about machine translation and give it a try.</p>\n<p>Most of code in this notebook is based on tensorflow tutorial on their website\n<a href=\"https://www.tensorflow.org/addons/tutorials/networks_seq2seq_nmt\">TensorFlow Addons Networks : Sequence-to-Sequence NMT with Attention Mechanism</a>.</p>\n<p>This notebook is basically my notebook run on <a href=\"https://www.kaggle.com/\">kaggle</a> so if you want to try\nand run the code with same environment as mine go to link bellow.</p>\n<p>Kaggle Notebook : <a href=\"https://www.kaggle.com/wahyusetianto/machine-translation-en-jp-seq2seq-tf\">Machine Translation EN-JP Seq2Seq Tensorflow</a></p>\n<p>P.S. Don’t forget to <em>upvote</em> if you like it 😊.</p>\n<h1>English - Japanese Machine Translation</h1>\n<p>So in this notebook we’re going to build English to Japanese machine translation, Japanese text contains lots of unique words because they have 3 type of it:</p>\n<ol>\n<li>Kanji</li>\n<li>Katakana</li>\n<li>Hiragana</li>\n</ol>\n<p>that’s the interesting part of it and so it’ll be little complicated to process. So let’s get started.</p>\n<h1>Table Of Content</h1>\n<ol>\n<li>Load Dataset</li>\n<li>Text Preprocessing\n<ul>\n<li>English misspell handling</li>\n<li>Segmenting Japanese words</li>\n<li>Add BOS and EOS to train sentences</li>\n</ul>\n</li>\n<li>Word Tokenizing\n<ul>\n<li>Word Cloud</li>\n</ul>\n</li>\n<li>Build &#x26; Train Model</li>\n<li>Scoring Bleu</li>\n<li>Test with Some Raw Input</li>\n</ol>\n<h2>Install some tools</h2>\n<ol>\n<li>Sacreblue for calculate BLEU score</li>\n<li>Googletrans => Google Translate for testing some sentences later</li>\n</ol>\n<p>Note : You can use NLTK for calculating BLEU score <a href=\"https://www.nltk.org/_modules/nltk/translate/bleu_score.html\">documentation</a></p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\">!pip <span class=\"token operator\">-</span>q install sacrebleu\n!pip <span class=\"token operator\">-</span>q install googletrans\n!pip <span class=\"token operator\">-</span>q install tensorflow<span class=\"token operator\">-</span>addons <span class=\"token operator\">-</span><span class=\"token operator\">-</span>upgrade</code></pre></div>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token keyword\">import</span> random<span class=\"token punctuation\">,</span> re<span class=\"token punctuation\">,</span> string<span class=\"token punctuation\">,</span> itertools<span class=\"token punctuation\">,</span> timeit<span class=\"token punctuation\">,</span> sacrebleu\n<span class=\"token keyword\">import</span> numpy <span class=\"token keyword\">as</span> np\n<span class=\"token keyword\">import</span> pandas <span class=\"token keyword\">as</span> pd\n<span class=\"token keyword\">import</span> matplotlib<span class=\"token punctuation\">.</span>pyplot <span class=\"token keyword\">as</span> plt\n<span class=\"token keyword\">import</span> seaborn <span class=\"token keyword\">as</span> sns\n<span class=\"token keyword\">from</span> tqdm<span class=\"token punctuation\">.</span>notebook <span class=\"token keyword\">import</span> tqdm\n<span class=\"token keyword\">from</span> IPython<span class=\"token punctuation\">.</span>display <span class=\"token keyword\">import</span> display<span class=\"token punctuation\">,</span> clear_output\n<span class=\"token keyword\">from</span> sklearn<span class=\"token punctuation\">.</span>model_selection <span class=\"token keyword\">import</span> train_test_split\n\n<span class=\"token comment\"># Tensorflow &amp; Keras</span>\n<span class=\"token keyword\">import</span> tensorflow <span class=\"token keyword\">as</span> tf\n<span class=\"token keyword\">import</span> tensorflow_addons <span class=\"token keyword\">as</span> tfa\n<span class=\"token keyword\">import</span> tensorflow<span class=\"token punctuation\">.</span>keras<span class=\"token punctuation\">.</span>backend <span class=\"token keyword\">as</span> K\n<span class=\"token keyword\">from</span> tensorflow<span class=\"token punctuation\">.</span>keras<span class=\"token punctuation\">.</span>layers <span class=\"token keyword\">import</span> Input<span class=\"token punctuation\">,</span> Dense<span class=\"token punctuation\">,</span> LSTM<span class=\"token punctuation\">,</span> LSTMCell\n<span class=\"token keyword\">from</span> tensorflow<span class=\"token punctuation\">.</span>keras<span class=\"token punctuation\">.</span>layers <span class=\"token keyword\">import</span> Embedding<span class=\"token punctuation\">,</span> Bidirectional\n<span class=\"token keyword\">from</span> tensorflow<span class=\"token punctuation\">.</span>keras<span class=\"token punctuation\">.</span>models <span class=\"token keyword\">import</span> Model\n<span class=\"token keyword\">from</span> tensorflow<span class=\"token punctuation\">.</span>keras<span class=\"token punctuation\">.</span>preprocessing<span class=\"token punctuation\">.</span>text <span class=\"token keyword\">import</span> Tokenizer\n<span class=\"token keyword\">from</span> tensorflow<span class=\"token punctuation\">.</span>keras<span class=\"token punctuation\">.</span>preprocessing<span class=\"token punctuation\">.</span>sequence <span class=\"token keyword\">import</span> pad_sequences\n<span class=\"token keyword\">from</span> tensorflow<span class=\"token punctuation\">.</span>keras<span class=\"token punctuation\">.</span>losses <span class=\"token keyword\">import</span> SparseCategoricalCrossentropy\n<span class=\"token keyword\">from</span> tensorflow<span class=\"token punctuation\">.</span>keras<span class=\"token punctuation\">.</span>optimizers <span class=\"token keyword\">import</span> Adam\n\n<span class=\"token comment\"># Japanese Word Tokenizer</span>\n<span class=\"token keyword\">from</span> janome<span class=\"token punctuation\">.</span>tokenizer <span class=\"token keyword\">import</span> Tokenizer <span class=\"token keyword\">as</span> janome_tokenizer\n\nplt<span class=\"token punctuation\">.</span>style<span class=\"token punctuation\">.</span>use<span class=\"token punctuation\">(</span><span class=\"token string\">'seaborn-pastel'</span><span class=\"token punctuation\">)</span></code></pre></div>\n<h1>Dataset</h1>\n<p>Here we use 55463 en-jp corpus from <a href=\"http://www.manythings.org/bilingual/\">ManyThings.org Bilingual Sentence Pairs</a></p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token comment\"># Download Data &amp; Unzip Data</span>\n!wget http<span class=\"token punctuation\">:</span><span class=\"token operator\">//</span>www<span class=\"token punctuation\">.</span>manythings<span class=\"token punctuation\">.</span>org<span class=\"token operator\">/</span>anki<span class=\"token operator\">/</span>jpn<span class=\"token operator\">-</span>eng<span class=\"token punctuation\">.</span><span class=\"token builtin\">zip</span>\n!unzip jpn<span class=\"token operator\">-</span>eng<span class=\"token punctuation\">.</span><span class=\"token builtin\">zip</span></code></pre></div>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">--2020-08-19 01:52:31--  http://www.manythings.org/anki/jpn-eng.zip\nResolving www.manythings.org (www.manythings.org)... 104.24.109.196, 104.24.108.196, 172.67.173.198, ...\nConnecting to www.manythings.org (www.manythings.org)|104.24.109.196|:80... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 2303148 (2.2M) [application/zip]\nSaving to: ‘jpn-eng.zip’\n\njpn-eng.zip         100%[===================>]   2.20M  9.70MB/s    in 0.2s\n\n2020-08-19 01:52:32 (9.70 MB/s) - ‘jpn-eng.zip’ saved [2303148/2303148]\n\nArchive:  jpn-eng.zip\n  inflating: jpn.txt\n  inflating: _about.txt</code></pre></div>\n<p>Load data to memory.</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\">data <span class=\"token operator\">=</span> <span class=\"token punctuation\">[</span><span class=\"token punctuation\">]</span>\n\nf1 <span class=\"token operator\">=</span> <span class=\"token builtin\">open</span><span class=\"token punctuation\">(</span><span class=\"token string\">'./jpn.txt'</span><span class=\"token punctuation\">,</span> <span class=\"token string\">'r'</span><span class=\"token punctuation\">)</span>\ndata <span class=\"token operator\">+=</span> <span class=\"token punctuation\">[</span>x<span class=\"token punctuation\">.</span>rstrip<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>lower<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>split<span class=\"token punctuation\">(</span><span class=\"token string\">'\\t'</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">[</span><span class=\"token punctuation\">:</span><span class=\"token number\">2</span><span class=\"token punctuation\">]</span> <span class=\"token keyword\">for</span> x <span class=\"token keyword\">in</span> tqdm<span class=\"token punctuation\">(</span>f1<span class=\"token punctuation\">.</span>readlines<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">]</span>\nf1<span class=\"token punctuation\">.</span>close<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token string-interpolation\"><span class=\"token string\">f'Loaded </span><span class=\"token interpolation\"><span class=\"token punctuation\">{</span><span class=\"token builtin\">len</span><span class=\"token punctuation\">(</span>data<span class=\"token punctuation\">)</span><span class=\"token punctuation\">}</span></span><span class=\"token string\"> Sentences'</span></span><span class=\"token punctuation\">)</span></code></pre></div>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">Loaded 53594 Sentences</code></pre></div>\n<h1>Text Preprocessing</h1>\n<h3>Handling misspell words &#x26; Clearing Punctuation</h3>\n<p>we’re gonna change the misspell words in english sentences and clearing punctuation from text.</p>\n<p>“aren’t my english bad?” -> “are not my english bad”</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\">mispell_dict <span class=\"token operator\">=</span> <span class=\"token punctuation\">{</span>\n    <span class=\"token string\">\"aren't\"</span> <span class=\"token punctuation\">:</span> <span class=\"token string\">\"are not\"</span><span class=\"token punctuation\">,</span>\n    <span class=\"token string\">\"can't\"</span> <span class=\"token punctuation\">:</span> <span class=\"token string\">\"cannot\"</span><span class=\"token punctuation\">,</span>\n    <span class=\"token string\">\"couldn't\"</span> <span class=\"token punctuation\">:</span> <span class=\"token string\">\"could not\"</span><span class=\"token punctuation\">,</span>\n    <span class=\"token string\">\"didn't\"</span> <span class=\"token punctuation\">:</span> <span class=\"token string\">\"did not\"</span><span class=\"token punctuation\">,</span>\n    <span class=\"token string\">\"doesn't\"</span> <span class=\"token punctuation\">:</span> <span class=\"token string\">\"does not\"</span><span class=\"token punctuation\">,</span>\n    <span class=\"token string\">\"don't\"</span> <span class=\"token punctuation\">:</span> <span class=\"token string\">\"do not\"</span><span class=\"token punctuation\">,</span>\n    <span class=\"token string\">\"hadn't\"</span> <span class=\"token punctuation\">:</span> <span class=\"token string\">\"had not\"</span><span class=\"token punctuation\">,</span>\n    <span class=\"token string\">\"hasn't\"</span> <span class=\"token punctuation\">:</span> <span class=\"token string\">\"has not\"</span><span class=\"token punctuation\">,</span>\n    <span class=\"token string\">\"haven't\"</span> <span class=\"token punctuation\">:</span> <span class=\"token string\">\"have not\"</span><span class=\"token punctuation\">,</span>\n    <span class=\"token string\">\"he'd\"</span> <span class=\"token punctuation\">:</span> <span class=\"token string\">\"he would\"</span><span class=\"token punctuation\">,</span>\n    <span class=\"token string\">\"he'll\"</span> <span class=\"token punctuation\">:</span> <span class=\"token string\">\"he will\"</span><span class=\"token punctuation\">,</span>\n    <span class=\"token string\">\"he's\"</span> <span class=\"token punctuation\">:</span> <span class=\"token string\">\"he is\"</span><span class=\"token punctuation\">,</span>\n    <span class=\"token string\">\"i'd\"</span> <span class=\"token punctuation\">:</span> <span class=\"token string\">\"i would\"</span><span class=\"token punctuation\">,</span>\n    <span class=\"token string\">\"i'd\"</span> <span class=\"token punctuation\">:</span> <span class=\"token string\">\"i had\"</span><span class=\"token punctuation\">,</span>\n    <span class=\"token string\">\"i'll\"</span> <span class=\"token punctuation\">:</span> <span class=\"token string\">\"i will\"</span><span class=\"token punctuation\">,</span>\n    <span class=\"token string\">\"i'm\"</span> <span class=\"token punctuation\">:</span> <span class=\"token string\">\"i am\"</span><span class=\"token punctuation\">,</span>\n    <span class=\"token string\">\"isn't\"</span> <span class=\"token punctuation\">:</span> <span class=\"token string\">\"is not\"</span><span class=\"token punctuation\">,</span>\n    <span class=\"token string\">\"it's\"</span> <span class=\"token punctuation\">:</span> <span class=\"token string\">\"it is\"</span><span class=\"token punctuation\">,</span>\n    <span class=\"token string\">\"it'll\"</span><span class=\"token punctuation\">:</span><span class=\"token string\">\"it will\"</span><span class=\"token punctuation\">,</span>\n    <span class=\"token string\">\"i've\"</span> <span class=\"token punctuation\">:</span> <span class=\"token string\">\"i have\"</span><span class=\"token punctuation\">,</span>\n    <span class=\"token string\">\"let's\"</span> <span class=\"token punctuation\">:</span> <span class=\"token string\">\"let us\"</span><span class=\"token punctuation\">,</span>\n    <span class=\"token string\">\"mightn't\"</span> <span class=\"token punctuation\">:</span> <span class=\"token string\">\"might not\"</span><span class=\"token punctuation\">,</span>\n    <span class=\"token string\">\"mustn't\"</span> <span class=\"token punctuation\">:</span> <span class=\"token string\">\"must not\"</span><span class=\"token punctuation\">,</span>\n    <span class=\"token string\">\"shan't\"</span> <span class=\"token punctuation\">:</span> <span class=\"token string\">\"shall not\"</span><span class=\"token punctuation\">,</span>\n    <span class=\"token string\">\"she'd\"</span> <span class=\"token punctuation\">:</span> <span class=\"token string\">\"she would\"</span><span class=\"token punctuation\">,</span>\n    <span class=\"token string\">\"she'll\"</span> <span class=\"token punctuation\">:</span> <span class=\"token string\">\"she will\"</span><span class=\"token punctuation\">,</span>\n    <span class=\"token string\">\"she's\"</span> <span class=\"token punctuation\">:</span> <span class=\"token string\">\"she is\"</span><span class=\"token punctuation\">,</span>\n    <span class=\"token string\">\"shouldn't\"</span> <span class=\"token punctuation\">:</span> <span class=\"token string\">\"should not\"</span><span class=\"token punctuation\">,</span>\n    <span class=\"token string\">\"that's\"</span> <span class=\"token punctuation\">:</span> <span class=\"token string\">\"that is\"</span><span class=\"token punctuation\">,</span>\n    <span class=\"token string\">\"there's\"</span> <span class=\"token punctuation\">:</span> <span class=\"token string\">\"there is\"</span><span class=\"token punctuation\">,</span>\n    <span class=\"token string\">\"they'd\"</span> <span class=\"token punctuation\">:</span> <span class=\"token string\">\"they would\"</span><span class=\"token punctuation\">,</span>\n    <span class=\"token string\">\"they'll\"</span> <span class=\"token punctuation\">:</span> <span class=\"token string\">\"they will\"</span><span class=\"token punctuation\">,</span>\n    <span class=\"token string\">\"they're\"</span> <span class=\"token punctuation\">:</span> <span class=\"token string\">\"they are\"</span><span class=\"token punctuation\">,</span>\n    <span class=\"token string\">\"they've\"</span> <span class=\"token punctuation\">:</span> <span class=\"token string\">\"they have\"</span><span class=\"token punctuation\">,</span>\n    <span class=\"token string\">\"we'd\"</span> <span class=\"token punctuation\">:</span> <span class=\"token string\">\"we would\"</span><span class=\"token punctuation\">,</span>\n    <span class=\"token string\">\"we're\"</span> <span class=\"token punctuation\">:</span> <span class=\"token string\">\"we are\"</span><span class=\"token punctuation\">,</span>\n    <span class=\"token string\">\"weren't\"</span> <span class=\"token punctuation\">:</span> <span class=\"token string\">\"were not\"</span><span class=\"token punctuation\">,</span>\n    <span class=\"token string\">\"we've\"</span> <span class=\"token punctuation\">:</span> <span class=\"token string\">\"we have\"</span><span class=\"token punctuation\">,</span>\n    <span class=\"token string\">\"what'll\"</span> <span class=\"token punctuation\">:</span> <span class=\"token string\">\"what will\"</span><span class=\"token punctuation\">,</span>\n    <span class=\"token string\">\"what're\"</span> <span class=\"token punctuation\">:</span> <span class=\"token string\">\"what are\"</span><span class=\"token punctuation\">,</span>\n    <span class=\"token string\">\"what's\"</span> <span class=\"token punctuation\">:</span> <span class=\"token string\">\"what is\"</span><span class=\"token punctuation\">,</span>\n    <span class=\"token string\">\"what've\"</span> <span class=\"token punctuation\">:</span> <span class=\"token string\">\"what have\"</span><span class=\"token punctuation\">,</span>\n    <span class=\"token string\">\"where's\"</span> <span class=\"token punctuation\">:</span> <span class=\"token string\">\"where is\"</span><span class=\"token punctuation\">,</span>\n    <span class=\"token string\">\"who'd\"</span> <span class=\"token punctuation\">:</span> <span class=\"token string\">\"who would\"</span><span class=\"token punctuation\">,</span>\n    <span class=\"token string\">\"who'll\"</span> <span class=\"token punctuation\">:</span> <span class=\"token string\">\"who will\"</span><span class=\"token punctuation\">,</span>\n    <span class=\"token string\">\"who're\"</span> <span class=\"token punctuation\">:</span> <span class=\"token string\">\"who are\"</span><span class=\"token punctuation\">,</span>\n    <span class=\"token string\">\"who's\"</span> <span class=\"token punctuation\">:</span> <span class=\"token string\">\"who is\"</span><span class=\"token punctuation\">,</span>\n    <span class=\"token string\">\"who've\"</span> <span class=\"token punctuation\">:</span> <span class=\"token string\">\"who have\"</span><span class=\"token punctuation\">,</span>\n    <span class=\"token string\">\"won't\"</span> <span class=\"token punctuation\">:</span> <span class=\"token string\">\"will not\"</span><span class=\"token punctuation\">,</span>\n    <span class=\"token string\">\"wouldn't\"</span> <span class=\"token punctuation\">:</span> <span class=\"token string\">\"would not\"</span><span class=\"token punctuation\">,</span>\n    <span class=\"token string\">\"you'd\"</span> <span class=\"token punctuation\">:</span> <span class=\"token string\">\"you would\"</span><span class=\"token punctuation\">,</span>\n    <span class=\"token string\">\"you'll\"</span> <span class=\"token punctuation\">:</span> <span class=\"token string\">\"you will\"</span><span class=\"token punctuation\">,</span>\n    <span class=\"token string\">\"you're\"</span> <span class=\"token punctuation\">:</span> <span class=\"token string\">\"you are\"</span><span class=\"token punctuation\">,</span>\n    <span class=\"token string\">\"you've\"</span> <span class=\"token punctuation\">:</span> <span class=\"token string\">\"you have\"</span><span class=\"token punctuation\">,</span>\n    <span class=\"token string\">\"'re\"</span><span class=\"token punctuation\">:</span> <span class=\"token string\">\" are\"</span><span class=\"token punctuation\">,</span>\n    <span class=\"token string\">\"wasn't\"</span><span class=\"token punctuation\">:</span> <span class=\"token string\">\"was not\"</span><span class=\"token punctuation\">,</span>\n    <span class=\"token string\">\"we'll\"</span><span class=\"token punctuation\">:</span><span class=\"token string\">\" will\"</span><span class=\"token punctuation\">,</span>\n    <span class=\"token string\">\"didn't\"</span><span class=\"token punctuation\">:</span> <span class=\"token string\">\"did not\"</span><span class=\"token punctuation\">,</span>\n    <span class=\"token string\">\"tryin'\"</span><span class=\"token punctuation\">:</span><span class=\"token string\">\"trying\"</span>\n<span class=\"token punctuation\">}</span>\n\nmispell_re <span class=\"token operator\">=</span> re<span class=\"token punctuation\">.</span><span class=\"token builtin\">compile</span><span class=\"token punctuation\">(</span><span class=\"token string\">'(%s)'</span> <span class=\"token operator\">%</span> <span class=\"token string\">'|'</span><span class=\"token punctuation\">.</span>join<span class=\"token punctuation\">(</span>mispell_dict<span class=\"token punctuation\">.</span>keys<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n\n<span class=\"token keyword\">def</span> <span class=\"token function\">preprocess</span><span class=\"token punctuation\">(</span>text<span class=\"token punctuation\">)</span> <span class=\"token operator\">-</span><span class=\"token operator\">></span> <span class=\"token builtin\">str</span><span class=\"token punctuation\">:</span>\n    <span class=\"token keyword\">def</span> <span class=\"token function\">replace</span><span class=\"token punctuation\">(</span>match<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n        <span class=\"token keyword\">return</span> mispell_dict<span class=\"token punctuation\">[</span>match<span class=\"token punctuation\">.</span>group<span class=\"token punctuation\">(</span><span class=\"token number\">0</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">]</span>\n\n    text <span class=\"token operator\">=</span> mispell_re<span class=\"token punctuation\">.</span>sub<span class=\"token punctuation\">(</span>replace<span class=\"token punctuation\">,</span> text<span class=\"token punctuation\">)</span>\n    <span class=\"token keyword\">return</span> text</code></pre></div>\n<p>Japanese words have their own punctuation like 【this】</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token comment\"># Adding Japanese Punctuation</span>\nstring<span class=\"token punctuation\">.</span>punctuation <span class=\"token operator\">+=</span> <span class=\"token string\">'、。【】「」『』…・〽（）〜？！｡：､；･'</span>\n\nCP <span class=\"token operator\">=</span> <span class=\"token keyword\">lambda</span> x <span class=\"token punctuation\">:</span> x<span class=\"token punctuation\">.</span>translate<span class=\"token punctuation\">(</span><span class=\"token builtin\">str</span><span class=\"token punctuation\">.</span>maketrans<span class=\"token punctuation\">(</span><span class=\"token string\">''</span><span class=\"token punctuation\">,</span> <span class=\"token string\">''</span><span class=\"token punctuation\">,</span> string<span class=\"token punctuation\">.</span>punctuation<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span></code></pre></div>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\">data <span class=\"token operator\">=</span> <span class=\"token punctuation\">[</span>x <span class=\"token keyword\">for</span> x <span class=\"token keyword\">in</span> data <span class=\"token keyword\">if</span> <span class=\"token builtin\">len</span><span class=\"token punctuation\">(</span>x<span class=\"token punctuation\">)</span> <span class=\"token operator\">==</span> <span class=\"token number\">2</span><span class=\"token punctuation\">]</span>\n\neng_data <span class=\"token operator\">=</span> <span class=\"token punctuation\">[</span>CP<span class=\"token punctuation\">(</span>preprocess<span class=\"token punctuation\">(</span>x<span class=\"token punctuation\">[</span><span class=\"token number\">0</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span> <span class=\"token keyword\">for</span> x <span class=\"token keyword\">in</span> data<span class=\"token punctuation\">]</span>\njpn_data <span class=\"token operator\">=</span> <span class=\"token punctuation\">[</span>CP<span class=\"token punctuation\">(</span>x<span class=\"token punctuation\">[</span><span class=\"token number\">1</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span> <span class=\"token keyword\">for</span> x <span class=\"token keyword\">in</span> data<span class=\"token punctuation\">]</span></code></pre></div>\n<h3>Segmenting Japanese Sentences</h3>\n<p>Unlike english sentence we can tokenize it by splitting words with space just like this,</p>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">'This is english or i think so'.split()\n\nOutput:\n['This', 'is', 'english', 'or', 'i', 'think', 'so']</code></pre></div>\n<p>but in Japanese we can’t do it that way. Here we gonna use Janome Tokenizer to segmenting Japanese sentence and adding space to it so Keras Tokenizer can handle it.</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token comment\"># Initialize Janome Tokenizer</span>\ntoken_jp <span class=\"token operator\">=</span> janome_tokenizer<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span></code></pre></div>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\">sample_text <span class=\"token operator\">=</span> <span class=\"token string\">'ここで私は英語で話している'</span>\n<span class=\"token string\">' '</span><span class=\"token punctuation\">.</span>join<span class=\"token punctuation\">(</span><span class=\"token punctuation\">[</span>word <span class=\"token keyword\">for</span> word <span class=\"token keyword\">in</span> token_jp<span class=\"token punctuation\">.</span>tokenize<span class=\"token punctuation\">(</span>sample_text<span class=\"token punctuation\">,</span> wakati<span class=\"token operator\">=</span><span class=\"token boolean\">True</span><span class=\"token punctuation\">)</span> \\\n          <span class=\"token keyword\">if</span> word <span class=\"token operator\">!=</span> <span class=\"token string\">' '</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span></code></pre></div>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">'ここ で 私 は 英語 で 話し て いる'</code></pre></div>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token comment\"># Apply to Japanese Sentences</span>\njpn_data <span class=\"token operator\">=</span> <span class=\"token punctuation\">[</span><span class=\"token string\">' '</span><span class=\"token punctuation\">.</span>join<span class=\"token punctuation\">(</span><span class=\"token punctuation\">[</span>word <span class=\"token keyword\">for</span> word <span class=\"token keyword\">in</span> token_jp<span class=\"token punctuation\">.</span>tokenize<span class=\"token punctuation\">(</span>x<span class=\"token punctuation\">,</span> wakati<span class=\"token operator\">=</span><span class=\"token boolean\">True</span><span class=\"token punctuation\">)</span> \\\n                      <span class=\"token keyword\">if</span> word <span class=\"token operator\">!=</span> <span class=\"token string\">' '</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span> <span class=\"token keyword\">for</span> x <span class=\"token keyword\">in</span> tqdm<span class=\"token punctuation\">(</span>jpn_data<span class=\"token punctuation\">)</span><span class=\"token punctuation\">]</span></code></pre></div>\n<p>For evaluating our model let’s split our data.</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\">eng_train<span class=\"token punctuation\">,</span> eng_test<span class=\"token punctuation\">,</span> jpn_train<span class=\"token punctuation\">,</span> jpn_test <span class=\"token operator\">=</span> \\\ntrain_test_split<span class=\"token punctuation\">(</span>eng_data<span class=\"token punctuation\">,</span> jpn_data<span class=\"token punctuation\">,</span> test_size <span class=\"token operator\">=</span> <span class=\"token number\">0.04</span><span class=\"token punctuation\">,</span> random_state <span class=\"token operator\">=</span> <span class=\"token number\">42</span><span class=\"token punctuation\">)</span>\n\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span>f\"Splitting to <span class=\"token punctuation\">{</span><span class=\"token builtin\">len</span><span class=\"token punctuation\">(</span>eng_train<span class=\"token punctuation\">)</span><span class=\"token punctuation\">}</span> Train data <span class=\"token keyword\">and</span> \\\n<span class=\"token punctuation\">{</span><span class=\"token builtin\">len</span><span class=\"token punctuation\">(</span>eng_test<span class=\"token punctuation\">)</span><span class=\"token punctuation\">}</span> Test data\"<span class=\"token punctuation\">)</span></code></pre></div>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">Splitting to 51450 Train data and 2144 Test data</code></pre></div>\n<h3>Add BOS and EOS</h3>\n<p>We put BOS “Begin of Sequence” and EOS “End of Sequence” to help our decoder recognize begin and end of a sequence.</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\">eng_train <span class=\"token operator\">=</span> <span class=\"token punctuation\">[</span><span class=\"token string\">'bos '</span><span class=\"token operator\">+</span> x <span class=\"token operator\">+</span> <span class=\"token string\">' eos'</span> <span class=\"token keyword\">for</span> x <span class=\"token keyword\">in</span> eng_train <span class=\"token operator\">+</span> <span class=\"token punctuation\">[</span><span class=\"token string\">'unk unk unk'</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">]</span>\njpn_train <span class=\"token operator\">=</span> <span class=\"token punctuation\">[</span><span class=\"token string\">'bos '</span><span class=\"token operator\">+</span> x <span class=\"token operator\">+</span> <span class=\"token string\">' eos'</span> <span class=\"token keyword\">for</span> x <span class=\"token keyword\">in</span> jpn_train <span class=\"token operator\">+</span> <span class=\"token punctuation\">[</span><span class=\"token string\">'unk unk unk'</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">]</span>\n\neng_val <span class=\"token operator\">=</span> <span class=\"token punctuation\">[</span><span class=\"token string\">'bos '</span><span class=\"token operator\">+</span> x <span class=\"token operator\">+</span> <span class=\"token string\">' eos'</span> <span class=\"token keyword\">for</span> x <span class=\"token keyword\">in</span> eng_test<span class=\"token punctuation\">]</span>\njpn_val <span class=\"token operator\">=</span> <span class=\"token punctuation\">[</span><span class=\"token string\">'bos '</span><span class=\"token operator\">+</span> x <span class=\"token operator\">+</span> <span class=\"token string\">' eos'</span> <span class=\"token keyword\">for</span> x <span class=\"token keyword\">in</span> jpn_test<span class=\"token punctuation\">]</span></code></pre></div>\n<h1>Word Tokenizing</h1>\n<p>Here we use Tokenizer API from Keras to make vocabulary and tokenizing our data</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token comment\"># English Tokenizer</span>\nen_tokenizer <span class=\"token operator\">=</span> Tokenizer<span class=\"token punctuation\">(</span>filters<span class=\"token operator\">=</span><span class=\"token string\">''</span><span class=\"token punctuation\">)</span>\nen_tokenizer<span class=\"token punctuation\">.</span>fit_on_texts<span class=\"token punctuation\">(</span>eng_train<span class=\"token punctuation\">)</span>\n\n<span class=\"token comment\"># Japannese Tokenizer</span>\njp_tokenizer <span class=\"token operator\">=</span> Tokenizer<span class=\"token punctuation\">(</span>filters<span class=\"token operator\">=</span><span class=\"token string\">''</span><span class=\"token punctuation\">)</span>\njp_tokenizer<span class=\"token punctuation\">.</span>fit_on_texts<span class=\"token punctuation\">(</span>jpn_train<span class=\"token punctuation\">)</span></code></pre></div>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token string-interpolation\"><span class=\"token string\">f'English vocab size   :'</span></span><span class=\"token punctuation\">,</span> <span class=\"token builtin\">len</span><span class=\"token punctuation\">(</span>en_tokenizer<span class=\"token punctuation\">.</span>word_index<span class=\"token punctuation\">)</span> <span class=\"token operator\">-</span> <span class=\"token number\">3</span><span class=\"token punctuation\">)</span>\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token string-interpolation\"><span class=\"token string\">f'Japanese vocab size  :'</span></span><span class=\"token punctuation\">,</span> <span class=\"token builtin\">len</span><span class=\"token punctuation\">(</span>jp_tokenizer<span class=\"token punctuation\">.</span>word_index<span class=\"token punctuation\">)</span> <span class=\"token operator\">-</span> <span class=\"token number\">3</span><span class=\"token punctuation\">)</span></code></pre></div>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">English vocab size   : 9646\nJapanese vocab size  : 14403</code></pre></div>\n<h2>Word Cloud</h2>\n<p>What comes when doing NLP? It’s Word Cloud. Let’s do it for our vocab.</p>\n<p>Font : <a href=\"https://www.google.com/get/noto/\">Google Noto Fonts</a> -> Noto Sans CJK JP</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\">!wget https<span class=\"token punctuation\">:</span><span class=\"token operator\">//</span>noto<span class=\"token operator\">-</span>website<span class=\"token operator\">-</span><span class=\"token number\">2.</span>storage<span class=\"token punctuation\">.</span>googleapis<span class=\"token punctuation\">.</span>com<span class=\"token operator\">/</span>pkgs<span class=\"token operator\">/</span>NotoSansCJKjp<span class=\"token operator\">-</span>hinted<span class=\"token punctuation\">.</span><span class=\"token builtin\">zip</span>\n!wget https<span class=\"token punctuation\">:</span><span class=\"token operator\">//</span>raw<span class=\"token punctuation\">.</span>githubusercontent<span class=\"token punctuation\">.</span>com<span class=\"token operator\">/</span>Hyuto<span class=\"token operator\">/</span>NMT<span class=\"token operator\">-</span>TF<span class=\"token operator\">-</span>Seq2seq<span class=\"token operator\">-</span>EN<span class=\"token operator\">-</span>JP<span class=\"token operator\">/</span>master<span class=\"token operator\">/</span>Japan<span class=\"token punctuation\">.</span>jpg\n!wget https<span class=\"token punctuation\">:</span><span class=\"token operator\">//</span>raw<span class=\"token punctuation\">.</span>githubusercontent<span class=\"token punctuation\">.</span>com<span class=\"token operator\">/</span>Hyuto<span class=\"token operator\">/</span>NMT<span class=\"token operator\">-</span>TF<span class=\"token operator\">-</span>Seq2seq<span class=\"token operator\">-</span>EN<span class=\"token operator\">-</span>JP<span class=\"token operator\">/</span>master<span class=\"token operator\">/</span>English<span class=\"token punctuation\">.</span>png\n!mkdir font\n!unzip NotoSansCJKjp<span class=\"token operator\">-</span>hinted<span class=\"token punctuation\">.</span><span class=\"token builtin\">zip</span> <span class=\"token operator\">-</span>d <span class=\"token punctuation\">.</span><span class=\"token operator\">/</span>font</code></pre></div>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">--2020-08-19 01:53:52--  https://noto-website-2.storage.googleapis.com/pkgs/NotoSansCJKjp-hinted.zip\nResolving noto-website-2.storage.googleapis.com (noto-website-2.storage.googleapis.com)... 172.217.204.128, 2607:f8b0:400c:c15::80\nConnecting to noto-website-2.storage.googleapis.com (noto-website-2.storage.googleapis.com)|172.217.204.128|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 121096772 (115M) [application/zip]\nSaving to: ‘NotoSansCJKjp-hinted.zip’\n\nNotoSansCJKjp-hinte 100%[===================>] 115.49M  53.3MB/s    in 2.2s\n\n2020-08-19 01:53:54 (53.3 MB/s) - ‘NotoSansCJKjp-hinted.zip’ saved [121096772/121096772]\n\n--2020-08-19 01:53:55--  https://raw.githubusercontent.com/Hyuto/NMT-TF-Seq2seq-EN-JP/master/Japan.jpg\nResolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.200.133\nConnecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.200.133|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 3976 (3.9K) [image/jpeg]\nSaving to: ‘Japan.jpg’\n\nJapan.jpg           100%[===================>]   3.88K  --.-KB/s    in 0s\n\n2020-08-19 01:53:55 (46.7 MB/s) - ‘Japan.jpg’ saved [3976/3976]\n\n--2020-08-19 01:53:56--  https://raw.githubusercontent.com/Hyuto/NMT-TF-Seq2seq-EN-JP/master/English.png\nResolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.200.133\nConnecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.200.133|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 303684 (297K) [image/png]\nSaving to: ‘English.png’\n\nEnglish.png         100%[===================>] 296.57K  --.-KB/s    in 0.1s\n\n2020-08-19 01:53:56 (2.18 MB/s) - ‘English.png’ saved [303684/303684]\n\nArchive:  NotoSansCJKjp-hinted.zip\n  inflating: ./font/LICENSE_OFL.txt\n  inflating: ./font/NotoSansCJKjp-Black.otf\n  inflating: ./font/NotoSansCJKjp-Bold.otf\n  inflating: ./font/NotoSansCJKjp-DemiLight.otf\n  inflating: ./font/NotoSansCJKjp-Light.otf\n  inflating: ./font/NotoSansCJKjp-Medium.otf\n  inflating: ./font/NotoSansCJKjp-Regular.otf\n  inflating: ./font/NotoSansCJKjp-Thin.otf\n  inflating: ./font/NotoSansMonoCJKjp-Bold.otf\n  inflating: ./font/NotoSansMonoCJKjp-Regular.otf\n  inflating: ./font/README</code></pre></div>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token keyword\">from</span> wordcloud <span class=\"token keyword\">import</span> WordCloud<span class=\"token punctuation\">,</span> ImageColorGenerator\n<span class=\"token keyword\">from</span> PIL <span class=\"token keyword\">import</span> Image\n\n<span class=\"token keyword\">def</span> <span class=\"token function\">get_words</span><span class=\"token punctuation\">(</span>arr<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n    keys <span class=\"token operator\">=</span> <span class=\"token builtin\">list</span><span class=\"token punctuation\">(</span>arr<span class=\"token punctuation\">.</span>keys<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n    count <span class=\"token operator\">=</span> <span class=\"token builtin\">list</span><span class=\"token punctuation\">(</span>arr<span class=\"token punctuation\">.</span>values<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n    <span class=\"token keyword\">return</span> <span class=\"token string\">' '</span><span class=\"token punctuation\">.</span>join<span class=\"token punctuation\">(</span><span class=\"token punctuation\">[</span>x <span class=\"token keyword\">for</span> _<span class=\"token punctuation\">,</span>x <span class=\"token keyword\">in</span> <span class=\"token builtin\">sorted</span><span class=\"token punctuation\">(</span><span class=\"token builtin\">zip</span><span class=\"token punctuation\">(</span>count<span class=\"token punctuation\">,</span> keys<span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> reverse <span class=\"token operator\">=</span> <span class=\"token boolean\">True</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">[</span><span class=\"token number\">2</span><span class=\"token punctuation\">:</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span>\n\n<span class=\"token keyword\">def</span> <span class=\"token function\">transform</span><span class=\"token punctuation\">(</span>arr<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n    <span class=\"token keyword\">for</span> i <span class=\"token keyword\">in</span> <span class=\"token builtin\">range</span><span class=\"token punctuation\">(</span><span class=\"token builtin\">len</span><span class=\"token punctuation\">(</span>arr<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n        <span class=\"token keyword\">for</span> j <span class=\"token keyword\">in</span> <span class=\"token builtin\">range</span><span class=\"token punctuation\">(</span><span class=\"token builtin\">len</span><span class=\"token punctuation\">(</span>arr<span class=\"token punctuation\">[</span>i<span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n            <span class=\"token keyword\">if</span> <span class=\"token keyword\">not</span> <span class=\"token builtin\">any</span><span class=\"token punctuation\">(</span>arr<span class=\"token punctuation\">[</span>i<span class=\"token punctuation\">]</span><span class=\"token punctuation\">[</span>j<span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n                arr<span class=\"token punctuation\">[</span>i<span class=\"token punctuation\">]</span><span class=\"token punctuation\">[</span>j<span class=\"token punctuation\">]</span> <span class=\"token operator\">=</span> np<span class=\"token punctuation\">.</span>array<span class=\"token punctuation\">(</span><span class=\"token punctuation\">[</span><span class=\"token number\">225</span><span class=\"token punctuation\">,</span> <span class=\"token number\">225</span><span class=\"token punctuation\">,</span> <span class=\"token number\">225</span><span class=\"token punctuation\">,</span> <span class=\"token number\">225</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span>\n    <span class=\"token keyword\">return</span> arr\n\nfont_path <span class=\"token operator\">=</span> <span class=\"token string\">'./font/NotoSansCJKjp-Light.otf'</span>\n\n\nmask <span class=\"token operator\">=</span> <span class=\"token string\">'./English.png'</span>\nmask <span class=\"token operator\">=</span> np<span class=\"token punctuation\">.</span>array<span class=\"token punctuation\">(</span>Image<span class=\"token punctuation\">.</span><span class=\"token builtin\">open</span><span class=\"token punctuation\">(</span>mask<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\nmask <span class=\"token operator\">=</span> transform<span class=\"token punctuation\">(</span>mask<span class=\"token punctuation\">)</span>\nimage_colors <span class=\"token operator\">=</span> ImageColorGenerator<span class=\"token punctuation\">(</span>mask<span class=\"token punctuation\">)</span>\nwords <span class=\"token operator\">=</span> get_words<span class=\"token punctuation\">(</span>en_tokenizer<span class=\"token punctuation\">.</span>word_counts<span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>title<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\nwc <span class=\"token operator\">=</span> WordCloud<span class=\"token punctuation\">(</span>background_color<span class=\"token operator\">=</span><span class=\"token string\">\"white\"</span><span class=\"token punctuation\">,</span> max_words<span class=\"token operator\">=</span><span class=\"token number\">2000</span><span class=\"token punctuation\">,</span> random_state<span class=\"token operator\">=</span><span class=\"token number\">42</span><span class=\"token punctuation\">,</span>\n               width<span class=\"token operator\">=</span>mask<span class=\"token punctuation\">.</span>shape<span class=\"token punctuation\">[</span><span class=\"token number\">1</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">,</span> height<span class=\"token operator\">=</span>mask<span class=\"token punctuation\">.</span>shape<span class=\"token punctuation\">[</span><span class=\"token number\">0</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span>\nwc <span class=\"token operator\">=</span> wc<span class=\"token punctuation\">.</span>generate<span class=\"token punctuation\">(</span>words<span class=\"token punctuation\">)</span>\nfig1<span class=\"token punctuation\">,</span> ax1 <span class=\"token operator\">=</span> plt<span class=\"token punctuation\">.</span>subplots<span class=\"token punctuation\">(</span>figsize<span class=\"token operator\">=</span><span class=\"token punctuation\">(</span><span class=\"token number\">20</span><span class=\"token punctuation\">,</span><span class=\"token number\">15</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\nax1<span class=\"token punctuation\">.</span>imshow<span class=\"token punctuation\">(</span>wc<span class=\"token punctuation\">.</span>recolor<span class=\"token punctuation\">(</span>color_func<span class=\"token operator\">=</span>image_colors<span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> interpolation<span class=\"token operator\">=</span><span class=\"token string\">'bilinear'</span><span class=\"token punctuation\">)</span>\nax1<span class=\"token punctuation\">.</span>axis<span class=\"token punctuation\">(</span><span class=\"token string\">\"off\"</span><span class=\"token punctuation\">)</span>\n\nmask <span class=\"token operator\">=</span> <span class=\"token string\">'./Japan.jpg'</span>\nmask <span class=\"token operator\">=</span> np<span class=\"token punctuation\">.</span>array<span class=\"token punctuation\">(</span>Image<span class=\"token punctuation\">.</span><span class=\"token builtin\">open</span><span class=\"token punctuation\">(</span>mask<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\nimage_colors <span class=\"token operator\">=</span> ImageColorGenerator<span class=\"token punctuation\">(</span>mask<span class=\"token punctuation\">)</span>\nwords <span class=\"token operator\">=</span> get_words<span class=\"token punctuation\">(</span>jp_tokenizer<span class=\"token punctuation\">.</span>word_counts<span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>title<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\nwc <span class=\"token operator\">=</span> WordCloud<span class=\"token punctuation\">(</span>collocations<span class=\"token operator\">=</span><span class=\"token boolean\">False</span><span class=\"token punctuation\">,</span> background_color<span class=\"token operator\">=</span><span class=\"token string\">\"white\"</span><span class=\"token punctuation\">,</span> mode<span class=\"token operator\">=</span><span class=\"token string\">\"RGBA\"</span><span class=\"token punctuation\">,</span>\n               max_words<span class=\"token operator\">=</span><span class=\"token number\">6000</span><span class=\"token punctuation\">,</span> font_path<span class=\"token operator\">=</span>font_path<span class=\"token punctuation\">,</span> contour_width<span class=\"token operator\">=</span><span class=\"token number\">1</span><span class=\"token punctuation\">,</span>\n               scale<span class=\"token operator\">=</span><span class=\"token number\">5</span><span class=\"token punctuation\">,</span> max_font_size <span class=\"token operator\">=</span> <span class=\"token number\">50</span><span class=\"token punctuation\">,</span> relative_scaling<span class=\"token operator\">=</span><span class=\"token number\">0.5</span><span class=\"token punctuation\">,</span>\n               random_state<span class=\"token operator\">=</span><span class=\"token number\">42</span><span class=\"token punctuation\">,</span> width<span class=\"token operator\">=</span>mask<span class=\"token punctuation\">.</span>shape<span class=\"token punctuation\">[</span><span class=\"token number\">1</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">,</span> height<span class=\"token operator\">=</span>mask<span class=\"token punctuation\">.</span>shape<span class=\"token punctuation\">[</span><span class=\"token number\">0</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span>\nwc <span class=\"token operator\">=</span> wc<span class=\"token punctuation\">.</span>generate<span class=\"token punctuation\">(</span>words<span class=\"token punctuation\">)</span>\nfig2<span class=\"token punctuation\">,</span> ax2 <span class=\"token operator\">=</span> plt<span class=\"token punctuation\">.</span>subplots<span class=\"token punctuation\">(</span>figsize<span class=\"token operator\">=</span><span class=\"token punctuation\">(</span><span class=\"token number\">20</span><span class=\"token punctuation\">,</span><span class=\"token number\">15</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\nax2<span class=\"token punctuation\">.</span>imshow<span class=\"token punctuation\">(</span>wc<span class=\"token punctuation\">.</span>recolor<span class=\"token punctuation\">(</span>color_func<span class=\"token operator\">=</span>image_colors<span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> interpolation<span class=\"token operator\">=</span><span class=\"token string\">'bilinear'</span><span class=\"token punctuation\">)</span>\nax2<span class=\"token punctuation\">.</span>axis<span class=\"token punctuation\">(</span><span class=\"token string\">\"off\"</span><span class=\"token punctuation\">)</span>\n\nfig1<span class=\"token punctuation\">.</span>savefig<span class=\"token punctuation\">(</span><span class=\"token string\">'WC_English.png'</span><span class=\"token punctuation\">)</span>\nfig2<span class=\"token punctuation\">.</span>savefig<span class=\"token punctuation\">(</span><span class=\"token string\">'WC_Japanese.png'</span><span class=\"token punctuation\">)</span>\nplt<span class=\"token punctuation\">.</span>close<span class=\"token punctuation\">(</span>fig1<span class=\"token punctuation\">)</span>\nplt<span class=\"token punctuation\">.</span>close<span class=\"token punctuation\">(</span>fig2<span class=\"token punctuation\">)</span>\n\n!rm <span class=\"token operator\">-</span>rf <span class=\"token punctuation\">.</span><span class=\"token operator\">/</span>font</code></pre></div>\n<h2>WordCloud</h2>\n<p><strong>English</strong></p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 630px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/925d8b9db09014ceb7735de6722223fc/07a9c/WC_English.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 75.31645569620254%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAPCAYAAADkmO9VAAAACXBIWXMAAAsSAAALEgHS3X78AAACg0lEQVQ4y3WUe1PaUBDF/f5fojPtdOpM31qtDxQReQfQJAQSEhIICAQIT5WQhF8HrA62sH/cvbsze2bvPbtnj7+2XC75z7akdtlL/d5msFj4zD2PMAzx/YCQJU+TGaNml8Dz8D0P7/GR+fQB31sQLBYEfvBav/J7q+MlMRpPqVlNalYL+75Lr9Ul8/WcXCSNKWu0iiWMeA5XNbAydyjnSSYd5xVsDbjZYdsZYDXuKWsWomYhRFLkLrPkErfoQonCp2MK+7+pZ0TUsySp71fULYcw3NFhtzek1uhyq1hkZZPEWZqb4wSxwziVwyiZ9weopzd0FAMzXyZ/LWK2x+vv2QroDkbUzBaKZiOW60iKxfX+KZfvftLNSpg3ecq/rhjIGj3FQDmK0Tdbb5+8Ceg4LpJUparbFAsVBNEgmy2TjwrYxQpWPE/16IpZUWYmV3goyPTkCsFuUmbcNx3seoe61SZ9kSN+lECNCXRiWTo5kV6pSldSsW417EKZe6mynoqtgEN3REfRGZhNnHYfJZpDu0jzpFvoHw+QTxKMW11aJZ22VqcqlKnlxN1j8+AOcSSVodFgvGI8r9BMFQlVHf3DD5InWe4EjXHbwRCr2GqdiqjhBxuksAH4NJkyUHT6moXbaNOIC6T3zximb+l8OyEVKRCLyYjXRUoFDUPSMXSbxS6WZ30XRzMZmk06RpPI5yhnX2IYgkI9mka+SHOXlJHzJWqqQdOyMavmesO2Ak6cAbZSxVAMZMXk/EYikdcQFYuWM+Dh6ZHpbMZoPGYynTCdTXHd4RZSnqO1X+2vt1gwn88JA58g8J8LlkvCIFjf1w0s3wrKG3F4FZblboX5V402md1k+Q9clGHGV8oF9gAAAABJRU5ErkJggg=='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"wc-english\"\n        title=\"wc-english\"\n        src=\"/static/925d8b9db09014ceb7735de6722223fc/f058b/WC_English.png\"\n        srcset=\"/static/925d8b9db09014ceb7735de6722223fc/c26ae/WC_English.png 158w,\n/static/925d8b9db09014ceb7735de6722223fc/6bdcf/WC_English.png 315w,\n/static/925d8b9db09014ceb7735de6722223fc/f058b/WC_English.png 630w,\n/static/925d8b9db09014ceb7735de6722223fc/40601/WC_English.png 945w,\n/static/925d8b9db09014ceb7735de6722223fc/78612/WC_English.png 1260w,\n/static/925d8b9db09014ceb7735de6722223fc/07a9c/WC_English.png 1440w\"\n        sizes=\"(max-width: 630px) 100vw, 630px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n  </a>\n    </span></p>\n<p><strong>Japanese</strong></p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 630px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/97526e703f2d47eceebc07ee03968a2e/07a9c/WC_Japanese.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 75.31645569620254%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAPCAYAAADkmO9VAAAACXBIWXMAAAsSAAALEgHS3X78AAAA70lEQVQ4y62UDQuCMBiE/f+/LaikKCvsO/qiaE6duu1iA0Pi3VzQCzIQ9+yOuxmBGK31Z22f7nvfRKFAEk4cEPlgXahL8bfqfoVKkRtd9p3AdoPMS4jzHeX2ZNcmy0m1/ZalRMM4sukKbDjDazBFNllCHK+QpQi33H6gRIVicwSLE2TxwoL5fA02mqO6Pcmg/MC6QZHuweIFeLIBn6UWzMYJ6sfrd4UmDHG5W5Cxa5QZYJEeoKqabIQT+FHZSAswMJ6ska92NhiqTsEpa6lsOPWTQRals6+9ln1do8ofrtBz9XTI1etQSWDfTyLCn+cNu5Sfjc744QQAAAAASUVORK5CYII='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"wc-japanese\"\n        title=\"wc-japanese\"\n        src=\"/static/97526e703f2d47eceebc07ee03968a2e/f058b/WC_Japanese.png\"\n        srcset=\"/static/97526e703f2d47eceebc07ee03968a2e/c26ae/WC_Japanese.png 158w,\n/static/97526e703f2d47eceebc07ee03968a2e/6bdcf/WC_Japanese.png 315w,\n/static/97526e703f2d47eceebc07ee03968a2e/f058b/WC_Japanese.png 630w,\n/static/97526e703f2d47eceebc07ee03968a2e/40601/WC_Japanese.png 945w,\n/static/97526e703f2d47eceebc07ee03968a2e/78612/WC_Japanese.png 1260w,\n/static/97526e703f2d47eceebc07ee03968a2e/07a9c/WC_Japanese.png 1440w\"\n        sizes=\"(max-width: 630px) 100vw, 630px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n  </a>\n    </span></p>\n<p>now let’s transform our train sentences to sequences.</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token keyword\">def</span> <span class=\"token function\">Sequences</span><span class=\"token punctuation\">(</span>texts<span class=\"token punctuation\">,</span> tokenizer<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n    res <span class=\"token operator\">=</span> <span class=\"token punctuation\">[</span><span class=\"token punctuation\">]</span>\n    <span class=\"token keyword\">for</span> text <span class=\"token keyword\">in</span> texts<span class=\"token punctuation\">:</span>\n        seq <span class=\"token operator\">=</span> <span class=\"token punctuation\">[</span><span class=\"token punctuation\">]</span>\n        <span class=\"token keyword\">for</span> w <span class=\"token keyword\">in</span> text<span class=\"token punctuation\">.</span>split<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n            <span class=\"token keyword\">try</span><span class=\"token punctuation\">:</span>\n                seq<span class=\"token punctuation\">.</span>append<span class=\"token punctuation\">(</span>tokenizer<span class=\"token punctuation\">.</span>word_index<span class=\"token punctuation\">[</span>w<span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span>\n            <span class=\"token keyword\">except</span><span class=\"token punctuation\">:</span>\n                seq<span class=\"token punctuation\">.</span>append<span class=\"token punctuation\">(</span>tokenizer<span class=\"token punctuation\">.</span>word_index<span class=\"token punctuation\">[</span><span class=\"token string\">'unk'</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span>\n        res<span class=\"token punctuation\">.</span>append<span class=\"token punctuation\">(</span>seq<span class=\"token punctuation\">)</span>\n    <span class=\"token keyword\">return</span> res</code></pre></div>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token comment\"># Transform Sentences to Sequences</span>\ndata_en <span class=\"token operator\">=</span> en_tokenizer<span class=\"token punctuation\">.</span>texts_to_sequences<span class=\"token punctuation\">(</span>eng_train<span class=\"token punctuation\">)</span>\ndata_jp <span class=\"token operator\">=</span> jp_tokenizer<span class=\"token punctuation\">.</span>texts_to_sequences<span class=\"token punctuation\">(</span>jpn_train<span class=\"token punctuation\">)</span>\n\nval_en <span class=\"token operator\">=</span> Sequences<span class=\"token punctuation\">(</span>eng_val<span class=\"token punctuation\">,</span> en_tokenizer<span class=\"token punctuation\">)</span>\nval_jp <span class=\"token operator\">=</span> Sequences<span class=\"token punctuation\">(</span>jpn_val<span class=\"token punctuation\">,</span> jp_tokenizer<span class=\"token punctuation\">)</span></code></pre></div>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\">plt<span class=\"token punctuation\">.</span>figure<span class=\"token punctuation\">(</span>figsize <span class=\"token operator\">=</span> <span class=\"token punctuation\">(</span><span class=\"token number\">8</span><span class=\"token punctuation\">,</span><span class=\"token number\">8</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\nsns<span class=\"token punctuation\">.</span>distplot<span class=\"token punctuation\">(</span><span class=\"token punctuation\">[</span><span class=\"token builtin\">len</span><span class=\"token punctuation\">(</span>x<span class=\"token punctuation\">)</span> <span class=\"token keyword\">for</span> x <span class=\"token keyword\">in</span> data_en<span class=\"token punctuation\">]</span><span class=\"token punctuation\">,</span> label<span class=\"token operator\">=</span><span class=\"token string\">'English'</span><span class=\"token punctuation\">)</span>\nsns<span class=\"token punctuation\">.</span>distplot<span class=\"token punctuation\">(</span><span class=\"token punctuation\">[</span><span class=\"token builtin\">len</span><span class=\"token punctuation\">(</span>x<span class=\"token punctuation\">)</span> <span class=\"token keyword\">for</span> x <span class=\"token keyword\">in</span> data_jp<span class=\"token punctuation\">]</span><span class=\"token punctuation\">,</span> label<span class=\"token operator\">=</span><span class=\"token string\">'Japanese'</span><span class=\"token punctuation\">)</span>\nplt<span class=\"token punctuation\">.</span>title<span class=\"token punctuation\">(</span><span class=\"token string\">'Distribution of Sentences Length'</span><span class=\"token punctuation\">)</span>\nplt<span class=\"token punctuation\">.</span>legend<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\nplt<span class=\"token punctuation\">.</span>show<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span></code></pre></div>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 490px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/8ce63a24a5b8ffbe5487aa1074fa8a20/41d3c/output_32_0.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 98.10126582278481%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAUCAYAAACNiR0NAAAACXBIWXMAAAsSAAALEgHS3X78AAACwklEQVQ4y6WUW08TQRTH91UTL/XViES8pRD7RELDEwnhkY/gix/AJx8MGm+BGCWR1jbGxAh4ibEQiAoJIBcxxkBSwYhEpCz0Srud3XZ3u213ZnbnmG5boSGGFs/m7Ewyk9/+/2d2DudwODiv13PY5XKf8HoeH3e7+mwDg4PHOjs765ubmx1Op7Opo6OjoaWlpam1tdXe3t5+zul0Nra1tZ33+XxHHz3os3lcHtvAQL/N6/Ue4QCAEwThpWEYlFKKKKWSYRgSxiSt67qSz+dVjLFcnu8aZdMwJCWHU/EkQps8ryOE5iygLMujAADEBCAG1BR5E4OkpCEWjQJC6DsXCAQ4NZN5C8BAp0DylJkAzGTMyv3CBAYmAFBLECH+ssLhAjBPgKZyjBUWC+/ibG8wK4sbrIcxy5eu698sIEKiZVnOM4q0/YHFdfZ3rABGIhFOVTO+wjejCtCwvAOsNiqAsViMU0rAtSTQVaEMZFVD/2n5N2J0KWYVu8JWzcB0unAoACsJRr+GypwDKuR5vmQZwL+N6VxY+3+FqXTaUriU0OloKMpyJtl9grUDkwhZNVyVsnQkFGYyze0AoUZgMBjkFFW1LP9UEB2J8mxdUQ6uMJtROVVVhgoXblEU6LvtDeYXJQAwSz/3/tA9liVJHKUmwLwYouOJDfZJiANh1ddxD1BVlCFsmDAvBsmEsMEm4mGGaNq6stV0CMYYLQH9XGwzyOnZ3LBKMUwmeTYt8jApbMGKtg0AtfUyjPFysduIqTdbmggf4r+UycS6NpUIaFPxrcwPJaJJuqpliZ7BFGuYEo0YRCO0MjElKjUMls1lv3Acxx3qutF15vn0WMOT2ff1V3u7G+8Oui+89s/UXe7pbrxyv9f+bGHh1DXvHfvtpz32j2uf63pfuC7ect9rGl+eOt0//urs9Yc3L40tTpyZmZ89+Qdd/6OoUfpEpAAAAABJRU5ErkJggg=='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"png\"\n        title=\"png\"\n        src=\"/static/8ce63a24a5b8ffbe5487aa1074fa8a20/41d3c/output_32_0.png\"\n        srcset=\"/static/8ce63a24a5b8ffbe5487aa1074fa8a20/c26ae/output_32_0.png 158w,\n/static/8ce63a24a5b8ffbe5487aa1074fa8a20/6bdcf/output_32_0.png 315w,\n/static/8ce63a24a5b8ffbe5487aa1074fa8a20/41d3c/output_32_0.png 490w\"\n        sizes=\"(max-width: 490px) 100vw, 490px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n  </a>\n    </span></p>\n<p>based on the distplot English sentences contains about 20 - 40 words while Japanese have more wider range.</p>\n<p>Let’s check their max length</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\">max_en <span class=\"token operator\">=</span> <span class=\"token builtin\">max</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">[</span><span class=\"token builtin\">len</span><span class=\"token punctuation\">(</span>x<span class=\"token punctuation\">)</span> <span class=\"token keyword\">for</span> x <span class=\"token keyword\">in</span> data_en<span class=\"token punctuation\">]</span> <span class=\"token operator\">+</span> <span class=\"token punctuation\">[</span><span class=\"token builtin\">len</span><span class=\"token punctuation\">(</span>x<span class=\"token punctuation\">)</span> <span class=\"token keyword\">for</span> x <span class=\"token keyword\">in</span> val_en<span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span>\nmax_jp <span class=\"token operator\">=</span> <span class=\"token builtin\">max</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">[</span><span class=\"token builtin\">len</span><span class=\"token punctuation\">(</span>x<span class=\"token punctuation\">)</span> <span class=\"token keyword\">for</span> x <span class=\"token keyword\">in</span> data_jp<span class=\"token punctuation\">]</span> <span class=\"token operator\">+</span> <span class=\"token punctuation\">[</span><span class=\"token builtin\">len</span><span class=\"token punctuation\">(</span>x<span class=\"token punctuation\">)</span> <span class=\"token keyword\">for</span> x <span class=\"token keyword\">in</span> val_jp<span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span>\n\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token string-interpolation\"><span class=\"token string\">f'Maximum length of English sequences is  </span><span class=\"token interpolation\"><span class=\"token punctuation\">{</span>max_en<span class=\"token punctuation\">}</span></span><span class=\"token string\">'</span></span><span class=\"token punctuation\">)</span>\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token string-interpolation\"><span class=\"token string\">f'Maximum length of Japanese sequences is </span><span class=\"token interpolation\"><span class=\"token punctuation\">{</span>max_jp<span class=\"token punctuation\">}</span></span><span class=\"token string\">'</span></span><span class=\"token punctuation\">)</span></code></pre></div>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">Maximum length of English sequences is  49\nMaximum length of Japanese sequences is 54</code></pre></div>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token comment\"># Padding Sequences</span>\ndata_en <span class=\"token operator\">=</span> pad_sequences<span class=\"token punctuation\">(</span>data_en<span class=\"token punctuation\">,</span> padding<span class=\"token operator\">=</span><span class=\"token string\">'post'</span><span class=\"token punctuation\">,</span> maxlen <span class=\"token operator\">=</span> max_en<span class=\"token punctuation\">)</span>\ndata_jp <span class=\"token operator\">=</span> pad_sequences<span class=\"token punctuation\">(</span>data_jp<span class=\"token punctuation\">,</span> padding<span class=\"token operator\">=</span><span class=\"token string\">'post'</span><span class=\"token punctuation\">,</span> maxlen <span class=\"token operator\">=</span> max_jp<span class=\"token punctuation\">)</span>\n\nval_en <span class=\"token operator\">=</span> pad_sequences<span class=\"token punctuation\">(</span>val_en<span class=\"token punctuation\">,</span> padding<span class=\"token operator\">=</span><span class=\"token string\">'post'</span><span class=\"token punctuation\">,</span> maxlen <span class=\"token operator\">=</span> max_en<span class=\"token punctuation\">)</span>\nval_jp <span class=\"token operator\">=</span> pad_sequences<span class=\"token punctuation\">(</span>val_jp<span class=\"token punctuation\">,</span> padding<span class=\"token operator\">=</span><span class=\"token string\">'post'</span><span class=\"token punctuation\">,</span> maxlen <span class=\"token operator\">=</span> max_jp<span class=\"token punctuation\">)</span></code></pre></div>\n<h1>Build &#x26; Train Model</h1>\n<p>Now it’s the time brace yourself.</p>\n<p>We’ll build model based on Seq2seq approaches with Attention optimization.</p>\n<blockquote>\n<p>Seq2Seq is a method of encoder-decoder based machine translation that maps an input of sequence to an output of sequence with a tag and attention value. The idea is to use 2 RNN that will work together with a special token and trying to predict the next state sequence from the previous sequence.</p>\n</blockquote>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 630px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/a5a04f4dc28fd14e1a9c5e883fda9bf5/5a190/seq2seq.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 50.632911392405056%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAKCAIAAAA7N+mxAAAACXBIWXMAAAsTAAALEwEAmpwYAAABeElEQVQoz22R2W7bMBBF/f/fE+SlD0WA5K1tHFuKbS22ZWqjJNJcRdLm0ipKUBvtATHALBd3wFmEe5xzWZZRRkMIPvi56P38fJJkjPGrdeZiQwiL8A/GGGvtbeViOCdgFDBPl+xcNtWW4PL/4nsmc85AWz6t3x6T7bf18gHWzwTHzrnF/dhNvEk73P1cv5awXm3X2SmP0vf0mM/OU59JUrRpj9uyO3S4rvqi7k8dagDMjdFEqAPE7YD3LWoGfGj6lohJ7P0k7s4wLn5lRZI28b5KdyDagTgDyaZcEYorRHZIRvkxxeOmqN7rYY+4s/bTWY2kKlejKE/Fsu9SAKIe7jg51lXsvdXmAvtByBEOSEjZDUgq9Xdto3qBXzWPweEF7F+yzXfU/tAs4ujNXrX3nlIy73jL54dZ61qIrAuYsKu1Uiou9OXqhdTz8b337gP7xSQ2xgAAKKWMETXK8xkLwTljlJJRCsqotVZrzRj7iFRKKYRQSv051W8r8je8hZRFFgAAAABJRU5ErkJggg=='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"Seq2Seq\"\n        title=\"Seq2Seq\"\n        src=\"/static/a5a04f4dc28fd14e1a9c5e883fda9bf5/f058b/seq2seq.png\"\n        srcset=\"/static/a5a04f4dc28fd14e1a9c5e883fda9bf5/c26ae/seq2seq.png 158w,\n/static/a5a04f4dc28fd14e1a9c5e883fda9bf5/6bdcf/seq2seq.png 315w,\n/static/a5a04f4dc28fd14e1a9c5e883fda9bf5/f058b/seq2seq.png 630w,\n/static/a5a04f4dc28fd14e1a9c5e883fda9bf5/5a190/seq2seq.png 800w\"\n        sizes=\"(max-width: 630px) 100vw, 630px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n  </a>\n    </span></p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token comment\"># Config</span>\nepochs <span class=\"token operator\">=</span> <span class=\"token number\">7</span>\nBATCH_SIZE <span class=\"token operator\">=</span> <span class=\"token number\">64</span>\nBUFFER_SIZE <span class=\"token operator\">=</span> <span class=\"token builtin\">len</span><span class=\"token punctuation\">(</span>data_jp<span class=\"token punctuation\">)</span>\nsteps_per_epoch <span class=\"token operator\">=</span> BUFFER_SIZE<span class=\"token operator\">//</span>BATCH_SIZE\nval_steps_per_epoch <span class=\"token operator\">=</span> <span class=\"token builtin\">len</span><span class=\"token punctuation\">(</span>val_jp<span class=\"token punctuation\">)</span> <span class=\"token operator\">//</span> BATCH_SIZE\nembedding_dims <span class=\"token operator\">=</span> <span class=\"token number\">256</span>\nrnn_units <span class=\"token operator\">=</span> <span class=\"token number\">1024</span>\ndense_units <span class=\"token operator\">=</span> <span class=\"token number\">1024</span>\nDtype <span class=\"token operator\">=</span> tf<span class=\"token punctuation\">.</span>float32</code></pre></div>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token keyword\">def</span> <span class=\"token function\">max_len</span><span class=\"token punctuation\">(</span>tensor<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n    <span class=\"token triple-quoted-string string\">\"\"\"\n    Get max len in Sequences\n    \"\"\"</span>\n    <span class=\"token keyword\">return</span> <span class=\"token builtin\">max</span><span class=\"token punctuation\">(</span> <span class=\"token builtin\">len</span><span class=\"token punctuation\">(</span>t<span class=\"token punctuation\">)</span> <span class=\"token keyword\">for</span> t <span class=\"token keyword\">in</span> tensor<span class=\"token punctuation\">)</span></code></pre></div>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token comment\"># Max Len</span>\nTx <span class=\"token operator\">=</span> max_len<span class=\"token punctuation\">(</span>data_en<span class=\"token punctuation\">)</span>\nTy <span class=\"token operator\">=</span> max_len<span class=\"token punctuation\">(</span>data_jp<span class=\"token punctuation\">)</span>\n\n<span class=\"token comment\"># Vocab</span>\ninput_vocab_size <span class=\"token operator\">=</span> <span class=\"token builtin\">len</span><span class=\"token punctuation\">(</span>en_tokenizer<span class=\"token punctuation\">.</span>word_index<span class=\"token punctuation\">)</span> <span class=\"token operator\">+</span> <span class=\"token number\">1</span>   <span class=\"token comment\"># English</span>\noutput_vocab_size <span class=\"token operator\">=</span> <span class=\"token builtin\">len</span><span class=\"token punctuation\">(</span>jp_tokenizer<span class=\"token punctuation\">.</span>word_index<span class=\"token punctuation\">)</span> <span class=\"token operator\">+</span> <span class=\"token number\">1</span>  <span class=\"token comment\"># Japanese</span>\n\n<span class=\"token comment\"># Changing to TF data</span>\ndataset <span class=\"token operator\">=</span> <span class=\"token punctuation\">(</span>tf<span class=\"token punctuation\">.</span>data<span class=\"token punctuation\">.</span>Dataset<span class=\"token punctuation\">.</span>from_tensor_slices<span class=\"token punctuation\">(</span><span class=\"token punctuation\">(</span>data_en<span class=\"token punctuation\">,</span> data_jp<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n           <span class=\"token punctuation\">.</span>shuffle<span class=\"token punctuation\">(</span>BUFFER_SIZE<span class=\"token punctuation\">)</span>\n           <span class=\"token punctuation\">.</span>batch<span class=\"token punctuation\">(</span>BATCH_SIZE<span class=\"token punctuation\">,</span> drop_remainder<span class=\"token operator\">=</span><span class=\"token boolean\">True</span><span class=\"token punctuation\">)</span>\n          <span class=\"token punctuation\">)</span>\n\nval_dataset <span class=\"token operator\">=</span> <span class=\"token punctuation\">(</span>tf<span class=\"token punctuation\">.</span>data<span class=\"token punctuation\">.</span>Dataset<span class=\"token punctuation\">.</span>from_tensor_slices<span class=\"token punctuation\">(</span><span class=\"token punctuation\">(</span>val_en<span class=\"token punctuation\">,</span> val_jp<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n               <span class=\"token punctuation\">.</span>batch<span class=\"token punctuation\">(</span>BATCH_SIZE<span class=\"token punctuation\">)</span>\n              <span class=\"token punctuation\">)</span></code></pre></div>\n<p>Let’s define our based Seq2Seq Model</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token comment\">#ENCODER</span>\n<span class=\"token keyword\">class</span> <span class=\"token class-name\">EncoderNetwork</span><span class=\"token punctuation\">(</span>tf<span class=\"token punctuation\">.</span>keras<span class=\"token punctuation\">.</span>Model<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n    <span class=\"token keyword\">def</span> <span class=\"token function\">__init__</span><span class=\"token punctuation\">(</span>self<span class=\"token punctuation\">,</span>input_vocab_size<span class=\"token punctuation\">,</span>embedding_dims<span class=\"token punctuation\">,</span> rnn_units <span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n        <span class=\"token builtin\">super</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>__init__<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n        self<span class=\"token punctuation\">.</span>encoder_embedding <span class=\"token operator\">=</span> Embedding<span class=\"token punctuation\">(</span>input_dim<span class=\"token operator\">=</span>input_vocab_size<span class=\"token punctuation\">,</span>\n                                           output_dim<span class=\"token operator\">=</span>embedding_dims<span class=\"token punctuation\">)</span>\n        self<span class=\"token punctuation\">.</span>encoder_rnnlayer <span class=\"token operator\">=</span> LSTM<span class=\"token punctuation\">(</span>rnn_units<span class=\"token punctuation\">,</span>return_sequences<span class=\"token operator\">=</span><span class=\"token boolean\">True</span><span class=\"token punctuation\">,</span>\n                                     return_state<span class=\"token operator\">=</span><span class=\"token boolean\">True</span> <span class=\"token punctuation\">)</span>\n\n<span class=\"token comment\">#DECODER</span>\n<span class=\"token keyword\">class</span> <span class=\"token class-name\">DecoderNetwork</span><span class=\"token punctuation\">(</span>tf<span class=\"token punctuation\">.</span>keras<span class=\"token punctuation\">.</span>Model<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n    <span class=\"token keyword\">def</span> <span class=\"token function\">__init__</span><span class=\"token punctuation\">(</span>self<span class=\"token punctuation\">,</span>output_vocab_size<span class=\"token punctuation\">,</span> embedding_dims<span class=\"token punctuation\">,</span> rnn_units<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n        <span class=\"token builtin\">super</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>__init__<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n        self<span class=\"token punctuation\">.</span>decoder_embedding <span class=\"token operator\">=</span> Embedding<span class=\"token punctuation\">(</span>input_dim<span class=\"token operator\">=</span>output_vocab_size<span class=\"token punctuation\">,</span>\n                                           output_dim<span class=\"token operator\">=</span>embedding_dims<span class=\"token punctuation\">)</span>\n        self<span class=\"token punctuation\">.</span>dense_layer <span class=\"token operator\">=</span> Dense<span class=\"token punctuation\">(</span>output_vocab_size<span class=\"token punctuation\">)</span>\n        self<span class=\"token punctuation\">.</span>decoder_rnncell <span class=\"token operator\">=</span> LSTMCell<span class=\"token punctuation\">(</span>rnn_units<span class=\"token punctuation\">)</span>\n        <span class=\"token comment\"># Sampler</span>\n        self<span class=\"token punctuation\">.</span>sampler <span class=\"token operator\">=</span> tfa<span class=\"token punctuation\">.</span>seq2seq<span class=\"token punctuation\">.</span>sampler<span class=\"token punctuation\">.</span>TrainingSampler<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n        <span class=\"token comment\"># Create attention mechanism with memory = None</span>\n        self<span class=\"token punctuation\">.</span>attention_mechanism <span class=\"token operator\">=</span> \\\n            self<span class=\"token punctuation\">.</span>build_attention_mechanism<span class=\"token punctuation\">(</span>dense_units<span class=\"token punctuation\">,</span><span class=\"token boolean\">None</span><span class=\"token punctuation\">,</span>BATCH_SIZE<span class=\"token operator\">*</span><span class=\"token punctuation\">[</span>Tx<span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span>\n        self<span class=\"token punctuation\">.</span>rnn_cell <span class=\"token operator\">=</span> self<span class=\"token punctuation\">.</span>build_rnn_cell<span class=\"token punctuation\">(</span>BATCH_SIZE<span class=\"token punctuation\">)</span>\n        self<span class=\"token punctuation\">.</span>decoder <span class=\"token operator\">=</span> tfa<span class=\"token punctuation\">.</span>seq2seq<span class=\"token punctuation\">.</span>BasicDecoder<span class=\"token punctuation\">(</span>self<span class=\"token punctuation\">.</span>rnn_cell<span class=\"token punctuation\">,</span>\n                                                sampler<span class=\"token operator\">=</span> self<span class=\"token punctuation\">.</span>sampler<span class=\"token punctuation\">,</span>\n                                                output_layer <span class=\"token operator\">=</span> self<span class=\"token punctuation\">.</span>dense_layer\n                                               <span class=\"token punctuation\">)</span>\n\n    <span class=\"token keyword\">def</span> <span class=\"token function\">build_attention_mechanism</span><span class=\"token punctuation\">(</span>self<span class=\"token punctuation\">,</span> units<span class=\"token punctuation\">,</span> memory<span class=\"token punctuation\">,</span> MSL<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n        <span class=\"token triple-quoted-string string\">\"\"\"\n        MSL : Memory Sequence Length\n        \"\"\"</span>\n        <span class=\"token comment\">#return tfa.seq2seq.LuongAttention(units, memory = memory,</span>\n        <span class=\"token comment\">#                                  memory_sequence_length = MSL)</span>\n        <span class=\"token keyword\">return</span> tfa<span class=\"token punctuation\">.</span>seq2seq<span class=\"token punctuation\">.</span>BahdanauAttention<span class=\"token punctuation\">(</span>units<span class=\"token punctuation\">,</span> memory <span class=\"token operator\">=</span> memory<span class=\"token punctuation\">,</span>\n                                             memory_sequence_length <span class=\"token operator\">=</span> MSL<span class=\"token punctuation\">)</span>\n\n    <span class=\"token comment\"># wrap decodernn cell</span>\n    <span class=\"token keyword\">def</span> <span class=\"token function\">build_rnn_cell</span><span class=\"token punctuation\">(</span>self<span class=\"token punctuation\">,</span> batch_size<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n        <span class=\"token keyword\">return</span> tfa<span class=\"token punctuation\">.</span>seq2seq<span class=\"token punctuation\">.</span>AttentionWrapper<span class=\"token punctuation\">(</span>self<span class=\"token punctuation\">.</span>decoder_rnncell<span class=\"token punctuation\">,</span>\n                                            self<span class=\"token punctuation\">.</span>attention_mechanism<span class=\"token punctuation\">,</span>\n                                            attention_layer_size<span class=\"token operator\">=</span>dense_units<span class=\"token punctuation\">)</span>\n\n    <span class=\"token keyword\">def</span> <span class=\"token function\">build_decoder_initial_state</span><span class=\"token punctuation\">(</span>self<span class=\"token punctuation\">,</span> batch_size<span class=\"token punctuation\">,</span> encoder_state<span class=\"token punctuation\">,</span> Dtype<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n        decoder_initial_state <span class=\"token operator\">=</span> self<span class=\"token punctuation\">.</span>rnn_cell<span class=\"token punctuation\">.</span>get_initial_state<span class=\"token punctuation\">(</span>batch_size <span class=\"token operator\">=</span> batch_size<span class=\"token punctuation\">,</span>\n                                                                dtype <span class=\"token operator\">=</span> Dtype<span class=\"token punctuation\">)</span>\n        decoder_initial_state <span class=\"token operator\">=</span> decoder_initial_state<span class=\"token punctuation\">.</span>clone<span class=\"token punctuation\">(</span>cell_state <span class=\"token operator\">=</span> encoder_state<span class=\"token punctuation\">)</span>\n        <span class=\"token keyword\">return</span> decoder_initial_state</code></pre></div>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token comment\"># Build Model</span>\nencoderNetwork <span class=\"token operator\">=</span> EncoderNetwork<span class=\"token punctuation\">(</span>input_vocab_size<span class=\"token punctuation\">,</span> embedding_dims<span class=\"token punctuation\">,</span> rnn_units<span class=\"token punctuation\">)</span>\ndecoderNetwork <span class=\"token operator\">=</span> DecoderNetwork<span class=\"token punctuation\">(</span>output_vocab_size<span class=\"token punctuation\">,</span> embedding_dims<span class=\"token punctuation\">,</span> rnn_units<span class=\"token punctuation\">)</span>\n\n<span class=\"token comment\"># Optimizer</span>\noptimizer <span class=\"token operator\">=</span> tf<span class=\"token punctuation\">.</span>keras<span class=\"token punctuation\">.</span>optimizers<span class=\"token punctuation\">.</span>Adam<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span></code></pre></div>\n<p>Make custom training loop</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token keyword\">def</span> <span class=\"token function\">loss_function</span><span class=\"token punctuation\">(</span>y_pred<span class=\"token punctuation\">,</span> y<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n    <span class=\"token comment\">#shape of y [batch_size, ty]</span>\n    <span class=\"token comment\">#shape of y_pred [batch_size, Ty, output_vocab_size]</span>\n    sparsecategoricalcrossentropy <span class=\"token operator\">=</span> SparseCategoricalCrossentropy<span class=\"token punctuation\">(</span>from_logits<span class=\"token operator\">=</span><span class=\"token boolean\">True</span><span class=\"token punctuation\">,</span>\n                                                                  reduction<span class=\"token operator\">=</span><span class=\"token string\">'none'</span><span class=\"token punctuation\">)</span>\n    loss <span class=\"token operator\">=</span> sparsecategoricalcrossentropy<span class=\"token punctuation\">(</span>y_true<span class=\"token operator\">=</span>y<span class=\"token punctuation\">,</span> y_pred<span class=\"token operator\">=</span>y_pred<span class=\"token punctuation\">)</span>\n    mask <span class=\"token operator\">=</span> tf<span class=\"token punctuation\">.</span>logical_not<span class=\"token punctuation\">(</span>tf<span class=\"token punctuation\">.</span>math<span class=\"token punctuation\">.</span>equal<span class=\"token punctuation\">(</span>y<span class=\"token punctuation\">,</span><span class=\"token number\">0</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>   <span class=\"token comment\">#output 0 for y=0 else output 1</span>\n    mask <span class=\"token operator\">=</span> tf<span class=\"token punctuation\">.</span>cast<span class=\"token punctuation\">(</span>mask<span class=\"token punctuation\">,</span> dtype<span class=\"token operator\">=</span>loss<span class=\"token punctuation\">.</span>dtype<span class=\"token punctuation\">)</span>\n    loss <span class=\"token operator\">=</span> mask <span class=\"token operator\">*</span> loss\n    loss <span class=\"token operator\">=</span> tf<span class=\"token punctuation\">.</span>reduce_mean<span class=\"token punctuation\">(</span>loss<span class=\"token punctuation\">)</span>\n    <span class=\"token keyword\">return</span> loss\n\n<span class=\"token decorator annotation punctuation\">@tf<span class=\"token punctuation\">.</span>function</span>\n<span class=\"token keyword\">def</span> <span class=\"token function\">train_step</span><span class=\"token punctuation\">(</span>input_batch<span class=\"token punctuation\">,</span> output_batch<span class=\"token punctuation\">,</span> encoder_initial_cell_state<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n    <span class=\"token comment\"># initialize loss = 0</span>\n    loss <span class=\"token operator\">=</span> <span class=\"token number\">0</span>\n    <span class=\"token keyword\">with</span> tf<span class=\"token punctuation\">.</span>GradientTape<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span> <span class=\"token keyword\">as</span> tape<span class=\"token punctuation\">:</span>\n        encoder_emb_inp <span class=\"token operator\">=</span> encoderNetwork<span class=\"token punctuation\">.</span>encoder_embedding<span class=\"token punctuation\">(</span>input_batch<span class=\"token punctuation\">)</span>\n        a<span class=\"token punctuation\">,</span> a_tx<span class=\"token punctuation\">,</span> c_tx <span class=\"token operator\">=</span> encoderNetwork<span class=\"token punctuation\">.</span>encoder_rnnlayer<span class=\"token punctuation\">(</span>encoder_emb_inp<span class=\"token punctuation\">,</span>\n                                                        initial_state <span class=\"token operator\">=</span> encoder_initial_cell_state<span class=\"token punctuation\">)</span>\n\n        <span class=\"token comment\"># [last step activations,last memory_state] of</span>\n        <span class=\"token comment\"># encoder passed as input to decoder Network</span>\n\n        <span class=\"token comment\"># Prepare correct Decoder input &amp; output sequence data</span>\n        decoder_input <span class=\"token operator\">=</span> output_batch<span class=\"token punctuation\">[</span><span class=\"token punctuation\">:</span><span class=\"token punctuation\">,</span><span class=\"token punctuation\">:</span><span class=\"token operator\">-</span><span class=\"token number\">1</span><span class=\"token punctuation\">]</span> <span class=\"token comment\"># ignore eos</span>\n        <span class=\"token comment\"># compare logits with timestepped +1 version of decoder_input</span>\n        decoder_output <span class=\"token operator\">=</span> output_batch<span class=\"token punctuation\">[</span><span class=\"token punctuation\">:</span><span class=\"token punctuation\">,</span><span class=\"token number\">1</span><span class=\"token punctuation\">:</span><span class=\"token punctuation\">]</span> <span class=\"token comment\">#ignore bos</span>\n\n        <span class=\"token comment\"># Decoder Embeddings</span>\n        decoder_emb_inp <span class=\"token operator\">=</span> decoderNetwork<span class=\"token punctuation\">.</span>decoder_embedding<span class=\"token punctuation\">(</span>decoder_input<span class=\"token punctuation\">)</span>\n\n        <span class=\"token comment\"># Setting up decoder memory from encoder output</span>\n        <span class=\"token comment\"># and Zero State for AttentionWrapperState</span>\n        decoderNetwork<span class=\"token punctuation\">.</span>attention_mechanism<span class=\"token punctuation\">.</span>setup_memory<span class=\"token punctuation\">(</span>a<span class=\"token punctuation\">)</span>\n        decoder_initial_state <span class=\"token operator\">=</span> decoderNetwork<span class=\"token punctuation\">.</span>build_decoder_initial_state<span class=\"token punctuation\">(</span>BATCH_SIZE<span class=\"token punctuation\">,</span>\n                                                                           encoder_state<span class=\"token operator\">=</span><span class=\"token punctuation\">[</span>a_tx<span class=\"token punctuation\">,</span> c_tx<span class=\"token punctuation\">]</span><span class=\"token punctuation\">,</span>\n                                                                           Dtype<span class=\"token operator\">=</span>tf<span class=\"token punctuation\">.</span>float32<span class=\"token punctuation\">)</span>\n\n        <span class=\"token comment\"># BasicDecoderOutput</span>\n        outputs<span class=\"token punctuation\">,</span> _<span class=\"token punctuation\">,</span> _ <span class=\"token operator\">=</span> decoderNetwork<span class=\"token punctuation\">.</span>decoder<span class=\"token punctuation\">(</span>decoder_emb_inp<span class=\"token punctuation\">,</span>initial_state<span class=\"token operator\">=</span>decoder_initial_state<span class=\"token punctuation\">,</span>\n                                               sequence_length<span class=\"token operator\">=</span>BATCH_SIZE<span class=\"token operator\">*</span><span class=\"token punctuation\">[</span>Ty<span class=\"token operator\">-</span><span class=\"token number\">1</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span>\n\n        logits <span class=\"token operator\">=</span> outputs<span class=\"token punctuation\">.</span>rnn_output\n\n        <span class=\"token comment\"># Calculate loss</span>\n        loss <span class=\"token operator\">=</span> loss_function<span class=\"token punctuation\">(</span>logits<span class=\"token punctuation\">,</span> decoder_output<span class=\"token punctuation\">)</span>\n\n    <span class=\"token comment\"># Returns the list of all layer variables / weights.</span>\n    variables <span class=\"token operator\">=</span> encoderNetwork<span class=\"token punctuation\">.</span>trainable_variables <span class=\"token operator\">+</span> decoderNetwork<span class=\"token punctuation\">.</span>trainable_variables\n    <span class=\"token comment\"># differentiate loss wrt variables</span>\n    gradients <span class=\"token operator\">=</span> tape<span class=\"token punctuation\">.</span>gradient<span class=\"token punctuation\">(</span>loss<span class=\"token punctuation\">,</span> variables<span class=\"token punctuation\">)</span>\n\n    <span class=\"token comment\"># grads_and_vars – List of(gradient, variable) pairs.</span>\n    grads_and_vars <span class=\"token operator\">=</span> <span class=\"token builtin\">zip</span><span class=\"token punctuation\">(</span>gradients<span class=\"token punctuation\">,</span>variables<span class=\"token punctuation\">)</span>\n    optimizer<span class=\"token punctuation\">.</span>apply_gradients<span class=\"token punctuation\">(</span>grads_and_vars<span class=\"token punctuation\">)</span>\n    <span class=\"token keyword\">return</span> loss\n\n<span class=\"token decorator annotation punctuation\">@tf<span class=\"token punctuation\">.</span>function</span>\n<span class=\"token keyword\">def</span> <span class=\"token function\">evaluate</span><span class=\"token punctuation\">(</span>input_batch<span class=\"token punctuation\">,</span> output_batch<span class=\"token punctuation\">,</span> encoder_initial_cell_state<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n    loss <span class=\"token operator\">=</span> <span class=\"token number\">0</span>\n    encoder_emb_inp <span class=\"token operator\">=</span> encoderNetwork<span class=\"token punctuation\">.</span>encoder_embedding<span class=\"token punctuation\">(</span>input_batch<span class=\"token punctuation\">)</span>\n    a<span class=\"token punctuation\">,</span> a_tx<span class=\"token punctuation\">,</span> c_tx <span class=\"token operator\">=</span> encoderNetwork<span class=\"token punctuation\">.</span>encoder_rnnlayer<span class=\"token punctuation\">(</span>encoder_emb_inp<span class=\"token punctuation\">,</span>\n                                                    initial_state <span class=\"token operator\">=</span>encoder_initial_cell_state<span class=\"token punctuation\">)</span>\n    decoder_input <span class=\"token operator\">=</span> output_batch<span class=\"token punctuation\">[</span><span class=\"token punctuation\">:</span><span class=\"token punctuation\">,</span><span class=\"token punctuation\">:</span><span class=\"token operator\">-</span><span class=\"token number\">1</span><span class=\"token punctuation\">]</span>\n    decoder_output <span class=\"token operator\">=</span> output_batch<span class=\"token punctuation\">[</span><span class=\"token punctuation\">:</span><span class=\"token punctuation\">,</span><span class=\"token number\">1</span><span class=\"token punctuation\">:</span><span class=\"token punctuation\">]</span>\n    decoder_emb_inp <span class=\"token operator\">=</span> decoderNetwork<span class=\"token punctuation\">.</span>decoder_embedding<span class=\"token punctuation\">(</span>decoder_input<span class=\"token punctuation\">)</span>\n    decoderNetwork<span class=\"token punctuation\">.</span>attention_mechanism<span class=\"token punctuation\">.</span>setup_memory<span class=\"token punctuation\">(</span>a<span class=\"token punctuation\">)</span>\n    decoder_initial_state <span class=\"token operator\">=</span> decoderNetwork<span class=\"token punctuation\">.</span>build_decoder_initial_state<span class=\"token punctuation\">(</span>BATCH_SIZE<span class=\"token punctuation\">,</span>\n                                                                       encoder_state<span class=\"token operator\">=</span><span class=\"token punctuation\">[</span>a_tx<span class=\"token punctuation\">,</span> c_tx<span class=\"token punctuation\">]</span><span class=\"token punctuation\">,</span>\n                                                                       Dtype<span class=\"token operator\">=</span>tf<span class=\"token punctuation\">.</span>float32<span class=\"token punctuation\">)</span>\n    outputs<span class=\"token punctuation\">,</span> _<span class=\"token punctuation\">,</span> _ <span class=\"token operator\">=</span> decoderNetwork<span class=\"token punctuation\">.</span>decoder<span class=\"token punctuation\">(</span>decoder_emb_inp<span class=\"token punctuation\">,</span>initial_state<span class=\"token operator\">=</span>decoder_initial_state<span class=\"token punctuation\">,</span>\n                                           sequence_length<span class=\"token operator\">=</span>BATCH_SIZE<span class=\"token operator\">*</span><span class=\"token punctuation\">[</span>Ty<span class=\"token operator\">-</span><span class=\"token number\">1</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span>\n    logits <span class=\"token operator\">=</span> outputs<span class=\"token punctuation\">.</span>rnn_output\n    loss <span class=\"token operator\">=</span> loss_function<span class=\"token punctuation\">(</span>logits<span class=\"token punctuation\">,</span> decoder_output<span class=\"token punctuation\">)</span>\n    <span class=\"token keyword\">return</span> loss</code></pre></div>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token comment\"># RNN LSTM hidden and memory state initializer</span>\n<span class=\"token keyword\">def</span> <span class=\"token function\">initialize_initial_state</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n    <span class=\"token keyword\">return</span> <span class=\"token punctuation\">[</span>tf<span class=\"token punctuation\">.</span>zeros<span class=\"token punctuation\">(</span><span class=\"token punctuation\">(</span>BATCH_SIZE<span class=\"token punctuation\">,</span> rnn_units<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> tf<span class=\"token punctuation\">.</span>zeros<span class=\"token punctuation\">(</span><span class=\"token punctuation\">(</span>BATCH_SIZE<span class=\"token punctuation\">,</span> rnn_units<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">]</span></code></pre></div>\n<h1>Calculating BLEU Score</h1>\n<blockquote>\n<p>BLEU (bilingual evaluation understudy) is an algorithm for evaluating the quality of text which has been machine-translated from one natural language to another. Quality is considered to be the correspondence between a machine’s output and that of a human: “the closer a machine translation is to a professional human translation, the better it is”. - Wikipedia</p>\n</blockquote>\n<p>BLEU is a metric for evaluating a generated sentence to a reference sentence.\nA perfect match results in a score of 1.0, whereas a perfect mismatch results in a score of 0.0.</p>\n<p>So now we’re going to define our translation function &#x26; calculate the BLEU score for test data at the end of every epoch while training. Note that test data is sentences that our tokenizer didn’t train with, so there must be some words that our tokenizer didn’t know.\nI’m currently working to fix this issue. Based on keras Tokenizer API it have <code class=\"language-text\">oov_token</code> for handling this but i’m not sure.</p>\n<p>For now i’m handling this by adding <code class=\"language-text\">unk</code> in train dataset so the tokenizer can read it, and then when coming to translation if there is word that our tokenizer don’t know i’ll set it by index of <code class=\"language-text\">unk</code> not very eficient but it works.</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token comment\"># Translate</span>\n<span class=\"token keyword\">def</span> <span class=\"token function\">Translate</span><span class=\"token punctuation\">(</span>input_raw<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n    input_raw <span class=\"token operator\">=</span> CP<span class=\"token punctuation\">(</span>preprocess<span class=\"token punctuation\">(</span>input_raw<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n    input_lines <span class=\"token operator\">=</span> <span class=\"token punctuation\">[</span><span class=\"token string\">'bos '</span><span class=\"token operator\">+</span> input_raw <span class=\"token operator\">+</span> <span class=\"token string\">''</span><span class=\"token punctuation\">]</span>\n\n    input_sequences<span class=\"token punctuation\">,</span> unique <span class=\"token operator\">=</span> <span class=\"token punctuation\">[</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">,</span> <span class=\"token punctuation\">[</span><span class=\"token punctuation\">]</span>\n    <span class=\"token keyword\">for</span> line <span class=\"token keyword\">in</span> input_lines<span class=\"token punctuation\">:</span>\n        temp <span class=\"token operator\">=</span> <span class=\"token punctuation\">[</span><span class=\"token punctuation\">]</span>\n        <span class=\"token keyword\">for</span> w <span class=\"token keyword\">in</span> line<span class=\"token punctuation\">.</span>split<span class=\"token punctuation\">(</span><span class=\"token string\">' '</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n            <span class=\"token keyword\">try</span><span class=\"token punctuation\">:</span>\n                temp<span class=\"token punctuation\">.</span>append<span class=\"token punctuation\">(</span>en_tokenizer<span class=\"token punctuation\">.</span>word_index<span class=\"token punctuation\">[</span>w<span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span>\n            <span class=\"token keyword\">except</span><span class=\"token punctuation\">:</span> <span class=\"token comment\"># Avoid Error</span>\n                unique<span class=\"token punctuation\">.</span>append<span class=\"token punctuation\">(</span>w<span class=\"token punctuation\">)</span>\n                temp<span class=\"token punctuation\">.</span>append<span class=\"token punctuation\">(</span>en_tokenizer<span class=\"token punctuation\">.</span>word_index<span class=\"token punctuation\">[</span><span class=\"token string\">'unk'</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span>\n        input_sequences<span class=\"token punctuation\">.</span>append<span class=\"token punctuation\">(</span>temp<span class=\"token punctuation\">)</span>\n\n    input_sequences <span class=\"token operator\">=</span> pad_sequences<span class=\"token punctuation\">(</span>input_sequences<span class=\"token punctuation\">,</span> maxlen<span class=\"token operator\">=</span>Tx<span class=\"token punctuation\">,</span> padding<span class=\"token operator\">=</span><span class=\"token string\">'post'</span><span class=\"token punctuation\">)</span>\n    inp <span class=\"token operator\">=</span> tf<span class=\"token punctuation\">.</span>convert_to_tensor<span class=\"token punctuation\">(</span>input_sequences<span class=\"token punctuation\">)</span>\n    inference_batch_size <span class=\"token operator\">=</span> input_sequences<span class=\"token punctuation\">.</span>shape<span class=\"token punctuation\">[</span><span class=\"token number\">0</span><span class=\"token punctuation\">]</span>\n    encoder_initial_cell_state <span class=\"token operator\">=</span> <span class=\"token punctuation\">[</span>tf<span class=\"token punctuation\">.</span>zeros<span class=\"token punctuation\">(</span><span class=\"token punctuation\">(</span>inference_batch_size<span class=\"token punctuation\">,</span> rnn_units<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>\n                                  tf<span class=\"token punctuation\">.</span>zeros<span class=\"token punctuation\">(</span><span class=\"token punctuation\">(</span>inference_batch_size<span class=\"token punctuation\">,</span> rnn_units<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">]</span>\n    encoder_emb_inp <span class=\"token operator\">=</span> encoderNetwork<span class=\"token punctuation\">.</span>encoder_embedding<span class=\"token punctuation\">(</span>inp<span class=\"token punctuation\">)</span>\n    a<span class=\"token punctuation\">,</span> a_tx<span class=\"token punctuation\">,</span> c_tx <span class=\"token operator\">=</span> encoderNetwork<span class=\"token punctuation\">.</span>encoder_rnnlayer<span class=\"token punctuation\">(</span>encoder_emb_inp<span class=\"token punctuation\">,</span>\n                                                    initial_state <span class=\"token operator\">=</span> encoder_initial_cell_state<span class=\"token punctuation\">)</span>\n\n    start_tokens <span class=\"token operator\">=</span> tf<span class=\"token punctuation\">.</span>fill<span class=\"token punctuation\">(</span><span class=\"token punctuation\">[</span>inference_batch_size<span class=\"token punctuation\">]</span><span class=\"token punctuation\">,</span> jp_tokenizer<span class=\"token punctuation\">.</span>word_index<span class=\"token punctuation\">[</span><span class=\"token string\">'bos'</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span>\n\n    end_token <span class=\"token operator\">=</span> jp_tokenizer<span class=\"token punctuation\">.</span>word_index<span class=\"token punctuation\">[</span><span class=\"token string\">'eos'</span><span class=\"token punctuation\">]</span>\n\n    greedy_sampler <span class=\"token operator\">=</span> tfa<span class=\"token punctuation\">.</span>seq2seq<span class=\"token punctuation\">.</span>GreedyEmbeddingSampler<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n\n    decoder_input <span class=\"token operator\">=</span> tf<span class=\"token punctuation\">.</span>expand_dims<span class=\"token punctuation\">(</span><span class=\"token punctuation\">[</span>jp_tokenizer<span class=\"token punctuation\">.</span>word_index<span class=\"token punctuation\">[</span><span class=\"token string\">'bos'</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">]</span> <span class=\"token operator\">*</span> inference_batch_size<span class=\"token punctuation\">,</span><span class=\"token number\">1</span><span class=\"token punctuation\">)</span>\n    decoder_emb_inp <span class=\"token operator\">=</span> decoderNetwork<span class=\"token punctuation\">.</span>decoder_embedding<span class=\"token punctuation\">(</span>decoder_input<span class=\"token punctuation\">)</span>\n\n    decoder_instance <span class=\"token operator\">=</span> tfa<span class=\"token punctuation\">.</span>seq2seq<span class=\"token punctuation\">.</span>BasicDecoder<span class=\"token punctuation\">(</span>cell <span class=\"token operator\">=</span> decoderNetwork<span class=\"token punctuation\">.</span>rnn_cell<span class=\"token punctuation\">,</span>\n                                                sampler <span class=\"token operator\">=</span> greedy_sampler<span class=\"token punctuation\">,</span>\n                                                output_layer <span class=\"token operator\">=</span> decoderNetwork<span class=\"token punctuation\">.</span>dense_layer<span class=\"token punctuation\">)</span>\n    decoderNetwork<span class=\"token punctuation\">.</span>attention_mechanism<span class=\"token punctuation\">.</span>setup_memory<span class=\"token punctuation\">(</span>a<span class=\"token punctuation\">)</span>\n\n    decoder_initial_state <span class=\"token operator\">=</span> decoderNetwork<span class=\"token punctuation\">.</span>build_decoder_initial_state<span class=\"token punctuation\">(</span>\n        inference_batch_size<span class=\"token punctuation\">,</span> encoder_state<span class=\"token operator\">=</span><span class=\"token punctuation\">[</span>a_tx<span class=\"token punctuation\">,</span> c_tx<span class=\"token punctuation\">]</span><span class=\"token punctuation\">,</span> Dtype<span class=\"token operator\">=</span>tf<span class=\"token punctuation\">.</span>float32<span class=\"token punctuation\">)</span>\n\n    maximum_iterations <span class=\"token operator\">=</span> tf<span class=\"token punctuation\">.</span><span class=\"token builtin\">round</span><span class=\"token punctuation\">(</span>tf<span class=\"token punctuation\">.</span>reduce_max<span class=\"token punctuation\">(</span>Tx<span class=\"token punctuation\">)</span> <span class=\"token operator\">*</span> <span class=\"token number\">2</span><span class=\"token punctuation\">)</span>\n\n    decoder_embedding_matrix <span class=\"token operator\">=</span> decoderNetwork<span class=\"token punctuation\">.</span>decoder_embedding<span class=\"token punctuation\">.</span>variables<span class=\"token punctuation\">[</span><span class=\"token number\">0</span><span class=\"token punctuation\">]</span>\n    <span class=\"token punctuation\">(</span>first_finished<span class=\"token punctuation\">,</span> first_inputs<span class=\"token punctuation\">,</span>first_state<span class=\"token punctuation\">)</span> <span class=\"token operator\">=</span> decoder_instance<span class=\"token punctuation\">.</span>initialize<span class=\"token punctuation\">(</span>\n        decoder_embedding_matrix<span class=\"token punctuation\">,</span> start_tokens <span class=\"token operator\">=</span> start_tokens<span class=\"token punctuation\">,</span>\n        end_token <span class=\"token operator\">=</span> end_token<span class=\"token punctuation\">,</span> initial_state <span class=\"token operator\">=</span> decoder_initial_state<span class=\"token punctuation\">)</span>\n\n    inputs <span class=\"token operator\">=</span> first_inputs\n    state <span class=\"token operator\">=</span> first_state\n    predictions <span class=\"token operator\">=</span> np<span class=\"token punctuation\">.</span>empty<span class=\"token punctuation\">(</span><span class=\"token punctuation\">(</span>inference_batch_size<span class=\"token punctuation\">,</span><span class=\"token number\">0</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> dtype <span class=\"token operator\">=</span> np<span class=\"token punctuation\">.</span>int32<span class=\"token punctuation\">)</span>\n    <span class=\"token keyword\">for</span> j <span class=\"token keyword\">in</span> <span class=\"token builtin\">range</span><span class=\"token punctuation\">(</span>maximum_iterations<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n        outputs<span class=\"token punctuation\">,</span> next_state<span class=\"token punctuation\">,</span> next_inputs<span class=\"token punctuation\">,</span> finished <span class=\"token operator\">=</span> decoder_instance<span class=\"token punctuation\">.</span>step<span class=\"token punctuation\">(</span>j<span class=\"token punctuation\">,</span> inputs<span class=\"token punctuation\">,</span>state<span class=\"token punctuation\">)</span>\n        inputs <span class=\"token operator\">=</span> next_inputs\n        state <span class=\"token operator\">=</span> next_state\n        outputs <span class=\"token operator\">=</span> np<span class=\"token punctuation\">.</span>expand_dims<span class=\"token punctuation\">(</span>outputs<span class=\"token punctuation\">.</span>sample_id<span class=\"token punctuation\">,</span>axis <span class=\"token operator\">=</span> <span class=\"token operator\">-</span><span class=\"token number\">1</span><span class=\"token punctuation\">)</span>\n        predictions <span class=\"token operator\">=</span> np<span class=\"token punctuation\">.</span>append<span class=\"token punctuation\">(</span>predictions<span class=\"token punctuation\">,</span> outputs<span class=\"token punctuation\">,</span> axis <span class=\"token operator\">=</span> <span class=\"token operator\">-</span><span class=\"token number\">1</span><span class=\"token punctuation\">)</span>\n\n    res <span class=\"token operator\">=</span> <span class=\"token string\">''</span>\n    <span class=\"token keyword\">for</span> i <span class=\"token keyword\">in</span> <span class=\"token builtin\">range</span><span class=\"token punctuation\">(</span><span class=\"token builtin\">len</span><span class=\"token punctuation\">(</span>predictions<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n        line <span class=\"token operator\">=</span> predictions<span class=\"token punctuation\">[</span>i<span class=\"token punctuation\">,</span><span class=\"token punctuation\">:</span><span class=\"token punctuation\">]</span>\n        seq <span class=\"token operator\">=</span> <span class=\"token builtin\">list</span><span class=\"token punctuation\">(</span>itertools<span class=\"token punctuation\">.</span>takewhile<span class=\"token punctuation\">(</span><span class=\"token keyword\">lambda</span> index<span class=\"token punctuation\">:</span> index <span class=\"token operator\">!=</span><span class=\"token number\">2</span><span class=\"token punctuation\">,</span> line<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n        res <span class=\"token operator\">+=</span> <span class=\"token string\">\" \"</span><span class=\"token punctuation\">.</span>join<span class=\"token punctuation\">(</span> <span class=\"token punctuation\">[</span>jp_tokenizer<span class=\"token punctuation\">.</span>index_word<span class=\"token punctuation\">[</span>w<span class=\"token punctuation\">]</span> <span class=\"token keyword\">for</span> w <span class=\"token keyword\">in</span> seq<span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span>\n    res <span class=\"token operator\">=</span> res<span class=\"token punctuation\">.</span>split<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n\n    <span class=\"token comment\"># Return back Unique words</span>\n    <span class=\"token keyword\">for</span> i <span class=\"token keyword\">in</span> <span class=\"token builtin\">range</span><span class=\"token punctuation\">(</span><span class=\"token builtin\">len</span><span class=\"token punctuation\">(</span>res<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n        <span class=\"token keyword\">if</span> res<span class=\"token punctuation\">[</span>i<span class=\"token punctuation\">]</span> <span class=\"token operator\">==</span> <span class=\"token string\">'unk'</span> <span class=\"token keyword\">and</span> unique <span class=\"token operator\">!=</span> <span class=\"token punctuation\">[</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">:</span>\n            res<span class=\"token punctuation\">[</span>i<span class=\"token punctuation\">]</span> <span class=\"token operator\">=</span> unique<span class=\"token punctuation\">.</span>pop<span class=\"token punctuation\">(</span><span class=\"token number\">0</span><span class=\"token punctuation\">)</span>\n\n    <span class=\"token keyword\">return</span> <span class=\"token string\">' '</span><span class=\"token punctuation\">.</span>join<span class=\"token punctuation\">(</span>res<span class=\"token punctuation\">)</span>\n\n<span class=\"token comment\"># Calculate BLEU</span>\n<span class=\"token keyword\">def</span> <span class=\"token function\">BLEU</span><span class=\"token punctuation\">(</span>X<span class=\"token punctuation\">,</span> y<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n    <span class=\"token comment\"># Prediction</span>\n    pred <span class=\"token operator\">=</span> <span class=\"token punctuation\">[</span>Translate<span class=\"token punctuation\">(</span>w<span class=\"token punctuation\">)</span> <span class=\"token keyword\">for</span> w <span class=\"token keyword\">in</span> tqdm<span class=\"token punctuation\">(</span>X<span class=\"token punctuation\">)</span><span class=\"token punctuation\">]</span>\n    <span class=\"token comment\"># Calculate BLEU</span>\n    score <span class=\"token operator\">=</span> sacrebleu<span class=\"token punctuation\">.</span>corpus_bleu<span class=\"token punctuation\">(</span>pred<span class=\"token punctuation\">,</span> <span class=\"token punctuation\">[</span>y<span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>score <span class=\"token operator\">/</span> <span class=\"token number\">100</span>\n    <span class=\"token keyword\">return</span> score<span class=\"token punctuation\">,</span> pred</code></pre></div>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token comment\"># Custom Train Progress</span>\n<span class=\"token keyword\">class</span> <span class=\"token class-name\">Progress</span><span class=\"token punctuation\">:</span>\n    <span class=\"token keyword\">def</span> <span class=\"token function\">__init__</span><span class=\"token punctuation\">(</span>self<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n        self<span class=\"token punctuation\">.</span>fig <span class=\"token operator\">=</span> plt<span class=\"token punctuation\">.</span>figure<span class=\"token punctuation\">(</span>figsize <span class=\"token operator\">=</span> <span class=\"token punctuation\">(</span><span class=\"token number\">8</span><span class=\"token punctuation\">,</span><span class=\"token number\">6</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n        self<span class=\"token punctuation\">.</span>ax <span class=\"token operator\">=</span> self<span class=\"token punctuation\">.</span>fig<span class=\"token punctuation\">.</span>add_subplot<span class=\"token punctuation\">(</span><span class=\"token number\">1</span><span class=\"token punctuation\">,</span> <span class=\"token number\">1</span><span class=\"token punctuation\">,</span> <span class=\"token number\">1</span><span class=\"token punctuation\">)</span>\n        self<span class=\"token punctuation\">.</span>loss<span class=\"token punctuation\">,</span> self<span class=\"token punctuation\">.</span>val_loss<span class=\"token punctuation\">,</span> self<span class=\"token punctuation\">.</span>BLEU <span class=\"token operator\">=</span> <span class=\"token punctuation\">[</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">,</span> <span class=\"token punctuation\">[</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">,</span> <span class=\"token punctuation\">[</span><span class=\"token punctuation\">]</span>\n        self<span class=\"token punctuation\">.</span>epoch_loss <span class=\"token operator\">=</span> <span class=\"token number\">0</span>\n\n    <span class=\"token keyword\">def</span> <span class=\"token function\">get_val_loss</span><span class=\"token punctuation\">(</span>self<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n        <span class=\"token keyword\">return</span> <span class=\"token punctuation\">[</span>x<span class=\"token punctuation\">[</span><span class=\"token number\">1</span><span class=\"token punctuation\">]</span> <span class=\"token keyword\">for</span> x <span class=\"token keyword\">in</span> self<span class=\"token punctuation\">.</span>val_loss<span class=\"token punctuation\">]</span>\n\n    <span class=\"token comment\"># Plot</span>\n    <span class=\"token keyword\">def</span> <span class=\"token function\">dynamic_plot</span><span class=\"token punctuation\">(</span>self<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n        self<span class=\"token punctuation\">.</span>ax<span class=\"token punctuation\">.</span>cla<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n        self<span class=\"token punctuation\">.</span>ax<span class=\"token punctuation\">.</span>plot<span class=\"token punctuation\">(</span><span class=\"token builtin\">range</span><span class=\"token punctuation\">(</span><span class=\"token builtin\">len</span><span class=\"token punctuation\">(</span>self<span class=\"token punctuation\">.</span>loss<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> self<span class=\"token punctuation\">.</span>loss<span class=\"token punctuation\">,</span> label<span class=\"token operator\">=</span><span class=\"token string\">'loss'</span><span class=\"token punctuation\">)</span>\n        <span class=\"token keyword\">if</span> <span class=\"token builtin\">len</span><span class=\"token punctuation\">(</span>self<span class=\"token punctuation\">.</span>val_loss<span class=\"token punctuation\">)</span> <span class=\"token operator\">>=</span> <span class=\"token number\">1</span><span class=\"token punctuation\">:</span>\n            x <span class=\"token operator\">=</span> <span class=\"token punctuation\">[</span>l<span class=\"token punctuation\">[</span><span class=\"token number\">0</span><span class=\"token punctuation\">]</span> <span class=\"token keyword\">for</span> l <span class=\"token keyword\">in</span> self<span class=\"token punctuation\">.</span>val_loss<span class=\"token punctuation\">]</span>\n            y <span class=\"token operator\">=</span> <span class=\"token punctuation\">[</span>l<span class=\"token punctuation\">[</span><span class=\"token number\">1</span><span class=\"token punctuation\">]</span> <span class=\"token keyword\">for</span> l <span class=\"token keyword\">in</span> self<span class=\"token punctuation\">.</span>val_loss<span class=\"token punctuation\">]</span>\n            self<span class=\"token punctuation\">.</span>ax<span class=\"token punctuation\">.</span>plot<span class=\"token punctuation\">(</span>x<span class=\"token punctuation\">,</span> y<span class=\"token punctuation\">,</span> color <span class=\"token operator\">=</span> <span class=\"token string\">'r'</span><span class=\"token punctuation\">,</span> label<span class=\"token operator\">=</span><span class=\"token string\">'val_loss'</span><span class=\"token punctuation\">)</span>\n            self<span class=\"token punctuation\">.</span>ax<span class=\"token punctuation\">.</span>plot<span class=\"token punctuation\">(</span>x<span class=\"token punctuation\">,</span> self<span class=\"token punctuation\">.</span>BLEU<span class=\"token punctuation\">,</span> color <span class=\"token operator\">=</span> <span class=\"token string\">'purple'</span><span class=\"token punctuation\">,</span> label<span class=\"token operator\">=</span><span class=\"token string\">'BLEU'</span><span class=\"token punctuation\">)</span>\n        self<span class=\"token punctuation\">.</span>ax<span class=\"token punctuation\">.</span>set_ylim<span class=\"token punctuation\">(</span><span class=\"token number\">0</span><span class=\"token punctuation\">,</span><span class=\"token punctuation\">)</span>\n        self<span class=\"token punctuation\">.</span>ax<span class=\"token punctuation\">.</span>legend<span class=\"token punctuation\">(</span>loc <span class=\"token operator\">=</span> <span class=\"token number\">1</span><span class=\"token punctuation\">)</span>\n        display<span class=\"token punctuation\">(</span>self<span class=\"token punctuation\">.</span>fig<span class=\"token punctuation\">)</span>\n\n    <span class=\"token comment\"># Train step progress</span>\n    <span class=\"token keyword\">def</span> <span class=\"token function\">train_progress</span><span class=\"token punctuation\">(</span>self<span class=\"token punctuation\">,</span> epoch<span class=\"token punctuation\">,</span> step<span class=\"token punctuation\">,</span> steps_per_epoch<span class=\"token punctuation\">,</span> start<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n        self<span class=\"token punctuation\">.</span>dynamic_plot<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n        <span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token string-interpolation\"><span class=\"token string\">f'Working on Epoch </span><span class=\"token interpolation\"><span class=\"token punctuation\">{</span>epoch<span class=\"token punctuation\">}</span></span><span class=\"token string\">'</span></span><span class=\"token punctuation\">)</span>\n        <span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token string\">'['</span> <span class=\"token operator\">+</span> <span class=\"token punctuation\">(</span><span class=\"token string\">'='</span> <span class=\"token operator\">*</span> <span class=\"token builtin\">int</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">(</span>step <span class=\"token operator\">+</span> <span class=\"token number\">1</span><span class=\"token punctuation\">)</span> <span class=\"token operator\">/</span> steps_per_epoch <span class=\"token operator\">*</span> <span class=\"token number\">60</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>ljust<span class=\"token punctuation\">(</span><span class=\"token number\">61</span><span class=\"token punctuation\">,</span> <span class=\"token string\">' '</span><span class=\"token punctuation\">)</span>\n              <span class=\"token operator\">+</span> <span class=\"token string-interpolation\"><span class=\"token string\">f']  </span><span class=\"token interpolation\"><span class=\"token punctuation\">{</span>step <span class=\"token operator\">+</span> <span class=\"token number\">1</span><span class=\"token punctuation\">}</span></span><span class=\"token string\">/</span><span class=\"token interpolation\"><span class=\"token punctuation\">{</span>steps_per_epoch<span class=\"token punctuation\">}</span></span><span class=\"token string\"> - loss : </span><span class=\"token interpolation\"><span class=\"token punctuation\">{</span><span class=\"token builtin\">round</span><span class=\"token punctuation\">(</span>self<span class=\"token punctuation\">.</span>epoch_loss <span class=\"token operator\">/</span> step<span class=\"token punctuation\">,</span> <span class=\"token number\">4</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">}</span></span><span class=\"token string\">'</span></span><span class=\"token punctuation\">)</span>\n        <span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token string-interpolation\"><span class=\"token string\">f'Time per Step </span><span class=\"token interpolation\"><span class=\"token punctuation\">{</span><span class=\"token builtin\">round</span><span class=\"token punctuation\">(</span>timeit<span class=\"token punctuation\">.</span>default_timer<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span> <span class=\"token operator\">-</span> start<span class=\"token punctuation\">,</span> <span class=\"token number\">2</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">}</span></span><span class=\"token string\"> s'</span></span><span class=\"token punctuation\">)</span>\n\n    <span class=\"token keyword\">def</span> <span class=\"token function\">summary</span><span class=\"token punctuation\">(</span>self<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n        loss <span class=\"token operator\">=</span> np<span class=\"token punctuation\">.</span>array_split<span class=\"token punctuation\">(</span>np<span class=\"token punctuation\">.</span>array<span class=\"token punctuation\">(</span>self<span class=\"token punctuation\">.</span>loss<span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> <span class=\"token builtin\">len</span><span class=\"token punctuation\">(</span>self<span class=\"token punctuation\">.</span>val_loss<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n        loss <span class=\"token operator\">=</span> <span class=\"token punctuation\">[</span>np<span class=\"token punctuation\">.</span>mean<span class=\"token punctuation\">(</span>x<span class=\"token punctuation\">)</span> <span class=\"token keyword\">for</span> x <span class=\"token keyword\">in</span> loss<span class=\"token punctuation\">]</span>\n        val_loss <span class=\"token operator\">=</span> <span class=\"token punctuation\">[</span>x<span class=\"token punctuation\">[</span><span class=\"token number\">1</span><span class=\"token punctuation\">]</span> <span class=\"token keyword\">for</span> x <span class=\"token keyword\">in</span> self<span class=\"token punctuation\">.</span>val_loss<span class=\"token punctuation\">]</span>\n        df <span class=\"token operator\">=</span> pd<span class=\"token punctuation\">.</span>DataFrame<span class=\"token punctuation\">(</span><span class=\"token punctuation\">{</span><span class=\"token string\">'Epochs'</span> <span class=\"token punctuation\">:</span> <span class=\"token builtin\">range</span><span class=\"token punctuation\">(</span><span class=\"token number\">1</span><span class=\"token punctuation\">,</span> <span class=\"token builtin\">len</span><span class=\"token punctuation\">(</span>val_loss<span class=\"token punctuation\">)</span> <span class=\"token operator\">+</span> <span class=\"token number\">1</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> <span class=\"token string\">'loss'</span> <span class=\"token punctuation\">:</span> loss<span class=\"token punctuation\">,</span>\n                           <span class=\"token string\">'val loss'</span> <span class=\"token punctuation\">:</span> val_loss<span class=\"token punctuation\">,</span> <span class=\"token string\">'BLEU'</span> <span class=\"token punctuation\">:</span> self<span class=\"token punctuation\">.</span>BLEU<span class=\"token punctuation\">}</span><span class=\"token punctuation\">)</span>\n\n        self<span class=\"token punctuation\">.</span>dynamic_plot<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n        clear_output<span class=\"token punctuation\">(</span>wait <span class=\"token operator\">=</span> <span class=\"token boolean\">True</span><span class=\"token punctuation\">)</span>\n        display<span class=\"token punctuation\">(</span>df<span class=\"token punctuation\">)</span></code></pre></div>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token comment\"># Initialize Train Progress</span>\nTP <span class=\"token operator\">=</span> Progress<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\nbest_prediction <span class=\"token operator\">=</span> <span class=\"token punctuation\">[</span><span class=\"token punctuation\">]</span>\n\n<span class=\"token keyword\">for</span> i <span class=\"token keyword\">in</span> <span class=\"token builtin\">range</span><span class=\"token punctuation\">(</span><span class=\"token number\">1</span><span class=\"token punctuation\">,</span> epochs <span class=\"token operator\">+</span> <span class=\"token number\">1</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n\n    encoder_initial_cell_state <span class=\"token operator\">=</span> initialize_initial_state<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n    total_loss <span class=\"token operator\">=</span> <span class=\"token number\">0.0</span>\n    <span class=\"token comment\"># Train Loss</span>\n    TP<span class=\"token punctuation\">.</span>epoch_loss <span class=\"token operator\">=</span> <span class=\"token number\">0</span>\n\n    <span class=\"token comment\"># Train</span>\n    <span class=\"token keyword\">for</span> <span class=\"token punctuation\">(</span>batch <span class=\"token punctuation\">,</span> <span class=\"token punctuation\">(</span>input_batch<span class=\"token punctuation\">,</span> output_batch<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span> <span class=\"token keyword\">in</span> <span class=\"token builtin\">enumerate</span><span class=\"token punctuation\">(</span>dataset<span class=\"token punctuation\">.</span>take<span class=\"token punctuation\">(</span>steps_per_epoch<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n        start <span class=\"token operator\">=</span> timeit<span class=\"token punctuation\">.</span>default_timer<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n        batch_loss <span class=\"token operator\">=</span> train_step<span class=\"token punctuation\">(</span>input_batch<span class=\"token punctuation\">,</span> output_batch<span class=\"token punctuation\">,</span> encoder_initial_cell_state<span class=\"token punctuation\">)</span>\n        total_loss <span class=\"token operator\">+=</span> batch_loss\n        TP<span class=\"token punctuation\">.</span>loss<span class=\"token punctuation\">.</span>append<span class=\"token punctuation\">(</span>batch_loss<span class=\"token punctuation\">.</span>numpy<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n        TP<span class=\"token punctuation\">.</span>epoch_loss <span class=\"token operator\">+=</span> batch_loss<span class=\"token punctuation\">.</span>numpy<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n\n        <span class=\"token keyword\">if</span> <span class=\"token punctuation\">(</span>batch<span class=\"token operator\">+</span><span class=\"token number\">1</span><span class=\"token punctuation\">)</span> <span class=\"token operator\">%</span> <span class=\"token number\">30</span> <span class=\"token operator\">==</span> <span class=\"token number\">0</span><span class=\"token punctuation\">:</span>\n            TP<span class=\"token punctuation\">.</span>train_progress<span class=\"token punctuation\">(</span>i<span class=\"token punctuation\">,</span> batch<span class=\"token punctuation\">,</span> steps_per_epoch<span class=\"token punctuation\">,</span> start<span class=\"token punctuation\">)</span>\n            clear_output<span class=\"token punctuation\">(</span>wait <span class=\"token operator\">=</span> <span class=\"token boolean\">True</span><span class=\"token punctuation\">)</span>\n\n    <span class=\"token comment\"># Validitate</span>\n    encoderNetwork<span class=\"token punctuation\">.</span>trainable <span class=\"token operator\">=</span> <span class=\"token boolean\">False</span>  <span class=\"token comment\"># Freeze our model layer to make sure</span>\n    decoderNetwork<span class=\"token punctuation\">.</span>trainable <span class=\"token operator\">=</span> <span class=\"token boolean\">False</span>  <span class=\"token comment\"># it didn't learn anything from val_data</span>\n\n    <span class=\"token comment\"># Valid loss</span>\n    val_loss <span class=\"token operator\">=</span> <span class=\"token number\">0</span>\n    <span class=\"token keyword\">for</span> <span class=\"token punctuation\">(</span>batch<span class=\"token punctuation\">,</span> <span class=\"token punctuation\">(</span>input_batch<span class=\"token punctuation\">,</span> output_batch<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span> <span class=\"token keyword\">in</span> <span class=\"token builtin\">enumerate</span><span class=\"token punctuation\">(</span>val_dataset<span class=\"token punctuation\">.</span>take<span class=\"token punctuation\">(</span>val_steps_per_epoch<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n        batch_loss <span class=\"token operator\">=</span> evaluate<span class=\"token punctuation\">(</span>input_batch<span class=\"token punctuation\">,</span> output_batch<span class=\"token punctuation\">,</span> encoder_initial_cell_state<span class=\"token punctuation\">)</span>\n        val_loss <span class=\"token operator\">+=</span> batch_loss<span class=\"token punctuation\">.</span>numpy<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n    val_loss <span class=\"token operator\">/=</span> val_steps_per_epoch\n\n    TP<span class=\"token punctuation\">.</span>val_loss<span class=\"token punctuation\">.</span>append<span class=\"token punctuation\">(</span><span class=\"token punctuation\">(</span>i <span class=\"token operator\">*</span> steps_per_epoch <span class=\"token operator\">-</span> <span class=\"token number\">1</span><span class=\"token punctuation\">,</span> val_loss<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n\n    <span class=\"token comment\"># Bleu Score</span>\n    bleu_score<span class=\"token punctuation\">,</span> pred <span class=\"token operator\">=</span> BLEU<span class=\"token punctuation\">(</span>eng_test<span class=\"token punctuation\">,</span> jpn_test<span class=\"token punctuation\">)</span>\n    TP<span class=\"token punctuation\">.</span>BLEU<span class=\"token punctuation\">.</span>append<span class=\"token punctuation\">(</span>bleu_score<span class=\"token punctuation\">)</span>\n\n    encoderNetwork<span class=\"token punctuation\">.</span>trainable <span class=\"token operator\">=</span> <span class=\"token boolean\">True</span>  <span class=\"token comment\"># Unfreeze layer for next epoch</span>\n    decoderNetwork<span class=\"token punctuation\">.</span>trainable <span class=\"token operator\">=</span> <span class=\"token boolean\">True</span>\n\n    <span class=\"token comment\"># Save best model</span>\n    <span class=\"token keyword\">if</span> bleu_score <span class=\"token operator\">==</span> <span class=\"token builtin\">max</span><span class=\"token punctuation\">(</span>TP<span class=\"token punctuation\">.</span>BLEU<span class=\"token punctuation\">)</span> <span class=\"token keyword\">and</span> val_loss <span class=\"token operator\">==</span> <span class=\"token builtin\">min</span><span class=\"token punctuation\">(</span>TP<span class=\"token punctuation\">.</span>get_val_loss<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n        best_prediction <span class=\"token operator\">=</span> pred\n        encoderNetwork<span class=\"token punctuation\">.</span>save_weights<span class=\"token punctuation\">(</span><span class=\"token string\">'encoderNetwork'</span><span class=\"token punctuation\">)</span>\n        decoderNetwork<span class=\"token punctuation\">.</span>save_weights<span class=\"token punctuation\">(</span><span class=\"token string\">'decoderNetwork'</span><span class=\"token punctuation\">)</span>\n\nTP<span class=\"token punctuation\">.</span>summary<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span></code></pre></div>\n\n      <div class=\"remark-sticky-table\"> \n      <div class=\"remark-sticky-table-wrapper\" style=\"width: 100%; height: 100%; overflow: auto;\">\n          <table class=\"remark-sticky-table-table\">\n      \n            \n               <thead class=\"remark-sticky-table-thead\">\n                     <tr class=\"remark-sticky-table-tr\">\n                     <th class=\"remark-sticky-table-th\" style=\"position: sticky; display: table-cell; width: auto; min-width: auto; white-space: nowrap; top: 0px; z-index: 2;\">Epochs</th><th class=\"remark-sticky-table-th\" style=\"position: sticky; display: table-cell; width: auto; min-width: auto; white-space: nowrap; top: 0px; z-index: 2;\">loss</th><th class=\"remark-sticky-table-th\" style=\"position: sticky; display: table-cell; width: auto; min-width: auto; white-space: nowrap; top: 0px; z-index: 2;\">val loss</th><th class=\"remark-sticky-table-th\" style=\"position: sticky; display: table-cell; width: auto; min-width: auto; white-space: nowrap; top: 0px; z-index: 2;\">BLEU</th>\n                        </tr> \n                        </thead><tbody class=\"remark-sticky-table-tbody\">\n                     <tr class=\"remark-sticky-table-tr\">\n                          <td class=\"remark-sticky-table-td\">1</td><td class=\"remark-sticky-table-td\">0.755007</td><td class=\"remark-sticky-table-td\">0.617767</td><td class=\"remark-sticky-table-td\">0.033657</td>\n                         </tr>\n                         </tbody><tbody class=\"remark-sticky-table-tbody\">\n                     <tr class=\"remark-sticky-table-tr\">\n                          <td class=\"remark-sticky-table-td\">2</td><td class=\"remark-sticky-table-td\">0.507803</td><td class=\"remark-sticky-table-td\">0.467358</td><td class=\"remark-sticky-table-td\">0.100527</td>\n                         </tr>\n                         </tbody><tbody class=\"remark-sticky-table-tbody\">\n                     <tr class=\"remark-sticky-table-tr\">\n                          <td class=\"remark-sticky-table-td\">3</td><td class=\"remark-sticky-table-td\">0.366943</td><td class=\"remark-sticky-table-td\">0.404535</td><td class=\"remark-sticky-table-td\">0.169416</td>\n                         </tr>\n                         </tbody><tbody class=\"remark-sticky-table-tbody\">\n                     <tr class=\"remark-sticky-table-tr\">\n                          <td class=\"remark-sticky-table-td\">4</td><td class=\"remark-sticky-table-td\">0.278531</td><td class=\"remark-sticky-table-td\">0.382699</td><td class=\"remark-sticky-table-td\">0.206622</td>\n                         </tr>\n                         </tbody><tbody class=\"remark-sticky-table-tbody\">\n                     <tr class=\"remark-sticky-table-tr\">\n                          <td class=\"remark-sticky-table-td\">5</td><td class=\"remark-sticky-table-td\">0.220114</td><td class=\"remark-sticky-table-td\">0.379538</td><td class=\"remark-sticky-table-td\">0.223136</td>\n                         </tr>\n                         </tbody><tbody class=\"remark-sticky-table-tbody\">\n                     <tr class=\"remark-sticky-table-tr\">\n                          <td class=\"remark-sticky-table-td\">6</td><td class=\"remark-sticky-table-td\">0.180076</td><td class=\"remark-sticky-table-td\">0.384533</td><td class=\"remark-sticky-table-td\">0.240543</td>\n                         </tr>\n                         </tbody><tbody class=\"remark-sticky-table-tbody\">\n                     <tr class=\"remark-sticky-table-tr\">\n                          <td class=\"remark-sticky-table-td\">7</td><td class=\"remark-sticky-table-td\">0.152284</td><td class=\"remark-sticky-table-td\">0.390775</td><td class=\"remark-sticky-table-td\">0.256055</td>\n                         </tr>\n                         </tbody>\n               \n        \n          </table>\n      </div>\n    </div>\n  \n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 490px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/4968cd1de5dc4c8cb3cbadd3b3801588/41d3c/output_49_1.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 72.78481012658227%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAPCAYAAADkmO9VAAAACXBIWXMAAAsSAAALEgHS3X78AAACXklEQVQ4y52RT08TQRjG9zt48SuYgISEePBuIn6HBoMaE60XPXKRqjF69WI8ATcPRk0jCUYM8k+FQOgChZZu6F/ows7u0m53d3Zn3sdsWy0NJlIneTIzyby/93mfUTKZzFXDMN6apjlpmuZUL2KMTR0Y1lShWJrQdf29pmnXlFxu7y4X+K9FAKRdR7lYRKlcBmPsoVIsFmIuFyACJ6KwF0kpQ243wqOq7hmGEQHvK9ns7qjLKQIKgIjonO46D6NDEN0ZY3GlWMiPuFz2BPz9KNqbFYQgDEOYphlX8vvaTceXkKeA/4KeAQKBEKLlMK9lRku2hJAQRB2HPYzeDazp+ZHUgUTJbjmUktqdW/Gcw203sJBL3zqqEybWIPIW6PRYv4V2g865ncvfRi6WKjFAIlWFeL0c0o90HbYTdrmQ1FKTgTPqBqqqeicaM6rT66B3qy7ezJhY2zmBrjvwTxwgSgOyrRamE01zBWEoYJosrlQqlZiUf365WVGugZIqp8nFBn1YdymVOaGVbYusQ4saVUZulRFsRjAZwYpkcDh1GLYdV9Lp9O3213blLwTgB4BWCTG9EeCzyjGT4vi07uNLysf3HR9fVR/b+z5WM77QDjm8hvVA0TQtFgQBhBBewAPumq5XNxzuGA73bdeTns8R+Bwi8Fwv4DwUvMbJK9nSNxzBNUZe5lg6xzUOix3fUxKJxMVkMnllc2uzb2lhaWDs0dj1+dn5ge3cTv+T50+H55aXBtXsXt/YeOLGys/Fwd0ttf/ls/HhjYXpocAsXXr14vGw+u3jkFHKXp6dW7jwC/SSU1VUgwWcAAAAAElFTkSuQmCC'); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"png\"\n        title=\"png\"\n        src=\"/static/4968cd1de5dc4c8cb3cbadd3b3801588/41d3c/output_49_1.png\"\n        srcset=\"/static/4968cd1de5dc4c8cb3cbadd3b3801588/c26ae/output_49_1.png 158w,\n/static/4968cd1de5dc4c8cb3cbadd3b3801588/6bdcf/output_49_1.png 315w,\n/static/4968cd1de5dc4c8cb3cbadd3b3801588/41d3c/output_49_1.png 490w\"\n        sizes=\"(max-width: 490px) 100vw, 490px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n  </a>\n    </span></p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token comment\"># Load best weights</span>\nencoderNetwork<span class=\"token punctuation\">.</span>load_weights<span class=\"token punctuation\">(</span><span class=\"token string\">'encoderNetwork'</span><span class=\"token punctuation\">)</span>\ndecoderNetwork<span class=\"token punctuation\">.</span>load_weights<span class=\"token punctuation\">(</span><span class=\"token string\">'decoderNetwork'</span><span class=\"token punctuation\">)</span></code></pre></div>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">&lt;tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7fb3601083d0></code></pre></div>\n<p>Let’s check the best prediction of our model.</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token keyword\">for</span> i <span class=\"token keyword\">in</span> <span class=\"token builtin\">range</span><span class=\"token punctuation\">(</span><span class=\"token number\">7</span><span class=\"token punctuation\">,</span><span class=\"token number\">16</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n    <span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token string\">\"English Sentence:\"</span><span class=\"token punctuation\">)</span>\n    <span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span>eng_test<span class=\"token punctuation\">[</span>i<span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span>\n    <span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token string\">\"\\nJapanese Translation:\"</span><span class=\"token punctuation\">)</span>\n    <span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span>best_prediction<span class=\"token punctuation\">[</span>i<span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span>\n    <span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token string\">\"\\nJapanese Reference:\"</span><span class=\"token punctuation\">)</span>\n    <span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span>jpn_test<span class=\"token punctuation\">[</span>i<span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span>\n    <span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token string\">''</span><span class=\"token punctuation\">.</span>ljust<span class=\"token punctuation\">(</span><span class=\"token number\">60</span><span class=\"token punctuation\">,</span> <span class=\"token string\">'-'</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span></code></pre></div>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">English Sentence:\nin september there are just a few people here and there on the beach\n\nJapanese Translation:\n９ 時 に は ここ に 他 の 人 が 住ん で い ない 人 が その ビーチ で 他 の 人達 が 残り の もと を あちら に 建設 見 た\n\nJapanese Reference:\n９月 の 海 は 人 が まばら だ ね\n------------------------------------------------------------\nEnglish Sentence:\nwhile you are young you should read a lot\n\nJapanese Translation:\n若い 頃 は 読み 終わっ たら いい よ\n\nJapanese Reference:\n若い うち に たくさん の 本 を 読む べき だ\n------------------------------------------------------------\nEnglish Sentence:\nhere i come\n\nJapanese Translation:\nここ に 来 た の\n\nJapanese Reference:\nいま 行き ます\n------------------------------------------------------------\nEnglish Sentence:\nonce you have decided when you will be coming let me know\n\nJapanese Translation:\n来る か 君 に は 連絡 を 言っ て き た よ\n\nJapanese Reference:\nいつ 来る か 決まっ たら 教え て\n------------------------------------------------------------\nEnglish Sentence:\nhe jumped on the train\n\nJapanese Translation:\n彼 は 電車 に 旗 を 飛び越え た\n\nJapanese Reference:\n彼 は 電車 に 飛び乗っ た\n------------------------------------------------------------\nEnglish Sentence:\nhe passed away yesterday\n\nJapanese Translation:\n彼 は 昨日 亡くなっ た\n\nJapanese Reference:\n彼 は 昨日 お 亡くなり に なり まし た\n------------------------------------------------------------\nEnglish Sentence:\ni had no other choice\n\nJapanese Translation:\n他 に 選択肢 が なかっ た\n\nJapanese Reference:\n他 に 手 が なかっ た の だ\n------------------------------------------------------------\nEnglish Sentence:\nare you good at bowling\n\nJapanese Translation:\nボーリング は 得意 です か\n\nJapanese Reference:\nボウリング は 得意\n------------------------------------------------------------\nEnglish Sentence:\nit is strange that you do not know anything about that matter\n\nJapanese Translation:\nそれ について 何 も 知ら ない こと が ない という こと は 変 だ\n\nJapanese Reference:\nあなた が その こと について 何 も 知ら ない の は 変 だ\n------------------------------------------------------------</code></pre></div>\n<h1>Test with Some Raw Input</h1>\n<p>Yeay now let’s play with <strong>our</strong> Machine Translation with some raw input. We’ll cross check the prediction from MT with Google Translate API to translate it back to english and see how bad <strong>our</strong> MT is :).</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token keyword\">from</span> googletrans <span class=\"token keyword\">import</span> Translator\n<span class=\"token comment\"># Google Translate</span>\ntranslator <span class=\"token operator\">=</span> Translator<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span></code></pre></div>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token builtin\">raw_input</span> <span class=\"token operator\">=</span> <span class=\"token punctuation\">[</span><span class=\"token string\">'i love you'</span><span class=\"token punctuation\">,</span> <span class=\"token string\">'i am sorry'</span><span class=\"token punctuation\">,</span> <span class=\"token string\">'hello'</span><span class=\"token punctuation\">,</span> <span class=\"token string\">'thank you'</span><span class=\"token punctuation\">,</span>\n             <span class=\"token string\">'is there something i can help?'</span><span class=\"token punctuation\">]</span>\n\n<span class=\"token keyword\">for</span> i <span class=\"token keyword\">in</span> <span class=\"token builtin\">range</span><span class=\"token punctuation\">(</span><span class=\"token builtin\">len</span><span class=\"token punctuation\">(</span><span class=\"token builtin\">raw_input</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n    prediction <span class=\"token operator\">=</span> Translate<span class=\"token punctuation\">(</span><span class=\"token builtin\">raw_input</span><span class=\"token punctuation\">[</span>i<span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span>\n    <span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token string\">\"English Sentence:\"</span><span class=\"token punctuation\">)</span>\n    <span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token builtin\">raw_input</span><span class=\"token punctuation\">[</span>i<span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span>\n    <span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token string\">\"\\nJapanese Translation:\"</span><span class=\"token punctuation\">)</span>\n    <span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span>prediction<span class=\"token punctuation\">)</span>\n    <span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token string\">\"\\nEnglish Translation from prediction [GoogleTranslate]:\"</span><span class=\"token punctuation\">)</span>\n    <span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span>translator<span class=\"token punctuation\">.</span>translate<span class=\"token punctuation\">(</span>prediction<span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>text<span class=\"token punctuation\">)</span>\n    <span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token string\">''</span><span class=\"token punctuation\">.</span>ljust<span class=\"token punctuation\">(</span><span class=\"token number\">60</span><span class=\"token punctuation\">,</span> <span class=\"token string\">'-'</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span></code></pre></div>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">English Sentence:\ni love you\n\nJapanese Translation:\n愛し てる よ\n\nEnglish Translation from prediction [GoogleTranslate]:\nI love you\n------------------------------------------------------------\nEnglish Sentence:\ni am sorry\n\nJapanese Translation:\nすみません\n\nEnglish Translation from prediction [GoogleTranslate]:\nExcuse me\n------------------------------------------------------------\nEnglish Sentence:\nhello\n\nJapanese Translation:\nもしもし\n\nEnglish Translation from prediction [GoogleTranslate]:\nHello\n------------------------------------------------------------\nEnglish Sentence:\nthank you\n\nJapanese Translation:\nありがとう ござい ます\n\nEnglish Translation from prediction [GoogleTranslate]:\nThank you\n------------------------------------------------------------\nEnglish Sentence:\nis there something i can help?\n\nJapanese Translation:\n何 か 手伝える もの が ある の\n\nEnglish Translation from prediction [GoogleTranslate]:\nThere is something to help\n------------------------------------------------------------</code></pre></div>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token keyword\">import</span> pickle\n\n<span class=\"token keyword\">with</span> <span class=\"token builtin\">open</span><span class=\"token punctuation\">(</span><span class=\"token string\">'en_tokenizer.pickle'</span><span class=\"token punctuation\">,</span> <span class=\"token string\">'wb'</span><span class=\"token punctuation\">)</span> <span class=\"token keyword\">as</span> handle<span class=\"token punctuation\">:</span>\n    pickle<span class=\"token punctuation\">.</span>dump<span class=\"token punctuation\">(</span>en_tokenizer<span class=\"token punctuation\">,</span> handle<span class=\"token punctuation\">,</span> protocol<span class=\"token operator\">=</span>pickle<span class=\"token punctuation\">.</span>HIGHEST_PROTOCOL<span class=\"token punctuation\">)</span>\nhandle<span class=\"token punctuation\">.</span>close<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n\n<span class=\"token keyword\">with</span> <span class=\"token builtin\">open</span><span class=\"token punctuation\">(</span><span class=\"token string\">'jp_tokenizer.pickle'</span><span class=\"token punctuation\">,</span> <span class=\"token string\">'wb'</span><span class=\"token punctuation\">)</span> <span class=\"token keyword\">as</span> handle<span class=\"token punctuation\">:</span>\n    pickle<span class=\"token punctuation\">.</span>dump<span class=\"token punctuation\">(</span>jp_tokenizer<span class=\"token punctuation\">,</span> handle<span class=\"token punctuation\">,</span> protocol<span class=\"token operator\">=</span>pickle<span class=\"token punctuation\">.</span>HIGHEST_PROTOCOL<span class=\"token punctuation\">)</span>\nhandle<span class=\"token punctuation\">.</span>close<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span></code></pre></div>\n<h1>Reference</h1>\n<ol>\n<li>TensorFlow Addons Networks : Sequence-to-Sequence NMT with Attention Mechanism <a href=\"https://www.tensorflow.org/addons/tutorials/networks_seq2seq_nmt\">Link</a></li>\n<li>seq2seq (Sequence to Sequence) Model for Deep Learning with PyTorch <a href=\"https://www.guru99.com/seq2seq-model.html\">Link</a></li>\n</ol>","frontmatter":{"title":"Machine Translation English to Japanese with Seq2Seq & Tensorflow","tags":"nlp, nmt, tensorflow, seq2seq","date":"August 19, 2020","description":"Machine Translation English to Japanese using Seq2Seq & Tensorflow 2"}},"previous":null,"next":{"fields":{"slug":"/cnn-keras-cv-0-996-tpu/"},"frontmatter":{"title":"MNIST Digit Classifier Using Keras, Tensorflow, and TPU"}}},"pageContext":{"id":"461be79d-3ea2-5bb6-b8f1-c4460049fd52","previousPostId":null,"nextPostId":"20ac24b9-a52d-51d4-be97-2e3ed610e572"}},"staticQueryHashes":["1503043946","3000541721","3274528899"]}