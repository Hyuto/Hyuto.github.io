<?xml version="1.0" encoding="UTF-8"?><rss xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" version="2.0"><channel><title><![CDATA[Hyuto's Blog]]></title><description><![CDATA[Wahyu Setianto Personal Blog.]]></description><link>https://Hyuto.github.io</link><generator>GatsbyJS</generator><lastBuildDate>Tue, 12 Apr 2022 08:06:18 GMT</lastBuildDate><item><title><![CDATA[Object Detection using YOLOv5]]></title><description><![CDATA[Object Detection Application using YOLOv5 and Tensorflow.js]]></description><link>https://Hyuto.github.io/showcase/yolov5-tfjs</link><guid isPermaLink="false">https://Hyuto.github.io/showcase/yolov5-tfjs</guid><pubDate>Tue, 01 Mar 2022 03:00:00 GMT</pubDate><content:encoded>
                        &lt;div&gt;
                          &lt;h2&gt;Object Detection using YOLOv5&lt;/h2&gt;
                          &lt;p&gt;Object Detection Application using YOLOv5 and Tensorflow.js&lt;/p&gt;
                        &lt;/div&gt;
                      </content:encoded></item><item><title><![CDATA[Gender Classification & Age Detection - BDC Satria Data 2021]]></title><description><![CDATA[Beberapa bulan yang lalu penulis dan teman - teman yang tergabung dalam sebuah tim mengikuti lomba
 Big Data Competition  di acara  Satriaâ€¦]]></description><link>https://Hyuto.github.io/blog/bdc-satria-data-2021/</link><guid isPermaLink="false">https://Hyuto.github.io/blog/bdc-satria-data-2021/</guid><pubDate>Wed, 01 Dec 2021 03:00:00 GMT</pubDate><content:encoded>&lt;p&gt;&lt;a href=&quot;https://github.com/Hyuto/bdc-2021&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/Github-black?logo=github&quot; alt=&quot;Github&quot;/&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;Beberapa bulan yang lalu penulis dan teman - teman yang tergabung dalam sebuah tim mengikuti lomba
&lt;code&gt;Big Data Competition&lt;/code&gt; di acara &lt;code&gt;Satria Data 2021&lt;/code&gt; yang diselenggarakan oleh &lt;em&gt;PUSPRESNAS&lt;/em&gt;
yang bekerjasama dengan &lt;em&gt;IPB University&lt;/em&gt;. Ini merupakan kali kedua penulis mengikuti lomba
&lt;code&gt;BDC - Satria Data&lt;/code&gt;, dengan susunan tim yang sedikit berbeda dan task yang berbeda juga penulis
mendapatkan banyak pengalaman dan ilmu dari lomba tersebut. Meskipun belum diberikan rezeki untuk
menjadi juara &lt;em&gt;Alhamdulillah&lt;/em&gt; tim kami berhasil melaju sampai babak final.&lt;/p&gt;&lt;p&gt;Berikut ini adalah kode dan langkah - langkah yang tim kami gunakan pada saat mengikuti lomba
&lt;code&gt;BDC Satria Data 2021&lt;/code&gt;.&lt;/p&gt;&lt;h2 id=&quot;author&quot;&gt;Author&lt;/h2&gt;&lt;ul&gt;&lt;li&gt;&lt;a href=&quot;https://www.linkedin.com/in/naufal-zhafran-albaqi-95360b184/&quot;&gt;Naufal Zhafran Albaqi&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href=&quot;https://www.linkedin.com/in/muhammadamanda/&quot;&gt;Muhammad Amanda&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href=&quot;https://www.linkedin.com/in/wahyu-setianto/&quot;&gt;Wahyu Setianto&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;h2 id=&quot;running-environment&quot;&gt;Running Environment&lt;/h2&gt;&lt;ul&gt;&lt;li&gt;Platform : Kaggle&lt;/li&gt;&lt;li&gt;Accelerator : GPU &lt;code&gt;NVIDIA TESLA P100&lt;/code&gt;&lt;/li&gt;&lt;/ul&gt;&lt;h2 id=&quot;main-dataset&quot;&gt;Main Dataset&lt;/h2&gt;&lt;p&gt;Dataset hasil preprocessing dan augmentasi dapat diakses lewat platform berikut.&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://github.com/Hyuto/bdc-2021/tree/master/data&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/Github-black?logo=github&quot; alt=&quot;Github&quot;/&gt;&lt;/a&gt;
&lt;a href=&quot;https://www.kaggle.com/wahyusetianto/bdc-2021&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/Kaggle-blue?logo=kaggle&quot; alt=&quot;kaggle&quot;/&gt;&lt;/a&gt;&lt;/p&gt;&lt;h1 id=&quot;gender-classification&quot;&gt;Gender Classification&lt;/h1&gt;&lt;p&gt;&lt;a href=&quot;https://www.kaggle.com/code/wahyusetianto/gender-bdc-2021&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/Kaggle%20Notebook-blue?logo=kaggle&quot; alt=&quot;kaggle&quot;/&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;Pada tahap penyisihan kami ditugaskan untuk melakukan pengklasifikasian terhadap &lt;em&gt;gender&lt;/em&gt; seseorang
didalam foto (&lt;em&gt;binary classification&lt;/em&gt;). Data yang disediakan merupakan data gambar dengan ukuran dan
resolusi yang berbeda sehingga merupakan tantangan tersendiri untuk melakukan &lt;em&gt;data preprocessing&lt;/em&gt;.
Pada data yang disediakan juga terdapat beberapa gambar yang memiliki lebih dari satu orang di
dalamnya sehingga menurut tim kami cukup sulit jika membersihkan data secara otomatis dan perlu
dilakukan &lt;em&gt;manual filtering&lt;/em&gt; untuk meningkatkan kualitas data sebelum dilatih kedalam model. Kami
juga melakukan proses &lt;em&gt;augmentasi&lt;/em&gt; sehingga data semakin banyak dan lebih banyak yang dapat
dipelajari oleh model. Model yang digunakan oleh tim kami dalam menyelesaikan masalah ini adalah
&lt;em&gt;tensorflow custom&lt;/em&gt; model dengan arsitektur 5 &lt;code&gt;convolution layer&lt;/code&gt; yang dipadukan dengan
&lt;code&gt;max pooling layer&lt;/code&gt; dan ditutup dengan &lt;code&gt;dense layer&lt;/code&gt; berkativasi &lt;em&gt;sigmoid&lt;/em&gt;. Ditahap akhir untuk
meningkatkan kualitas prediksi kami melakukan &lt;em&gt;ensembel&lt;/em&gt; terhadap hasil prediksi 5 model yang
dilatih dengan data yang berbeda (menggunakan &lt;code&gt;StratifiedKfold&lt;/code&gt;).&lt;/p&gt;&lt;h2 id=&quot;first-thing-first&quot;&gt;First Thing First&lt;/h2&gt;&lt;p&gt;Menginstall library yang diperlukan dan mengimport library - library yang akan digunakan serta
menseting variable config yang akan digunakan di dalam notebook ini.&lt;/p&gt;&lt;ol&gt;&lt;li&gt;Menginstal library &lt;code&gt;MTCNN&lt;/code&gt; dan &lt;code&gt;Albumentations&lt;/code&gt;&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;Library &lt;code&gt;MTCNN&lt;/code&gt; dan &lt;code&gt;Albumentations&lt;/code&gt; adalah library yang digunakan untuk preprocessing data gambar
pada notebook ini&lt;/p&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;!pip -q install mtcnn --upgrade
!pip -q install albumentations --upgrade
&lt;/code&gt;&lt;/pre&gt;&lt;pre&gt;&lt;code&gt;[33mWARNING: Running pip as the &amp;#x27;root&amp;#x27; user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv[0m
[33mWARNING: Running pip as the &amp;#x27;root&amp;#x27; user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv[0m
&lt;/code&gt;&lt;/pre&gt;&lt;ol start=&quot;2&quot;&gt;&lt;li&gt;Importing library&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;Mengimport library yang akan digunakan dalam notebook ini.&lt;/p&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;# Umum
import os, random, re
from tqdm.notebook import tqdm
import numpy as np
import pandas as pd
from PIL import Image

# Tensorflow
import tensorflow as tf
from tensorflow.keras.preprocessing.image import load_img, img_to_array

# Metrics &amp;amp; Splitting data
from sklearn.metrics import *
from sklearn.model_selection import *

# Plotting
import matplotlib.pyplot as plt
import seaborn as sns

# Preprocessing
import cv2
from mtcnn import MTCNN
import albumentations as A

print(&amp;quot;Tensorflow :&amp;quot;, tf.__version__)
&lt;/code&gt;&lt;/pre&gt;&lt;pre&gt;&lt;code&gt;Tensorflow : 2.6.0
&lt;/code&gt;&lt;/pre&gt;&lt;ol start=&quot;3&quot;&gt;&lt;li&gt;Setup &lt;code&gt;CONFIG&lt;/code&gt;&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;Mensetup varible - variable yang digunakan sebagai config pada notebook ini&lt;/p&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;SEED = 2021
SIZE = (200, 200)
BATCH_SIZE = 32
FACE_THRESHOLD = 0.95
FACE_DETECTOR = MTCNN()
&lt;/code&gt;&lt;/pre&gt;&lt;pre&gt;&lt;code&gt;2022-03-25 05:01:04.404789: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-03-25 05:01:04.507729: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-03-25 05:01:04.508532: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-03-25 05:01:04.510232: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-03-25 05:01:04.511454: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-03-25 05:01:04.512136: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-03-25 05:01:04.512828: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-03-25 05:01:06.301278: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-03-25 05:01:06.302130: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-03-25 05:01:06.302815: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-03-25 05:01:06.303445: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 15403 MB memory:  -&amp;gt; device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&quot;dataset&quot;&gt;Dataset&lt;/h2&gt;&lt;p&gt;Menggunakan dataset yang telah digunakan oleh tim kami dan melakukan loading agar mendapatkan
informasi path dari data gambar&lt;/p&gt;&lt;p&gt;Dataset : &lt;a href=&quot;https://www.kaggle.com/wahyusetianto/bdc-2021&quot;&gt;Data SD20210000722&lt;/a&gt;&lt;/p&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;train = pd.read_csv(&amp;quot;../input/bdc-2021/train.csv&amp;quot;)
test = pd.read_csv(&amp;quot;../input/bdc-2021/submission.csv&amp;quot;)
train.head()
&lt;/code&gt;&lt;/pre&gt;&lt;table&gt;&lt;thead&gt;&lt;tr&gt;&lt;th align=&quot;center&quot;&gt;nomor&lt;/th&gt;&lt;th align=&quot;center&quot;&gt;jenis kelamin&lt;/th&gt;&lt;th align=&quot;center&quot;&gt;usia&lt;/th&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td align=&quot;center&quot;&gt;1&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;0&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;27&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;center&quot;&gt;2&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;1&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;24&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;center&quot;&gt;3&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;0&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;29&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;center&quot;&gt;4&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;1&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;23&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;center&quot;&gt;5&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;0&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;20&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;memperjelas &lt;code&gt;path&lt;/code&gt; ke setiap data gambar&lt;/p&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;images = []
labels = []
test_images = []

TRAIN_DIR = &amp;quot;../input/bdc-2021/Training&amp;quot;
TEST_DIR = &amp;quot;../input/bdc-2021/Testing&amp;quot;

for no, label in train[[&amp;quot;nomor&amp;quot;, &amp;quot;jenis kelamin&amp;quot;]].values:
    TEMP_DIR = os.path.join(TRAIN_DIR, str(no))
    for file in os.listdir(TEMP_DIR):
        file_dir = os.path.join(TEMP_DIR, file)
        if &amp;quot;.ini&amp;quot; not in file_dir:
            images.append(file_dir)
            labels.append(label)

for no in test.id.values:
    file_dir = os.path.join(TEST_DIR, f&amp;quot;{no}.jpg&amp;quot;)
    if os.path.isfile(file_dir):
        test_images.append(file_dir)
    else:
        test_images.append(None)
        print(file_dir)
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;menampilkan dan mengecek beberapa gambar pada data &lt;code&gt;train&lt;/code&gt;&lt;/p&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;def read(path):
    &amp;quot;&amp;quot;&amp;quot;
    Read data gambar
    &amp;quot;&amp;quot;&amp;quot;
    img = Image.open(path)
    return img

def show_images(list_dir, label = None, load_image = read, seed = SEED):
    &amp;quot;&amp;quot;&amp;quot;
    Menampilkan Gambar Secara acak sebanyak 5 buah.
    &amp;quot;&amp;quot;&amp;quot;
    random.seed(seed)
    unique = [&amp;quot;init&amp;quot;]
    if label:
        unique = list(set(label))
    fig, axes = plt.subplots(len(unique), 5, figsize = (20, 5 * len(unique)))
    for i in range(len(unique)):
        if i == 0 and unique[i] == &amp;quot;init&amp;quot;:
            data = random.sample(list_dir, 5)
        else:
            data = random.sample([x for x in zip(list_dir, label) if x[1] == unique[i]], 5)
        for j in range(5):
            if unique[0] != &amp;quot;init&amp;quot;:
                img = load_image(data[j][0])
                axes[i, j].imshow(img)
                axes[i, j].set_title(f&amp;#x27;Label : {data[j][1]}&amp;#x27;, fontsize = 14)
                axes[i, j].axis(&amp;#x27;off&amp;#x27;)
            else:
                img = load_image(data[j])
                axes[j].imshow(img)
                axes[j].axis(&amp;#x27;off&amp;#x27;)
    fig.tight_layout()
    plt.show()
&lt;/code&gt;&lt;/pre&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;show_images(images, labels, seed=20)
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;span class=&quot;gatsby-resp-image-wrapper&quot; style=&quot;position:relative;display:block;margin-left:auto;margin-right:auto;max-width:630px&quot;&gt;
      &lt;a class=&quot;gatsby-resp-image-link&quot; href=&quot;/static/04f46f67e1d441a658b590367ff775c3/9de76/gender_output_14_0.png&quot; style=&quot;display:block&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;
    &lt;span class=&quot;gatsby-resp-image-background-image&quot; style=&quot;padding-bottom:50%;position:relative;bottom:0;left:0;background-image:url(&amp;#x27;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAKCAYAAAC0VX7mAAAACXBIWXMAAAsTAAALEwEAmpwYAAADNUlEQVQozwEqA9X8AM7Nzo1vY1ujsqijovHx83708vGDvtHHpcjKxqT07u18wcS5Ls3OykHZy8o+5eDeNNTHyTXJrrc/s5ykPsanrDTj4t+Iu7Gvo56MjaTs7ex4AJ+XkvesiXP/vJ6N/+vs7tvs5ePmdIZ+/5CGfv/m29rpwMG43OXp4//Nw8H/1sbC5uHX1um3oq3/fWFi/6J1fNnc1tXzuZeR/7KOi//m5eXSAL2tpObQoIr9xZeF/+Da1s3u5+nWcV1j/7WThP/UzczbiIR+3MDEv/90b2//i4WC5d7m6uh/iZ3+dkpG/5d6e9fItrXjqm1o/sGbnP/6/fzDALyzr++ziGz/xqKS//P09dTW1tneGhkq/0xIUf/R0dLkUE1I7ZOQjP9XUlX/bGpr9rW8wPhic4L/IRUe/zEtNea8m5Lsj2Vj/66Tlf/c19jLANbIwNfWsZbu2rad8Pbu57/4+fvIwcXT8dnc6PH6+PnELDAug3VVVqBrY2udnaKjjpifn5tfXmeqEg8arCYmLpKljYXSlXd476SJifOcnKe2AL64rqyBenHDfnJjw4+Gepm5vLmgnJaNxYSEfMREV1CZhYtfdVdnPZpKRTKbQDMebqOamACFgIcANx8nAAAAAACxsrKimpJuxYR5Y8WQjYCUAJ2gnfNjWVL/a2FY/01OSNi3rqjid3Jo/3BbUf9cXFPdWFY94kxBLf92ZE7/kYp447KwqJ3Bt6q+va6ivbq1rZrBu7rrfm5e/6mjkf+fkXXSAKWoqehOUFP/X2Jl/11eWc7Ar6bYqJeM/9uVgv+ohnXST0Arz6dzWf+ge2b/YFtN2uvp4u1zb3P/e2Na/29cVOOuj4Tkg1pR/4NtX/+rmn/IAI+Sleo8QUf/T1FY/1RUV9DRw73auKCe/6hyZP+Nc2nUc3Zu0p96Zv+Yf2//fntw24+XndpnaW/9YUg7/nNfUdF/eHflbV9c/46Df/+3qZzJAJyYj+qAg4v/q6yy/52bmNClk6HaZFZe/3NbX/9rW1/U3t/Z0qelmf+om4r/urSqyQEOGSmalIo3WzglNU06MzCVmqDelpiX/4SCgf9wYVbJ4DP1E+nYbFIAAAAASUVORK5CYII=&amp;#x27;);background-size:cover;display:block&quot;&gt;&lt;/span&gt;
  &lt;img class=&quot;gatsby-resp-image-image&quot; alt=&quot;png&quot; title=&quot;png&quot; src=&quot;/static/04f46f67e1d441a658b590367ff775c3/f058b/gender_output_14_0.png&quot; srcSet=&quot;/static/04f46f67e1d441a658b590367ff775c3/c26ae/gender_output_14_0.png 158w,/static/04f46f67e1d441a658b590367ff775c3/6bdcf/gender_output_14_0.png 315w,/static/04f46f67e1d441a658b590367ff775c3/f058b/gender_output_14_0.png 630w,/static/04f46f67e1d441a658b590367ff775c3/40601/gender_output_14_0.png 945w,/static/04f46f67e1d441a658b590367ff775c3/78612/gender_output_14_0.png 1260w,/static/04f46f67e1d441a658b590367ff775c3/9de76/gender_output_14_0.png 1423w&quot; sizes=&quot;(max-width: 630px) 100vw, 630px&quot; style=&quot;width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0&quot; loading=&quot;lazy&quot; decoding=&quot;async&quot;/&gt;
  &lt;/a&gt;
    &lt;/span&gt;&lt;/p&gt;&lt;h2 id=&quot;preprocess-data&quot;&gt;Preprocess Data&lt;/h2&gt;&lt;p&gt;Metode yang digunakan:&lt;/p&gt;&lt;ol&gt;&lt;li&gt;Mengekstrak wajah - wajah yang terdapat pada gambar menjadi gambar - gambar baru dengan label
yang sama dengan menggunakan model &lt;code&gt;MTCNN&lt;/code&gt;&lt;/li&gt;&lt;li&gt;Pada data test jika terdapat dua wajah yang terdeteksi pada satu gambar akan di ambil wajah
dengan tingkat confidence terbesar yang diberikan oleh model &lt;code&gt;MTCNN&lt;/code&gt;.&lt;/li&gt;&lt;li&gt;Jika tidak terdetect wajah pada salah satu gambar maka akan dilakukan crop pada bagian tengah
gambar sehingga gambar berbentuk persegi atau &lt;code&gt;jxj&lt;/code&gt; pixel.&lt;/li&gt;&lt;li&gt;Selanjutnya gambar akan di resize menjadi ukuran &lt;code&gt;256x256&lt;/code&gt; pixel&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;berikut adalah contoh hasil preprocess data gambar.&lt;/p&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;def get_faces(path):
    image = cv2.cvtColor(cv2.imread(path), cv2.COLOR_BGR2RGB)
    faces = FACE_DETECTOR.detect_faces(image)
    return faces

def load_and_preprocess_image(path: str, size = SIZE):
    &amp;quot;&amp;quot;&amp;quot;
    Load &amp;amp; Preprocess data gambar
    &amp;quot;&amp;quot;&amp;quot;
    image = img_to_array(load_img(path))
    faces = [x[&amp;#x27;box&amp;#x27;] for x in get_faces(path) if x[&amp;#x27;confidence&amp;#x27;] &amp;gt; FACE_THRESHOLD]
    if len(faces) &amp;gt; 0:
        x, y, w, h = faces[0]
        image = image[y:y+h, x:x+w]
    img = tf.convert_to_tensor(image, dtype=tf.float32)
    if len(faces) == 0:
        shapes = tf.shape(img)
        h, w = shapes[-3], shapes[-2]
        dim = tf.minimum(h, w)
        img = tf.image.resize_with_crop_or_pad(img, dim, dim)
    img = tf.image.resize(img, size)
    img = tf.cast(img, tf.float32) / 255.0
    return img.numpy()
&lt;/code&gt;&lt;/pre&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;show_images(images, labels, load_image = load_and_preprocess_image, seed=20)
&lt;/code&gt;&lt;/pre&gt;&lt;pre&gt;&lt;code&gt;2022-03-25 05:01:13.728692: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)
2022-03-25 05:01:14.721635: I tensorflow/stream_executor/cuda/cuda_dnn.cc:369] Loaded cuDNN version 8005
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;span class=&quot;gatsby-resp-image-wrapper&quot; style=&quot;position:relative;display:block;margin-left:auto;margin-right:auto;max-width:630px&quot;&gt;
      &lt;a class=&quot;gatsby-resp-image-link&quot; href=&quot;/static/b6218942c7aad401efc6439f7b46b27c/5bd27/gender_output_17_1.png&quot; style=&quot;display:block&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;
    &lt;span class=&quot;gatsby-resp-image-background-image&quot; style=&quot;padding-bottom:44.93670886075949%;position:relative;bottom:0;left:0;background-image:url(&amp;#x27;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAJCAYAAAAywQxIAAAACXBIWXMAAAsTAAALEwEAmpwYAAAC5ElEQVQozwHZAib9AHNcTovHpIyitYxyn6+moo83OU6ImoWCosaplKBkRDGNDggNih8VFqF5VU6hWD47ikZLYIxaPDyhcEM6oUUzNYjHwL+OooqJoaWJh6CllZiJAMShivXRrpn/y5+D/412b/tqX2nwu6GX/8uvnP+CXUX3RzY084xvaP+1jYL/b1JO805DUPdvQjz/nmZZ/29QSfChjIr7nHp0/76akP+ahofyANWljuXTp5b/0aaO/qV2Y+vFrqfh0rWs/8Wbi/+icFLnzaab5NiupP/CmIz/j25o5H9aV+eaXFH/n2VZ/7B6bOG8iILryYuD/ryJg/+khYviALeLcezhspf/1KCQ/5FkVvJ8bmbnzLCp/5xiXP9PMSHuwaWZ6s+flf+zh3v/qJqU6lY/Qe6gZVf/omRZ/3FPTOeca2fyuHZx/7F9fv/Uw8fpAHdZQkmxiW5VuJJ8VIdvZEsBBQdIXE1KVQwJBVQAAABKsJyTSa+TilWEbmJUp7SySQ8OG0ptRTtVg1FIVRobKEhfOj1LXzk4VYpscFS9x8xIAHtsYrO8lHzKrYp2yWJUSriLbWKw2aCQy92bicmreGi1CwUAskQvIcvClnXKmHxisplrV7Wxb1LKk1tDym1JO7BgXmC4o4NyynVOOsoyGxCxAM+un/PPo47/zqeU/8WllPnPmYXu0pSB/9aTgP+ze2X1mG1R8ZRpUP/KmHz/tpR+8a2Ke/Waa1z/fFNH/3ZOP+7SwL352q6e/4JWRf9wSjvwANWumefXqZz/1aeW/8imjezfpJHi7J+Q/9eHdf+0fmjpqX5g5cN+Yf/Okn//oYJz5cOnmumfcWT/g1RF/3BIOeLvyr7s57Oj/61xYP93TkLkAIl2benbs6X/x5+K/1pNRe69hXbk1YRv/7NuXf+HfHDqe1xF56JmT/+ncWH/kop+566pqeqshHv/dk5E/1M5MuSHYE7u1qaQ/5pmV/9SPDnmCFyfZUQpCF8AAAAASUVORK5CYII=&amp;#x27;);background-size:cover;display:block&quot;&gt;&lt;/span&gt;
  &lt;img class=&quot;gatsby-resp-image-image&quot; alt=&quot;png&quot; title=&quot;png&quot; src=&quot;/static/b6218942c7aad401efc6439f7b46b27c/f058b/gender_output_17_1.png&quot; srcSet=&quot;/static/b6218942c7aad401efc6439f7b46b27c/c26ae/gender_output_17_1.png 158w,/static/b6218942c7aad401efc6439f7b46b27c/6bdcf/gender_output_17_1.png 315w,/static/b6218942c7aad401efc6439f7b46b27c/f058b/gender_output_17_1.png 630w,/static/b6218942c7aad401efc6439f7b46b27c/40601/gender_output_17_1.png 945w,/static/b6218942c7aad401efc6439f7b46b27c/78612/gender_output_17_1.png 1260w,/static/b6218942c7aad401efc6439f7b46b27c/5bd27/gender_output_17_1.png 1432w&quot; sizes=&quot;(max-width: 630px) 100vw, 630px&quot; style=&quot;width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0&quot; loading=&quot;lazy&quot; decoding=&quot;async&quot;/&gt;
  &lt;/a&gt;
    &lt;/span&gt;&lt;/p&gt;&lt;h2 id=&quot;augmentasi-data&quot;&gt;Augmentasi Data&lt;/h2&gt;&lt;p&gt;Melakukan augmentasi untuk memperbanyak data. Metode augmentasi yang digunakan yaitu:&lt;/p&gt;&lt;ol&gt;&lt;li&gt;Horizontal flip&lt;/li&gt;&lt;li&gt;Donwscale kualitas gambar&lt;/li&gt;&lt;li&gt;Random rotate dengan rentang -30 sampai 30 derajad&lt;/li&gt;&lt;li&gt;Shift, scale, dan rotate gambar&lt;/li&gt;&lt;li&gt;Blur&lt;/li&gt;&lt;li&gt;Random brightness&lt;/li&gt;&lt;/ol&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;aug = A.Compose([
    A.HorizontalFlip(p=0.4),
    A.Downscale(scale_min=0.6, scale_max=0.9, p=0.3),
    A.Rotate(limit=(-30,30), p=0.6),
    A.ShiftScaleRotate(shift_limit=(-0.07, 0.07), scale_limit=(-0.05, 0.1), rotate_limit=(-15, 15), p=0.4),
    A.OneOf([
        A.MotionBlur(p=.4),
        A.MedianBlur(blur_limit=3, p=0.4),
        A.Blur(blur_limit=3, p=0.4),
    ], p=0.4),
    A.RandomBrightnessContrast(brightness_limit=(-0.25, 0.15), p=0.4),
])
&lt;/code&gt;&lt;/pre&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;def visualize_aug(path):
    fig, axes = plt.subplots(1, 5, figsize = (20, 5))
    image = load_and_preprocess_image(path)
    axes[0].imshow(image)
    axes[0].axis(&amp;#x27;off&amp;#x27;)
    for i in range(1, 5):
        augmented = aug(image=image)[&amp;#x27;image&amp;#x27;]
        axes[i].imshow(augmented)
        axes[i].axis(&amp;#x27;off&amp;#x27;)
    fig.tight_layout()
    plt.show()
&lt;/code&gt;&lt;/pre&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;random.seed(SEED)
for i in range(3):
    visualize_aug(images[i])
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;span class=&quot;gatsby-resp-image-wrapper&quot; style=&quot;position:relative;display:block;margin-left:auto;margin-right:auto;max-width:630px&quot;&gt;
      &lt;a class=&quot;gatsby-resp-image-link&quot; href=&quot;/static/3855eaee77f20270f295a581a4eff2ca/5bd27/gender_output_21_0.png&quot; style=&quot;display:block&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;
    &lt;span class=&quot;gatsby-resp-image-background-image&quot; style=&quot;padding-bottom:20.253164556962027%;position:relative;bottom:0;left:0;background-image:url(&amp;#x27;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAECAYAAACOXx+WAAAACXBIWXMAAAsTAAALEwEAmpwYAAABT0lEQVQY0wFEAbv+AHFJRM7bno3nyJCA5lA1M9ZwRT3LuoNz55tqXOZAJiPSVTQwztCRf+fppJHnZj04zmVCPtLeoY/mxI1951c7OMtwTEbWwYp65tOZieeiamDLAIBZU+rWoZL/y5iK/7Z+cPN4TkbntIV2/6d5bP+pcGHvyIZ069ScjP/kp5X/elBK63dSTu/ZpJT/yZeJ/8SHd+fMkoHzw5SH/9illf+ZaV/nAJJpX+bfoI//3KGR/8OPfu5zTkbiuYBx/7eDdP+4gnHq4qCL5uWlk//tpJH/i19W5oNdVerhoZD/2KCQ/9mei+LZopDu15+P/9GYif+AXFfiADUmJueqfHH/56yd/7mFde8jFBPjgltR/7yHev+rd2fr0JF85/Kwn/+xfXH/KBgZ5zAiI+uuf3T/46ma/8ePfePTnY3v1p6Q/7mHeP89Kyrj1BPHv/8qjUcAAAAASUVORK5CYII=&amp;#x27;);background-size:cover;display:block&quot;&gt;&lt;/span&gt;
  &lt;img class=&quot;gatsby-resp-image-image&quot; alt=&quot;png&quot; title=&quot;png&quot; src=&quot;/static/3855eaee77f20270f295a581a4eff2ca/f058b/gender_output_21_0.png&quot; srcSet=&quot;/static/3855eaee77f20270f295a581a4eff2ca/c26ae/gender_output_21_0.png 158w,/static/3855eaee77f20270f295a581a4eff2ca/6bdcf/gender_output_21_0.png 315w,/static/3855eaee77f20270f295a581a4eff2ca/f058b/gender_output_21_0.png 630w,/static/3855eaee77f20270f295a581a4eff2ca/40601/gender_output_21_0.png 945w,/static/3855eaee77f20270f295a581a4eff2ca/78612/gender_output_21_0.png 1260w,/static/3855eaee77f20270f295a581a4eff2ca/5bd27/gender_output_21_0.png 1432w&quot; sizes=&quot;(max-width: 630px) 100vw, 630px&quot; style=&quot;width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0&quot; loading=&quot;lazy&quot; decoding=&quot;async&quot;/&gt;
  &lt;/a&gt;
    &lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span class=&quot;gatsby-resp-image-wrapper&quot; style=&quot;position:relative;display:block;margin-left:auto;margin-right:auto;max-width:630px&quot;&gt;
      &lt;a class=&quot;gatsby-resp-image-link&quot; href=&quot;/static/b5c0218687d46fcbc6ad54fcbe2c92a7/5bd27/gender_output_21_1.png&quot; style=&quot;display:block&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;
    &lt;span class=&quot;gatsby-resp-image-background-image&quot; style=&quot;padding-bottom:20.253164556962027%;position:relative;bottom:0;left:0;background-image:url(&amp;#x27;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAECAYAAACOXx+WAAAACXBIWXMAAAsTAAALEwEAmpwYAAABT0lEQVQY0wFEAbv+AGtRSs7FnYvnw5+O5oVqYNaffm/LxqCN56aCc+ZZQj3Sr4Z0zr+Zh+e0k4Xnm31vzmBDNtJ8W0rmfVhG51gzJMtpTkXWr4x85qmJeud0XFPLAIdlXerKoI7/tY6C/7KOgPOoiHrnt5KE/7uSgv+jd2rvn3hp68egj/+4j4P/x5qJ631XSe9xTED/iWJR/1ExJud0V0/zs45+/557cP+gf3LnALmJeebPn4z/0qGR/8OZiO6wi3zixJaH/8ubif+7jXzqe1tU5sOTg//YqJX/0qSR5pdoV+qOYVD/glVF/08rIuKUbF/uuY18/7qOf/+xinviAIRrYefImYj/3bGf/6GDdO+LcGXjx56M/9aqmf+qgnLrmnBi55t2aP/Jm4r/2rGe55JrWOuSZFP/flJA/1c2KuNzW1Hvp39w/8Sci/+ef2/jSfjFyLnRcs4AAAAASUVORK5CYII=&amp;#x27;);background-size:cover;display:block&quot;&gt;&lt;/span&gt;
  &lt;img class=&quot;gatsby-resp-image-image&quot; alt=&quot;png&quot; title=&quot;png&quot; src=&quot;/static/b5c0218687d46fcbc6ad54fcbe2c92a7/f058b/gender_output_21_1.png&quot; srcSet=&quot;/static/b5c0218687d46fcbc6ad54fcbe2c92a7/c26ae/gender_output_21_1.png 158w,/static/b5c0218687d46fcbc6ad54fcbe2c92a7/6bdcf/gender_output_21_1.png 315w,/static/b5c0218687d46fcbc6ad54fcbe2c92a7/f058b/gender_output_21_1.png 630w,/static/b5c0218687d46fcbc6ad54fcbe2c92a7/40601/gender_output_21_1.png 945w,/static/b5c0218687d46fcbc6ad54fcbe2c92a7/78612/gender_output_21_1.png 1260w,/static/b5c0218687d46fcbc6ad54fcbe2c92a7/5bd27/gender_output_21_1.png 1432w&quot; sizes=&quot;(max-width: 630px) 100vw, 630px&quot; style=&quot;width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0&quot; loading=&quot;lazy&quot; decoding=&quot;async&quot;/&gt;
  &lt;/a&gt;
    &lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span class=&quot;gatsby-resp-image-wrapper&quot; style=&quot;position:relative;display:block;margin-left:auto;margin-right:auto;max-width:630px&quot;&gt;
      &lt;a class=&quot;gatsby-resp-image-link&quot; href=&quot;/static/751736eddba7e2de79bffc3926746346/5bd27/gender_output_21_2.png&quot; style=&quot;display:block&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;
    &lt;span class=&quot;gatsby-resp-image-background-image&quot; style=&quot;padding-bottom:20.253164556962027%;position:relative;bottom:0;left:0;background-image:url(&amp;#x27;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAECAYAAACOXx+WAAAACXBIWXMAAAsTAAALEwEAmpwYAAABT0lEQVQY0wFEAbv+AIJYOM7Um3DntX9U5kQpF9bOmm7L1J1x55pnPeZKLh3STC8czqx2Sufnr4DnmWpEzjQcENK7glPm1J1z559sRstoQynW0Jdt5r2HWudDKRnLAHBKNOqOXUT/nWxK/6tzRPOZZ0fniFtC/6x2UP+3dUDvvoBM65doSP+aaE3/glY665dlPO+icUz/iFhA/31TO+duRzHzjl1E/5xsSf+mcEPnAHhJMuagZUX/nmNG/6FkOO5wRi/iqm1L/5RcQ/+aXDPqo2Q85ptjR/+faEj/ZD8q5rBvPeqkaEj/oWhI/5BZPOJ4STHunGFD/6BmR/+ycT7iABMKCeduQTD/oWRE/209IO82IBfjVTAi/5xgRf+PVTDrjVY155VaQv9oPCn/NiAX53ZCIuuYYEH/hlA7/zAaE+MNBgfvfks4/5tgP/9sPB7jU4ClqEfrWuUAAAAASUVORK5CYII=&amp;#x27;);background-size:cover;display:block&quot;&gt;&lt;/span&gt;
  &lt;img class=&quot;gatsby-resp-image-image&quot; alt=&quot;png&quot; title=&quot;png&quot; src=&quot;/static/751736eddba7e2de79bffc3926746346/f058b/gender_output_21_2.png&quot; srcSet=&quot;/static/751736eddba7e2de79bffc3926746346/c26ae/gender_output_21_2.png 158w,/static/751736eddba7e2de79bffc3926746346/6bdcf/gender_output_21_2.png 315w,/static/751736eddba7e2de79bffc3926746346/f058b/gender_output_21_2.png 630w,/static/751736eddba7e2de79bffc3926746346/40601/gender_output_21_2.png 945w,/static/751736eddba7e2de79bffc3926746346/78612/gender_output_21_2.png 1260w,/static/751736eddba7e2de79bffc3926746346/5bd27/gender_output_21_2.png 1432w&quot; sizes=&quot;(max-width: 630px) 100vw, 630px&quot; style=&quot;width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0&quot; loading=&quot;lazy&quot; decoding=&quot;async&quot;/&gt;
  &lt;/a&gt;
    &lt;/span&gt;&lt;/p&gt;&lt;p&gt;Running preprocessing pada data gambar secara keseluruhan&lt;/p&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;def image_preprocessing(new_dir, images, labels=None):
    if os.path.isdir(new_dir):
        !rm -rf {new_dir}
    os.mkdir(new_dir)

    new_images, new_labels = [], []
    if not labels:
        labels = [None for _ in range(len(images))]

    for path, label in tqdm(zip(images, labels), total=len(images)):
        image = img_to_array(load_img(path))
        if label != None:
            faces = [x[&amp;#x27;box&amp;#x27;] for x in sorted(get_faces(path), key=lambda x: x[&amp;#x27;confidence&amp;#x27;],
                                              reverse=True) if x[&amp;#x27;confidence&amp;#x27;] &amp;gt; FACE_THRESHOLD]
        else:
            faces = [x[&amp;#x27;box&amp;#x27;] for x in sorted(get_faces(path), key=lambda x: x[&amp;#x27;confidence&amp;#x27;], reverse=True)]
        if len(faces) &amp;gt; 0:
            if label != None:
                for j, (x, y, w, h) in enumerate(faces):
                    img = image[y:y+h, x:x+w]
                    img = tf.convert_to_tensor(img, dtype=tf.float32)
                    img = tf.image.resize(img, SIZE)
                    img = tf.cast(img, tf.float32) / 255.0

                    img_dir = os.path.join(new_dir, f&amp;#x27;{j}_{path.split(&amp;quot;/&amp;quot;)[-1]}&amp;#x27;)
                    new_images.append(img_dir)
                    new_labels.append(label)
                    tf.keras.preprocessing.image.save_img(img_dir, img)

                    for k in range(3):
                        augmented = aug(image=img.numpy())[&amp;#x27;image&amp;#x27;]
                        img_dir = os.path.join(new_dir, f&amp;#x27;aug-{k}_{j}_{path.split(&amp;quot;/&amp;quot;)[-1]}&amp;#x27;)
                        new_images.append(img_dir)
                        new_labels.append(label)
                        tf.keras.preprocessing.image.save_img(img_dir, augmented)
            else:
                x, y, w, h = faces[0]
                img = image[y:y+h, x:x+w]
                img = tf.convert_to_tensor(img, dtype=tf.float32)
                img = tf.image.resize(img, SIZE)
                img = tf.cast(img, tf.float32) / 255.0

                img_dir = os.path.join(new_dir, path.split(&amp;#x27;/&amp;#x27;)[-1])
                new_images.append(img_dir)
                new_labels.append(label)
                tf.keras.preprocessing.image.save_img(img_dir, img)
        else :
            img = tf.convert_to_tensor(image, dtype=tf.float32)
            shapes = tf.shape(img)
            h, w = shapes[-3], shapes[-2]
            dim = tf.minimum(h, w)
            img = tf.image.resize_with_crop_or_pad(img, dim, dim)
            img = tf.image.resize(img, SIZE)
            img = tf.cast(img, tf.float32) / 255.0

            img_dir = os.path.join(new_dir, path.split(&amp;#x27;/&amp;#x27;)[-1])
            new_images.append(img_dir)
            new_labels.append(label)
            tf.keras.preprocessing.image.save_img(img_dir, img)

            if label != None:
                for k in range(3):
                    augmented = aug(image=img.numpy())[&amp;#x27;image&amp;#x27;]
                    img_dir = os.path.join(new_dir,  f&amp;#x27;aug-{k}_{path.split(&amp;quot;/&amp;quot;)[-1]}&amp;#x27;)
                    new_images.append(img_dir)
                    new_labels.append(label)
                    tf.keras.preprocessing.image.save_img(img_dir, augmented)

    return new_images, new_labels
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Untuk menghemat waktu running akan di skip bagian ini dan di ganti dengan meload data hasil
preprocess yang sudah di save pada run sebelumnya. Namun jika ingin melakukan preprocess pada run
sekarang maka uncomment code di bawah ini.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Peringatan&lt;/strong&gt; : running block code di bawah memakan waktu sekitar 50 menit dengan GPU Nvidia
Tesla P100-PCIE.&lt;/p&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;# new_train_dir = &amp;quot;./train&amp;quot;
# new_test_dir = &amp;quot;./test&amp;quot;

# random.seed(SEED)
# new_images, new_labels = image_preprocessing(new_train_dir, images, labels)
# new_test_images, _ = image_preprocessing(new_test_dir, test_images)
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt; : Comment dua block kode di bawah jika melakukan preprocess pada run saat ini.&lt;/p&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;preprocessed = pd.read_csv(&amp;quot;../input/bdc-2021/preprocessed-augmented/preprocessed.csv&amp;quot;)
preprocessed.head()
&lt;/code&gt;&lt;/pre&gt;&lt;table&gt;&lt;thead&gt;&lt;tr&gt;&lt;th align=&quot;left&quot;&gt;image&lt;/th&gt;&lt;th align=&quot;center&quot;&gt;label&lt;/th&gt;&lt;th align=&quot;center&quot;&gt;age&lt;/th&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;./train/0_1_3.jpg&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;0&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;27&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;./train/aug-0_0_1_3.jpg&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;0&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;27&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;./train/aug-1_0_1_3.jpg&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;0&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;27&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;./train/aug-2_0_1_3.jpg&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;0&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;27&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;./train/0_1_1.jpg&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;0&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;27&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;h2 id=&quot;filterisasi-data&quot;&gt;Filterisasi Data&lt;/h2&gt;&lt;p&gt;Menyaring data yang akan dilatih ke dalam model dengan melakukan filterisasi terhadap data gambar
yang memiliki kualitas gambar yang kurang baik dan misslabel.&lt;/p&gt;&lt;ol&gt;&lt;li&gt;Mendownload list data gambar yang akan dibuang.&lt;/li&gt;&lt;/ol&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;!wget https://raw.githubusercontent.com/Hyuto/bdc-2021/master/excluded-gender.txt

with open(&amp;quot;./excluded-gender.txt&amp;quot;) as f:
    excluded = f.read().split(&amp;quot;\n&amp;quot;)

patterns = fr&amp;#x27;{&amp;quot;|&amp;quot;.join(excluded)}&amp;#x27;
&lt;/code&gt;&lt;/pre&gt;&lt;pre&gt;&lt;code&gt;--2022-03-25 05:01:44--  https://raw.githubusercontent.com/Hyuto/bdc-2021/master/excluded-gender.txt
Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.111.133, 185.199.108.133, ...
Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.
HTTP request sent, awaiting response... 200 OK
Length: 3111 (3.0K) [text/plain]
Saving to: â€˜excluded-gender.txtâ€™

excluded-gender.txt 100%[===================&amp;gt;]   3.04K  --.-KB/s    in 0s

2022-03-25 05:01:44 (23.6 MB/s) - â€˜excluded-gender.txtâ€™ saved [3111/3111]
&lt;/code&gt;&lt;/pre&gt;&lt;ol start=&quot;2&quot;&gt;&lt;li&gt;Filterisasi data&lt;/li&gt;&lt;/ol&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;preprocessed_dir = &amp;quot;../input/bdc-2021/preprocessed-augmented&amp;quot;
new_images, new_labels = [], []

for image, label in preprocessed[[&amp;quot;image&amp;quot;, &amp;quot;label&amp;quot;]].values:
    if not re.search(patterns, image):
        new_images.append(os.path.join(preprocessed_dir, image))
        new_labels.append(label)

new_test_images = np.asarray([os.path.join(preprocessed_dir, &amp;quot;test&amp;quot;, f&amp;quot;{x}.jpg&amp;quot;) for x in test.id.values])

new_images = np.asarray(new_images)
new_labels = np.asarray(new_labels)
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Mengecek distribusi label pada data&lt;/p&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;plt.figure(figsize=(5, 5))
sns.countplot(x=new_labels)
plt.show()
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;span class=&quot;gatsby-resp-image-wrapper&quot; style=&quot;position:relative;display:block;margin-left:auto;margin-right:auto;max-width:339px&quot;&gt;
      &lt;a class=&quot;gatsby-resp-image-link&quot; href=&quot;/static/1296ad0f5709c2e080cc313eda605405/16caa/gender_output_33_0.png&quot; style=&quot;display:block&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;
    &lt;span class=&quot;gatsby-resp-image-background-image&quot; style=&quot;padding-bottom:89.24050632911393%;position:relative;bottom:0;left:0;background-image:url(&amp;#x27;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAASCAYAAABb0P4QAAAACXBIWXMAAAsTAAALEwEAmpwYAAADXUlEQVQ4y62U309TZxzGnw6mlJk1ODa2aYqIFUipKHS72q682G+c+wN2MUMdK1CVgp1tE1oTXcg2gwEOcBDbChMuptnFzDIXJVucYWEj00xwpbRnhfacth5CsizYw3m/y0HcEiTChd/kuXrzfvJ53vfNC5fLBW3+mktCEAS3lE6HkqLUm5RS/H8RU7yUSvGZTGbNpNPpPlmWA5FIxIMHk1MI4OW4dG/SO3yDPur6jj7uvUqHe7+n+r6r9OHZKzRw7TatNwsLC3+uAHXPFAGGu0Ly5v62ESq2cYtlDbyy284rFY39ykuHupTmwDWFiJQlVVUYY6tzXwPKsjwGURQB5OVYgE3TcWnswGeXyXJkYMnaGqKaliC9evwClTX2k+ern5YtVJURY49E1dbm5+fHHxq+qAd2hOPiWO3pS2R2nFuqcQap2hkka0uITHae3EM/rgDVNYDq/8C6ujpdsR7AC16E4+Iv6wLXPr5HDIv0QPFjDQdHl3dmrpyi2S/3U4I7SHNdtZToPkCzHW+omZEmklPJcRgMBj2Qk1cC6B9ruAIUhz6hSPPzFPPupqinlGIeE820blOTnW/TPWluHLm5ucWA7tlyYPNGgNLIEYp+uoOEk3tJ8FtI8FdR1GtSxd4PSNaABQUFeQC2rXspDw2HHTTjMi6DBF8lCT6LZqqKPQcfGGrz3CYA+a9hQ5U3AtSeTR5Q+kQMiQjA5pxXgKefWGXgqfxCwLCqMtvXHGDWlhAz2XnmHhxlRMTEYQebcRmZ4K9isTYzE3wWpgGTqyoXbgGKpuPSz7WnL5PZMbBY4wxmq51BxdoayprsfNY9OJoloqw07MhGXcascLIqK/gqFcFvyUY9uxY1Q1lKjKG9vV1XsQXAD4S7QmLqvVNfk7mpn6qdAdrXHCBrS5BM9j46ceH6cuXkxSaKuIwU8++hWJuZoj4LzbhLKcG9TxlxNoyGhga8vhXIBww3xn93v+kNnSu3dXTvsXf2VNq+OL+3sZvbeehMz1HuG+7vfxa5O7yN+9WxnZtwmXtuHisbmDhewf12rKT7zufv9Icnb59YuRSj9iXqJian8VbbEMoPn0VN6yB8RKiq78AuWyec/Le4v8QQPl+PW0e34w9PJWi2FlMeM245SxA+8y5i01P4FzZqd4BDdAkWAAAAAElFTkSuQmCC&amp;#x27;);background-size:cover;display:block&quot;&gt;&lt;/span&gt;
  &lt;img class=&quot;gatsby-resp-image-image&quot; alt=&quot;png&quot; title=&quot;png&quot; src=&quot;/static/1296ad0f5709c2e080cc313eda605405/16caa/gender_output_33_0.png&quot; srcSet=&quot;/static/1296ad0f5709c2e080cc313eda605405/c26ae/gender_output_33_0.png 158w,/static/1296ad0f5709c2e080cc313eda605405/6bdcf/gender_output_33_0.png 315w,/static/1296ad0f5709c2e080cc313eda605405/16caa/gender_output_33_0.png 339w&quot; sizes=&quot;(max-width: 339px) 100vw, 339px&quot; style=&quot;width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0&quot; loading=&quot;lazy&quot; decoding=&quot;async&quot;/&gt;
  &lt;/a&gt;
    &lt;/span&gt;&lt;/p&gt;&lt;p&gt;Jumlah data yang berlabel &lt;code&gt;0&lt;/code&gt; dan &lt;code&gt;1&lt;/code&gt; cenderung sama.&lt;/p&gt;&lt;h2 id=&quot;modelling&quot;&gt;Modelling&lt;/h2&gt;&lt;p&gt;Membuat model dengan arsitektur sebagai berikut&lt;/p&gt;&lt;p&gt;&lt;span class=&quot;gatsby-resp-image-wrapper&quot; style=&quot;position:relative;display:block;margin-left:auto;margin-right:auto;max-width:630px&quot;&gt;
      &lt;a class=&quot;gatsby-resp-image-link&quot; href=&quot;/static/0ca28c617e04d4f600f53c5b2651b42e/74866/arsitektur-model-gender.png&quot; style=&quot;display:block&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;
    &lt;span class=&quot;gatsby-resp-image-background-image&quot; style=&quot;padding-bottom:226.58227848101268%;position:relative;bottom:0;left:0;background-image:url(&amp;#x27;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAtCAIAAAAhntw/AAAACXBIWXMAAAsTAAALEwEAmpwYAAAFO0lEQVRIx21Wa3OjMBL0//9ju1u1azsGg3kIBAgBEkIgxMMYw12YBCe56w8ULiNm1NPd4tB1HWOMUsoYyzbkeU4ISdO0KArLsm63WxAEx+PR8zx7g+u6t9ttGIbDNE1d1w3/g/v9XpYlY0xKqZSqqqqua6VUs0FKOc/zYV3X5/M5fmIYhnEc+74fx5EQUtf1OI5t2w7D0DSNUqpt23Ecl2VZ1/V98ePxgM6h1TzPoyjKsuxyuaAN5/PZcRzDMEzTtCwrTdPH4/Gx+Pl8Qj9t22qt1YayLBFClNIsy6IoyvOcfkII8VH5uWGeZ7g+Hg+4ruvKOVdKwdunadof2HHIsqwsy6IooIgQIsuyNE2heUKIZVkY4ziOr9crxti2bcdxXNeNoujAGANW8zyHUkIIxlhRFGVZ5nlumqbv+0VR2Lbt+z5CKAgC3/cZY++jmqZp73bHuq5CiLZt4Sc8A9fH4zGO4zRN74St66q1ruu6aZr9KqX0fT9JEiklxrgoijiO0w1t28KqA/BWlmUURUmSwN9xHGOMDcN4e3vzPO/Xr1+GYVwuF9M0j8cjY2xd12VZPipD2z9QlmXTNNM09X0/DAO0PY7j8/n8qAyqmqbpvmHX2TzPoE2tddM0WmshBAizruuyLN+1zRjDGBNCPM8LggB69n0/CALP88IwNE3z79+/ruv++fPnfD4bhmFZ1u/fvymlB6Cq3aCU6jZorbuuA/JABVVVwVCllDBCrfXHnud5nj4B/c/zTCmVUk7TBG4Bt/V9/9ozsN00DbyYc75rBsycpunxeEQImaZp2/b1ehVCfGO7aRq+oSxLIQTnvCiKYAOl9Ha7xXEcBAHGOAiCuq5flWEwz+9Y11VKCU3CM8uy7P8uGw5Kqa9ZAcOAqSRJkmUZpRRjzBjzPC/aEMex67pSyoMQAjwUxzEhBHYLGQZ69DzPMAyM8eVyud1uYIy3tzfO+QHo/coz3M/zDJMDqnfc73fg/GWMvu8hRmDIcOO6LqW067o8z6uqEhs4533ffxAGQcE5T5KEEMI5h7YJIefz2bbtKIr+/ft3u91c1/U873Q6UUq/jarruv4LhmFQSgEFoGQgFQQH2fReGTa5Z8BuevCpUur+CchjMI/W+n6/vxsDGI7jOMsyznmaplEUpWmKMU6SxLIs3/ejKDqdTggh13UxxpZlEUIOUsqdjN10QoiqqkCtvu+HYUgIcV2XEAITRQgVRXEArezRu2NZFiGE1hq42RUGz7/YXpYFWIGCe2XTNDHGoG1KKeSU4zjfYgiKwGlQFAXnHKL7er3CMtM0HcdBCDmOY9s25/xVeV3XH+kLN4yxpmn26NolCMH8vngYBq113/e7trTWcCzCAQLRUdc1HAOANE3HcTw0TQNxizEOwzDfEIZhHMdhGFJKHcc5nU5BEFwuFzjZEULH47FpmlflrwA/gDGEEJBhwGhd11VVFUXxXhm6fz6fkFu7pR6PR5IkVVWBpECIEGPrJ16hD8YAe6dpyhiD4yKOY8gw13UNwwjD8EXYvhgUppSCxsqy9DwPISSlTJIkDEMgRSn1czGk7y4yUBJYdxfcLr6fba/r2rYtyAvKtm3r+77rupBeGGOE0PV6hSR4fdAAlFKU0jzPpZSUUs45jAdOn+v16nmeZVl7jPxs+6ul4aBTSu0eBnntx8W3xdM01XXddR0kRt/3kJ5aa5AafG793DNsYBzHKIoYY2CSoigIIcfjMUkS3/fP53MQBAih+/3+f/Y8TRMICCClzPM8SRL40CGE5HkuhNjn9N/F/wHxtNX0XCGclAAAAABJRU5ErkJggg==&amp;#x27;);background-size:cover;display:block&quot;&gt;&lt;/span&gt;
  &lt;img class=&quot;gatsby-resp-image-image&quot; alt=&quot;arsitektur model gender&quot; title=&quot;arsitektur model gender&quot; src=&quot;/static/0ca28c617e04d4f600f53c5b2651b42e/f058b/arsitektur-model-gender.png&quot; srcSet=&quot;/static/0ca28c617e04d4f600f53c5b2651b42e/c26ae/arsitektur-model-gender.png 158w,/static/0ca28c617e04d4f600f53c5b2651b42e/6bdcf/arsitektur-model-gender.png 315w,/static/0ca28c617e04d4f600f53c5b2651b42e/f058b/arsitektur-model-gender.png 630w,/static/0ca28c617e04d4f600f53c5b2651b42e/74866/arsitektur-model-gender.png 668w&quot; sizes=&quot;(max-width: 630px) 100vw, 630px&quot; style=&quot;width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0&quot; loading=&quot;lazy&quot; decoding=&quot;async&quot;/&gt;
  &lt;/a&gt;
    &lt;/span&gt;&lt;/p&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;def build_model(kernel_s=(3,3)):
    model = tf.keras.models.Sequential([
        tf.keras.layers.Conv2D(32,kernel_s,activation=&amp;#x27;relu&amp;#x27;,input_shape=(200,200,3),
                            kernel_regularizer=tf.keras.regularizers.l2(0.001),padding=&amp;quot;VALID&amp;quot;),
        tf.keras.layers.MaxPooling2D((2,2)),
        tf.keras.layers.Conv2D(64,kernel_s,activation=&amp;#x27;relu&amp;#x27;),
        tf.keras.layers.MaxPooling2D((2,2)),
        tf.keras.layers.Conv2D(64,kernel_s,activation=&amp;#x27;relu&amp;#x27;),
        tf.keras.layers.MaxPooling2D((2,2)),
        tf.keras.layers.Conv2D(128,kernel_s,activation=&amp;#x27;relu&amp;#x27;),
        tf.keras.layers.MaxPooling2D((2,2)),
        tf.keras.layers.Conv2D(128,kernel_s,activation=&amp;#x27;relu&amp;#x27;),
        tf.keras.layers.MaxPooling2D((2,2)),
        tf.keras.layers.Flatten(),
        tf.keras.layers.Dense(256, activation=&amp;#x27;relu&amp;#x27;,
                              kernel_regularizer=tf.keras.regularizers.l2(5e-4)),
        tf.keras.layers.Dense(1, activation=&amp;#x27;sigmoid&amp;#x27;)
    ])
    model.compile(loss=&amp;quot;binary_crossentropy&amp;quot;, optimizer=&amp;quot;adam&amp;quot;, metrics=[&amp;#x27;accuracy&amp;#x27;])
    return model
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;strong&gt;Tensorflow Data&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;Load data gambar menggunakan &lt;code&gt;Tensorflow Data&lt;/code&gt; agar pada saat pelatihan model penggunaan memmori
dapat lebih optimal&lt;/p&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;def decode_image(filename, label=None, image_size=SIZE):
    &amp;quot;&amp;quot;&amp;quot;
    Decode Image from String Path Tensor
    &amp;quot;&amp;quot;&amp;quot;
    bits = tf.io.read_file(filename)
    image = tf.image.decode_jpeg(bits, channels=3)
    image = tf.cast(image, tf.float32) / 255.0
    image = tf.image.resize(image, SIZE)

    if label is None: # if test
        return image
    else:
        return image, label
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;strong&gt;Training Model&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;Proses ini dilakukan dengan mensplit data menggunakan &lt;code&gt;StratifiedKFold&lt;/code&gt; sebanyak 5 split lalu untuk
setiap split akan dibangun sebuah model untuk dilatih. Setiap model ini akan digunakan untuk
melakukan peramalan terhadap data test. Pada proses trainingnya setiap model akan diukur tingkat
kebaikannya dengan menggunakan metric &lt;code&gt;accuracy&lt;/code&gt; lalu dikakukan penyimpanan weight pada model saat
&lt;code&gt;val_accuracy&lt;/code&gt; berada pada nilai terbesar selama pelatihan model sebanyak &lt;code&gt;50 epochs&lt;/code&gt;.&lt;/p&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;split = 5
prediksi = np.zeros((len(new_test_images), 1), dtype=np.float32)
acc_scores, f1_scores = [], []

test_dataset = (
    tf.data.Dataset
    .from_tensor_slices((new_test_images))
    .map(decode_image)
    .batch(BATCH_SIZE)
)

cv = StratifiedKFold(n_splits=split, shuffle=True, random_state=SEED)
for i, (train_index, test_index) in enumerate(cv.split(new_images, new_labels)):
    tf.keras.backend.clear_session()
    x_train, x_valid = new_images[train_index], new_images[test_index]
    y_train, y_valid = new_labels[train_index], new_labels[test_index]

    train_dataset = (
        tf.data.Dataset
        .from_tensor_slices((x_train, y_train))
        .map(decode_image, num_parallel_calls=tf.data.AUTOTUNE)
        .cache()
        .repeat()
        .shuffle(1024)
        .batch(BATCH_SIZE)
        .prefetch(tf.data.AUTOTUNE)
    )

    valid_dataset = (
        tf.data.Dataset
        .from_tensor_slices((x_valid, y_valid))
        .map(decode_image, num_parallel_calls=tf.data.AUTOTUNE)
        .batch(BATCH_SIZE)
        .cache()
        .prefetch(tf.data.AUTOTUNE)
    )

    model = build_model()
    checkpoint = tf.keras.callbacks.ModelCheckpoint(f&amp;#x27;{i}_best_model.h5&amp;#x27;, monitor=&amp;#x27;val_accuracy&amp;#x27;,
                                                save_best_only=True, save_weights_only=True,
                                                mode=&amp;#x27;max&amp;#x27;)
    print(f&amp;quot;\nCV {i+1}&amp;quot;)
    model.fit(train_dataset, epochs=50, validation_data=valid_dataset,
              steps_per_epoch=len(x_train) // BATCH_SIZE,
              callbacks = [checkpoint])
    model.load_weights(f&amp;#x27;{i}_best_model.h5&amp;#x27;)
    val_pred_classes = np.array(model.predict(valid_dataset).flatten() &amp;gt;= .5, dtype = &amp;#x27;int&amp;#x27;)
    acc, f1 = accuracy_score(y_valid, val_pred_classes), f1_score(y_valid, val_pred_classes)

    acc_scores.append(acc)
    f1_scores.append(f1)
    prediksi += model.predict(test_dataset)

    del train_dataset
    del valid_dataset
&lt;/code&gt;&lt;/pre&gt;&lt;pre&gt;&lt;code&gt;CV 1
Epoch 1/50
263/263 [==============================] - 32s 107ms/step - loss: 0.5496 - accuracy: 0.7317 - val_loss: 0.3710 - val_accuracy: 0.8544
Epoch 2/50
263/263 [==============================] - 7s 27ms/step - loss: 0.3210 - accuracy: 0.8819 - val_loss: 0.3272 - val_accuracy: 0.8753
Epoch 3/50
263/263 [==============================] - 7s 26ms/step - loss: 0.2792 - accuracy: 0.8977 - val_loss: 0.2815 - val_accuracy: 0.8957
Epoch 4/50
263/263 [==============================] - 7s 26ms/step - loss: 0.2451 - accuracy: 0.9177 - val_loss: 0.2392 - val_accuracy: 0.9189
Epoch 5/50
263/263 [==============================] - 7s 26ms/step - loss: 0.2160 - accuracy: 0.9268 - val_loss: 0.2512 - val_accuracy: 0.9151
Epoch 6/50
263/263 [==============================] - 7s 28ms/step - loss: 0.1843 - accuracy: 0.9437 - val_loss: 0.2210 - val_accuracy: 0.9265
Epoch 7/50
263/263 [==============================] - 7s 26ms/step - loss: 0.1688 - accuracy: 0.9470 - val_loss: 0.2120 - val_accuracy: 0.9336
Epoch 8/50
263/263 [==============================] - 7s 26ms/step - loss: 0.1480 - accuracy: 0.9578 - val_loss: 0.2075 - val_accuracy: 0.9346
Epoch 9/50
263/263 [==============================] - 7s 26ms/step - loss: 0.1341 - accuracy: 0.9627 - val_loss: 0.2290 - val_accuracy: 0.9294
Epoch 10/50
263/263 [==============================] - 7s 26ms/step - loss: 0.1163 - accuracy: 0.9701 - val_loss: 0.1956 - val_accuracy: 0.9417
Epoch 11/50
263/263 [==============================] - 7s 27ms/step - loss: 0.1062 - accuracy: 0.9718 - val_loss: 0.2243 - val_accuracy: 0.9369
Epoch 12/50
263/263 [==============================] - 7s 26ms/step - loss: 0.0872 - accuracy: 0.9802 - val_loss: 0.2190 - val_accuracy: 0.9403
Epoch 13/50
263/263 [==============================] - 7s 26ms/step - loss: 0.0953 - accuracy: 0.9781 - val_loss: 0.2050 - val_accuracy: 0.9507
Epoch 14/50
263/263 [==============================] - 7s 26ms/step - loss: 0.0794 - accuracy: 0.9841 - val_loss: 0.2156 - val_accuracy: 0.9455
Epoch 15/50
263/263 [==============================] - 7s 26ms/step - loss: 0.0732 - accuracy: 0.9874 - val_loss: 0.2320 - val_accuracy: 0.9426
Epoch 16/50
263/263 [==============================] - 7s 27ms/step - loss: 0.0663 - accuracy: 0.9870 - val_loss: 0.2245 - val_accuracy: 0.9502
Epoch 17/50
263/263 [==============================] - 7s 26ms/step - loss: 0.0748 - accuracy: 0.9843 - val_loss: 0.2315 - val_accuracy: 0.9469
Epoch 18/50
263/263 [==============================] - 7s 26ms/step - loss: 0.0637 - accuracy: 0.9884 - val_loss: 0.2470 - val_accuracy: 0.9497
Epoch 19/50
263/263 [==============================] - 7s 26ms/step - loss: 0.0601 - accuracy: 0.9892 - val_loss: 0.2627 - val_accuracy: 0.9436
Epoch 20/50
263/263 [==============================] - 7s 26ms/step - loss: 0.0570 - accuracy: 0.9901 - val_loss: 0.3084 - val_accuracy: 0.9360
Epoch 21/50
263/263 [==============================] - 7s 27ms/step - loss: 0.0535 - accuracy: 0.9916 - val_loss: 0.2784 - val_accuracy: 0.9393
Epoch 22/50
263/263 [==============================] - 7s 26ms/step - loss: 0.0552 - accuracy: 0.9910 - val_loss: 0.2408 - val_accuracy: 0.9488
Epoch 23/50
263/263 [==============================] - 7s 26ms/step - loss: 0.0517 - accuracy: 0.9925 - val_loss: 0.2810 - val_accuracy: 0.9350
Epoch 24/50
263/263 [==============================] - 7s 26ms/step - loss: 0.0525 - accuracy: 0.9914 - val_loss: 0.2122 - val_accuracy: 0.9459
Epoch 25/50
263/263 [==============================] - 7s 27ms/step - loss: 0.0522 - accuracy: 0.9917 - val_loss: 0.2259 - val_accuracy: 0.9493
Epoch 26/50
263/263 [==============================] - 7s 26ms/step - loss: 0.0522 - accuracy: 0.9919 - val_loss: 0.2751 - val_accuracy: 0.9379
Epoch 27/50
263/263 [==============================] - 7s 26ms/step - loss: 0.0560 - accuracy: 0.9904 - val_loss: 0.2802 - val_accuracy: 0.9422
Epoch 28/50
263/263 [==============================] - 7s 26ms/step - loss: 0.0383 - accuracy: 0.9960 - val_loss: 0.2612 - val_accuracy: 0.9464
Epoch 29/50
263/263 [==============================] - 7s 27ms/step - loss: 0.0558 - accuracy: 0.9905 - val_loss: 0.2870 - val_accuracy: 0.9440
Epoch 30/50
263/263 [==============================] - 7s 27ms/step - loss: 0.0329 - accuracy: 0.9981 - val_loss: 0.2645 - val_accuracy: 0.9559
Epoch 31/50
263/263 [==============================] - 7s 26ms/step - loss: 0.0476 - accuracy: 0.9923 - val_loss: 0.2235 - val_accuracy: 0.9464
Epoch 32/50
263/263 [==============================] - 7s 26ms/step - loss: 0.0371 - accuracy: 0.9961 - val_loss: 0.2934 - val_accuracy: 0.9521
Epoch 33/50
263/263 [==============================] - 7s 27ms/step - loss: 0.0494 - accuracy: 0.9923 - val_loss: 0.2960 - val_accuracy: 0.9393
Epoch 34/50
263/263 [==============================] - 7s 25ms/step - loss: 0.0551 - accuracy: 0.9905 - val_loss: 0.2176 - val_accuracy: 0.9535
Epoch 35/50
263/263 [==============================] - 7s 27ms/step - loss: 0.0355 - accuracy: 0.9958 - val_loss: 0.2783 - val_accuracy: 0.9502
Epoch 36/50
263/263 [==============================] - 7s 26ms/step - loss: 0.0387 - accuracy: 0.9949 - val_loss: 0.2497 - val_accuracy: 0.9493
Epoch 37/50
263/263 [==============================] - 7s 26ms/step - loss: 0.0338 - accuracy: 0.9957 - val_loss: 0.2888 - val_accuracy: 0.9521
Epoch 38/50
263/263 [==============================] - 7s 27ms/step - loss: 0.0592 - accuracy: 0.9891 - val_loss: 0.2465 - val_accuracy: 0.9422
Epoch 39/50
263/263 [==============================] - 7s 26ms/step - loss: 0.0314 - accuracy: 0.9980 - val_loss: 0.2826 - val_accuracy: 0.9478
Epoch 40/50
263/263 [==============================] - 7s 26ms/step - loss: 0.0322 - accuracy: 0.9966 - val_loss: 0.3041 - val_accuracy: 0.9474
Epoch 41/50
263/263 [==============================] - 7s 26ms/step - loss: 0.0365 - accuracy: 0.9954 - val_loss: 0.4735 - val_accuracy: 0.9241
Epoch 42/50
263/263 [==============================] - 7s 26ms/step - loss: 0.0423 - accuracy: 0.9935 - val_loss: 0.2528 - val_accuracy: 0.9578
Epoch 43/50
263/263 [==============================] - 7s 27ms/step - loss: 0.0274 - accuracy: 0.9982 - val_loss: 0.3552 - val_accuracy: 0.9459
Epoch 44/50
263/263 [==============================] - 7s 27ms/step - loss: 0.0300 - accuracy: 0.9973 - val_loss: 0.2836 - val_accuracy: 0.9398
Epoch 45/50
263/263 [==============================] - 7s 25ms/step - loss: 0.0340 - accuracy: 0.9952 - val_loss: 0.2927 - val_accuracy: 0.9497
Epoch 46/50
263/263 [==============================] - 7s 26ms/step - loss: 0.0442 - accuracy: 0.9917 - val_loss: 0.3046 - val_accuracy: 0.9388
Epoch 47/50
263/263 [==============================] - 7s 26ms/step - loss: 0.0264 - accuracy: 0.9986 - val_loss: 0.2410 - val_accuracy: 0.9540
Epoch 48/50
263/263 [==============================] - 7s 26ms/step - loss: 0.0287 - accuracy: 0.9973 - val_loss: 0.3600 - val_accuracy: 0.9374
Epoch 49/50
263/263 [==============================] - 7s 27ms/step - loss: 0.0464 - accuracy: 0.9907 - val_loss: 0.2628 - val_accuracy: 0.9469
Epoch 50/50
263/263 [==============================] - 7s 26ms/step - loss: 0.0275 - accuracy: 0.9976 - val_loss: 0.2887 - val_accuracy: 0.9578

CV 2
Epoch 1/50
263/263 [==============================] - 17s 57ms/step - loss: 0.6296 - accuracy: 0.6513 - val_loss: 0.5415 - val_accuracy: 0.7240
Epoch 2/50
263/263 [==============================] - 7s 26ms/step - loss: 0.3590 - accuracy: 0.8524 - val_loss: 0.3806 - val_accuracy: 0.8350
Epoch 3/50
263/263 [==============================] - 7s 26ms/step - loss: 0.3032 - accuracy: 0.8843 - val_loss: 0.3152 - val_accuracy: 0.8630
Epoch 4/50
263/263 [==============================] - 7s 26ms/step - loss: 0.2561 - accuracy: 0.9057 - val_loss: 0.2714 - val_accuracy: 0.8928
Epoch 5/50
263/263 [==============================] - 7s 27ms/step - loss: 0.2361 - accuracy: 0.9123 - val_loss: 0.2537 - val_accuracy: 0.9090
Epoch 6/50
263/263 [==============================] - 7s 26ms/step - loss: 0.2088 - accuracy: 0.9269 - val_loss: 0.2638 - val_accuracy: 0.8985
Epoch 7/50
263/263 [==============================] - 7s 26ms/step - loss: 0.2099 - accuracy: 0.9269 - val_loss: 0.2278 - val_accuracy: 0.9156
Epoch 8/50
263/263 [==============================] - 7s 26ms/step - loss: 0.1721 - accuracy: 0.9433 - val_loss: 0.2418 - val_accuracy: 0.9042
Epoch 9/50
263/263 [==============================] - 7s 25ms/step - loss: 0.1546 - accuracy: 0.9474 - val_loss: 0.2095 - val_accuracy: 0.9298
Epoch 10/50
263/263 [==============================] - 7s 27ms/step - loss: 0.1497 - accuracy: 0.9527 - val_loss: 0.2097 - val_accuracy: 0.9213
Epoch 11/50
263/263 [==============================] - 7s 26ms/step - loss: 0.1343 - accuracy: 0.9588 - val_loss: 0.2332 - val_accuracy: 0.9075
Epoch 12/50
263/263 [==============================] - 7s 26ms/step - loss: 0.1173 - accuracy: 0.9630 - val_loss: 0.1970 - val_accuracy: 0.9350
Epoch 13/50
263/263 [==============================] - 7s 26ms/step - loss: 0.1042 - accuracy: 0.9686 - val_loss: 0.1894 - val_accuracy: 0.9407
Epoch 14/50
263/263 [==============================] - 7s 26ms/step - loss: 0.1053 - accuracy: 0.9697 - val_loss: 0.2082 - val_accuracy: 0.9350
Epoch 15/50
263/263 [==============================] - 7s 27ms/step - loss: 0.0924 - accuracy: 0.9745 - val_loss: 0.2383 - val_accuracy: 0.9241
Epoch 16/50
263/263 [==============================] - 7s 27ms/step - loss: 0.0796 - accuracy: 0.9806 - val_loss: 0.2311 - val_accuracy: 0.9398
Epoch 17/50
263/263 [==============================] - 7s 26ms/step - loss: 0.0795 - accuracy: 0.9787 - val_loss: 0.1954 - val_accuracy: 0.9478
Epoch 18/50
263/263 [==============================] - 7s 26ms/step - loss: 0.0676 - accuracy: 0.9837 - val_loss: 0.2104 - val_accuracy: 0.9488
Epoch 19/50
263/263 [==============================] - 7s 26ms/step - loss: 0.0645 - accuracy: 0.9849 - val_loss: 0.2294 - val_accuracy: 0.9464
Epoch 20/50
263/263 [==============================] - 7s 28ms/step - loss: 0.0607 - accuracy: 0.9861 - val_loss: 0.2366 - val_accuracy: 0.9398
Epoch 21/50
263/263 [==============================] - 7s 26ms/step - loss: 0.0668 - accuracy: 0.9822 - val_loss: 0.2420 - val_accuracy: 0.9360
Epoch 22/50
263/263 [==============================] - 7s 26ms/step - loss: 0.0617 - accuracy: 0.9869 - val_loss: 0.2115 - val_accuracy: 0.9478
Epoch 23/50
263/263 [==============================] - 7s 26ms/step - loss: 0.0499 - accuracy: 0.9898 - val_loss: 0.3090 - val_accuracy: 0.9284
Epoch 24/50
263/263 [==============================] - 7s 26ms/step - loss: 0.0541 - accuracy: 0.9895 - val_loss: 0.2563 - val_accuracy: 0.9403
Epoch 25/50
263/263 [==============================] - 7s 28ms/step - loss: 0.0453 - accuracy: 0.9913 - val_loss: 0.2587 - val_accuracy: 0.9483
Epoch 26/50
263/263 [==============================] - 7s 26ms/step - loss: 0.0576 - accuracy: 0.9881 - val_loss: 0.2429 - val_accuracy: 0.9488
Epoch 27/50
263/263 [==============================] - 7s 26ms/step - loss: 0.0355 - accuracy: 0.9955 - val_loss: 0.2969 - val_accuracy: 0.9384
Epoch 28/50
263/263 [==============================] - 7s 26ms/step - loss: 0.0564 - accuracy: 0.9875 - val_loss: 0.2725 - val_accuracy: 0.9388
Epoch 29/50
263/263 [==============================] - 7s 26ms/step - loss: 0.0396 - accuracy: 0.9949 - val_loss: 0.2618 - val_accuracy: 0.9526
Epoch 30/50
263/263 [==============================] - 7s 27ms/step - loss: 0.0440 - accuracy: 0.9928 - val_loss: 0.3184 - val_accuracy: 0.9398
Epoch 31/50
263/263 [==============================] - 7s 26ms/step - loss: 0.0753 - accuracy: 0.9816 - val_loss: 0.2277 - val_accuracy: 0.9388
Epoch 32/50
263/263 [==============================] - 7s 26ms/step - loss: 0.0435 - accuracy: 0.9922 - val_loss: 0.2447 - val_accuracy: 0.9502
Epoch 33/50
263/263 [==============================] - 7s 26ms/step - loss: 0.0357 - accuracy: 0.9955 - val_loss: 0.2904 - val_accuracy: 0.9455
Epoch 34/50
263/263 [==============================] - 7s 26ms/step - loss: 0.0654 - accuracy: 0.9850 - val_loss: 0.2455 - val_accuracy: 0.9436
Epoch 35/50
263/263 [==============================] - 7s 27ms/step - loss: 0.0321 - accuracy: 0.9966 - val_loss: 0.2937 - val_accuracy: 0.9516
Epoch 36/50
263/263 [==============================] - 7s 26ms/step - loss: 0.0222 - accuracy: 0.9999 - val_loss: 0.2844 - val_accuracy: 0.9559
Epoch 37/50
263/263 [==============================] - 7s 26ms/step - loss: 0.0193 - accuracy: 1.0000 - val_loss: 0.2873 - val_accuracy: 0.9602
Epoch 38/50
263/263 [==============================] - 7s 26ms/step - loss: 0.0174 - accuracy: 1.0000 - val_loss: 0.2869 - val_accuracy: 0.9602
Epoch 39/50
263/263 [==============================] - 7s 27ms/step - loss: 0.0157 - accuracy: 1.0000 - val_loss: 0.2909 - val_accuracy: 0.9602
Epoch 40/50
263/263 [==============================] - 7s 27ms/step - loss: 0.0141 - accuracy: 1.0000 - val_loss: 0.2880 - val_accuracy: 0.9592
Epoch 41/50
263/263 [==============================] - 7s 25ms/step - loss: 0.0127 - accuracy: 1.0000 - val_loss: 0.2887 - val_accuracy: 0.9616
Epoch 42/50
263/263 [==============================] - 7s 26ms/step - loss: 0.0113 - accuracy: 1.0000 - val_loss: 0.2911 - val_accuracy: 0.9625
Epoch 43/50
263/263 [==============================] - 7s 26ms/step - loss: 0.0100 - accuracy: 1.0000 - val_loss: 0.2906 - val_accuracy: 0.9611
Epoch 44/50
263/263 [==============================] - 7s 26ms/step - loss: 0.0088 - accuracy: 1.0000 - val_loss: 0.2956 - val_accuracy: 0.9545
Epoch 45/50
263/263 [==============================] - 7s 28ms/step - loss: 0.0251 - accuracy: 0.9945 - val_loss: 0.3765 - val_accuracy: 0.8853
Epoch 46/50
263/263 [==============================] - 7s 26ms/step - loss: 0.1159 - accuracy: 0.9606 - val_loss: 0.2241 - val_accuracy: 0.9360
Epoch 47/50
263/263 [==============================] - 7s 26ms/step - loss: 0.0551 - accuracy: 0.9849 - val_loss: 0.2401 - val_accuracy: 0.9426
Epoch 48/50
263/263 [==============================] - 7s 27ms/step - loss: 0.0517 - accuracy: 0.9873 - val_loss: 0.2366 - val_accuracy: 0.9388
Epoch 49/50
263/263 [==============================] - 7s 26ms/step - loss: 0.0354 - accuracy: 0.9933 - val_loss: 0.2496 - val_accuracy: 0.9393
Epoch 50/50
263/263 [==============================] - 7s 28ms/step - loss: 0.0344 - accuracy: 0.9917 - val_loss: 0.2557 - val_accuracy: 0.9379

CV 3
Epoch 1/50
263/263 [==============================] - 18s 60ms/step - loss: 0.7068 - accuracy: 0.5899 - val_loss: 0.6871 - val_accuracy: 0.5965
Epoch 2/50
263/263 [==============================] - 7s 25ms/step - loss: 0.5515 - accuracy: 0.6947 - val_loss: 0.3532 - val_accuracy: 0.8525
Epoch 3/50
263/263 [==============================] - 7s 27ms/step - loss: 0.3284 - accuracy: 0.8645 - val_loss: 0.3120 - val_accuracy: 0.8853
Epoch 4/50
263/263 [==============================] - 7s 26ms/step - loss: 0.2807 - accuracy: 0.8931 - val_loss: 0.3025 - val_accuracy: 0.8862
Epoch 5/50
263/263 [==============================] - 7s 26ms/step - loss: 0.2362 - accuracy: 0.9144 - val_loss: 0.2469 - val_accuracy: 0.9099
Epoch 6/50
263/263 [==============================] - 7s 27ms/step - loss: 0.2126 - accuracy: 0.9247 - val_loss: 0.2345 - val_accuracy: 0.9194
Epoch 7/50
263/263 [==============================] - 7s 26ms/step - loss: 0.1923 - accuracy: 0.9344 - val_loss: 0.2541 - val_accuracy: 0.9203
Epoch 8/50
263/263 [==============================] - 7s 26ms/step - loss: 0.1788 - accuracy: 0.9387 - val_loss: 0.2092 - val_accuracy: 0.9355
Epoch 9/50
263/263 [==============================] - 7s 27ms/step - loss: 0.1509 - accuracy: 0.9491 - val_loss: 0.2047 - val_accuracy: 0.9398
Epoch 10/50
263/263 [==============================] - 7s 26ms/step - loss: 0.1353 - accuracy: 0.9569 - val_loss: 0.2152 - val_accuracy: 0.9369
Epoch 11/50
263/263 [==============================] - 7s 27ms/step - loss: 0.1220 - accuracy: 0.9664 - val_loss: 0.2115 - val_accuracy: 0.9417
Epoch 12/50
263/263 [==============================] - 7s 26ms/step - loss: 0.1036 - accuracy: 0.9709 - val_loss: 0.2067 - val_accuracy: 0.9440
Epoch 13/50
263/263 [==============================] - 7s 26ms/step - loss: 0.1018 - accuracy: 0.9712 - val_loss: 0.2111 - val_accuracy: 0.9369
Epoch 14/50
263/263 [==============================] - 7s 27ms/step - loss: 0.0852 - accuracy: 0.9766 - val_loss: 0.2244 - val_accuracy: 0.9412
Epoch 15/50
263/263 [==============================] - 7s 26ms/step - loss: 0.0773 - accuracy: 0.9812 - val_loss: 0.2498 - val_accuracy: 0.9260
Epoch 16/50
263/263 [==============================] - 7s 25ms/step - loss: 0.0757 - accuracy: 0.9805 - val_loss: 0.2084 - val_accuracy: 0.9440
Epoch 17/50
263/263 [==============================] - 7s 26ms/step - loss: 0.0645 - accuracy: 0.9844 - val_loss: 0.2288 - val_accuracy: 0.9426
Epoch 18/50
263/263 [==============================] - 7s 25ms/step - loss: 0.0628 - accuracy: 0.9840 - val_loss: 0.2407 - val_accuracy: 0.9422
Epoch 19/50
263/263 [==============================] - 7s 28ms/step - loss: 0.0561 - accuracy: 0.9873 - val_loss: 0.2402 - val_accuracy: 0.9488
Epoch 20/50
263/263 [==============================] - 7s 27ms/step - loss: 0.0521 - accuracy: 0.9894 - val_loss: 0.3115 - val_accuracy: 0.9379
Epoch 21/50
263/263 [==============================] - 7s 25ms/step - loss: 0.0628 - accuracy: 0.9847 - val_loss: 0.2724 - val_accuracy: 0.9426
Epoch 22/50
263/263 [==============================] - 7s 26ms/step - loss: 0.0561 - accuracy: 0.9861 - val_loss: 0.2290 - val_accuracy: 0.9512
Epoch 23/50
263/263 [==============================] - 7s 26ms/step - loss: 0.0502 - accuracy: 0.9904 - val_loss: 0.2277 - val_accuracy: 0.9474
Epoch 24/50
263/263 [==============================] - 7s 27ms/step - loss: 0.0501 - accuracy: 0.9898 - val_loss: 0.2613 - val_accuracy: 0.9464
Epoch 25/50
263/263 [==============================] - 7s 27ms/step - loss: 0.0316 - accuracy: 0.9963 - val_loss: 0.2643 - val_accuracy: 0.9488
Epoch 26/50
263/263 [==============================] - 7s 25ms/step - loss: 0.0223 - accuracy: 0.9998 - val_loss: 0.3362 - val_accuracy: 0.9502
Epoch 27/50
263/263 [==============================] - 7s 26ms/step - loss: 0.0540 - accuracy: 0.9872 - val_loss: 0.3159 - val_accuracy: 0.9161
Epoch 28/50
263/263 [==============================] - 7s 26ms/step - loss: 0.0542 - accuracy: 0.9882 - val_loss: 0.2793 - val_accuracy: 0.9431
Epoch 29/50
263/263 [==============================] - 7s 26ms/step - loss: 0.0355 - accuracy: 0.9947 - val_loss: 0.2806 - val_accuracy: 0.9417
Epoch 30/50
263/263 [==============================] - 7s 28ms/step - loss: 0.0498 - accuracy: 0.9887 - val_loss: 0.2897 - val_accuracy: 0.9464
Epoch 31/50
263/263 [==============================] - 7s 26ms/step - loss: 0.0443 - accuracy: 0.9918 - val_loss: 0.2955 - val_accuracy: 0.9341
Epoch 32/50
263/263 [==============================] - 7s 25ms/step - loss: 0.0433 - accuracy: 0.9923 - val_loss: 0.3032 - val_accuracy: 0.9497
Epoch 33/50
263/263 [==============================] - 7s 26ms/step - loss: 0.0397 - accuracy: 0.9932 - val_loss: 0.3064 - val_accuracy: 0.9483
Epoch 34/50
263/263 [==============================] - 7s 25ms/step - loss: 0.0307 - accuracy: 0.9963 - val_loss: 0.3349 - val_accuracy: 0.9436
Epoch 35/50
263/263 [==============================] - 7s 27ms/step - loss: 0.0324 - accuracy: 0.9964 - val_loss: 0.4585 - val_accuracy: 0.9256
Epoch 36/50
263/263 [==============================] - 7s 26ms/step - loss: 0.0497 - accuracy: 0.9900 - val_loss: 0.3056 - val_accuracy: 0.9317
Epoch 37/50
263/263 [==============================] - 7s 26ms/step - loss: 0.0489 - accuracy: 0.9888 - val_loss: 0.2959 - val_accuracy: 0.9469
Epoch 38/50
263/263 [==============================] - 7s 26ms/step - loss: 0.0313 - accuracy: 0.9962 - val_loss: 0.3199 - val_accuracy: 0.9469
Epoch 39/50
263/263 [==============================] - 7s 27ms/step - loss: 0.0278 - accuracy: 0.9973 - val_loss: 0.3277 - val_accuracy: 0.9445
Epoch 40/50
263/263 [==============================] - 7s 28ms/step - loss: 0.0473 - accuracy: 0.9904 - val_loss: 0.2563 - val_accuracy: 0.9412
Epoch 41/50
263/263 [==============================] - 7s 26ms/step - loss: 0.0343 - accuracy: 0.9944 - val_loss: 0.2936 - val_accuracy: 0.9474
Epoch 42/50
263/263 [==============================] - 7s 25ms/step - loss: 0.0344 - accuracy: 0.9941 - val_loss: 0.3198 - val_accuracy: 0.9450
Epoch 43/50
263/263 [==============================] - 7s 26ms/step - loss: 0.0365 - accuracy: 0.9945 - val_loss: 0.2744 - val_accuracy: 0.9426
Epoch 44/50
263/263 [==============================] - 7s 26ms/step - loss: 0.0310 - accuracy: 0.9962 - val_loss: 0.4059 - val_accuracy: 0.9360
Epoch 45/50
263/263 [==============================] - 7s 25ms/step - loss: 0.0439 - accuracy: 0.9913 - val_loss: 0.3263 - val_accuracy: 0.9450
Epoch 46/50
263/263 [==============================] - 7s 26ms/step - loss: 0.0281 - accuracy: 0.9970 - val_loss: 0.3318 - val_accuracy: 0.9483
Epoch 47/50
263/263 [==============================] - 7s 26ms/step - loss: 0.0315 - accuracy: 0.9956 - val_loss: 0.4163 - val_accuracy: 0.9317
Epoch 48/50
263/263 [==============================] - 7s 25ms/step - loss: 0.0377 - accuracy: 0.9932 - val_loss: 0.3119 - val_accuracy: 0.9417
Epoch 49/50
263/263 [==============================] - 7s 28ms/step - loss: 0.0329 - accuracy: 0.9952 - val_loss: 0.3192 - val_accuracy: 0.9455
Epoch 50/50
263/263 [==============================] - 7s 27ms/step - loss: 0.0200 - accuracy: 0.9993 - val_loss: 0.3720 - val_accuracy: 0.9497

CV 4
Epoch 1/50
263/263 [==============================] - 16s 54ms/step - loss: 0.5668 - accuracy: 0.7114 - val_loss: 0.3847 - val_accuracy: 0.8412
Epoch 2/50
263/263 [==============================] - 7s 27ms/step - loss: 0.3313 - accuracy: 0.8683 - val_loss: 0.3414 - val_accuracy: 0.8644
Epoch 3/50
263/263 [==============================] - 7s 25ms/step - loss: 0.2821 - accuracy: 0.8963 - val_loss: 0.2769 - val_accuracy: 0.8933
Epoch 4/50
263/263 [==============================] - 7s 26ms/step - loss: 0.2404 - accuracy: 0.9144 - val_loss: 0.2703 - val_accuracy: 0.9000
Epoch 5/50
263/263 [==============================] - 7s 26ms/step - loss: 0.2224 - accuracy: 0.9198 - val_loss: 0.3158 - val_accuracy: 0.8985
Epoch 6/50
263/263 [==============================] - 7s 26ms/step - loss: 0.2027 - accuracy: 0.9324 - val_loss: 0.2464 - val_accuracy: 0.9161
Epoch 7/50
263/263 [==============================] - 7s 27ms/step - loss: 0.1660 - accuracy: 0.9482 - val_loss: 0.2103 - val_accuracy: 0.9279
Epoch 8/50
263/263 [==============================] - 7s 28ms/step - loss: 0.1506 - accuracy: 0.9544 - val_loss: 0.2314 - val_accuracy: 0.9227
Epoch 9/50
263/263 [==============================] - 7s 26ms/step - loss: 0.1538 - accuracy: 0.9526 - val_loss: 0.1944 - val_accuracy: 0.9426
Epoch 10/50
263/263 [==============================] - 7s 26ms/step - loss: 0.1245 - accuracy: 0.9639 - val_loss: 0.2137 - val_accuracy: 0.9327
Epoch 11/50
263/263 [==============================] - 7s 25ms/step - loss: 0.1091 - accuracy: 0.9697 - val_loss: 0.2203 - val_accuracy: 0.9327
Epoch 12/50
263/263 [==============================] - 7s 26ms/step - loss: 0.1108 - accuracy: 0.9727 - val_loss: 0.2389 - val_accuracy: 0.9322
Epoch 13/50
263/263 [==============================] - 7s 25ms/step - loss: 0.0898 - accuracy: 0.9766 - val_loss: 0.2162 - val_accuracy: 0.9440
Epoch 14/50
263/263 [==============================] - 6s 25ms/step - loss: 0.0893 - accuracy: 0.9790 - val_loss: 0.2326 - val_accuracy: 0.9393
Epoch 15/50
263/263 [==============================] - 7s 25ms/step - loss: 0.0767 - accuracy: 0.9824 - val_loss: 0.2705 - val_accuracy: 0.9279
Epoch 16/50
263/263 [==============================] - 7s 25ms/step - loss: 0.0746 - accuracy: 0.9843 - val_loss: 0.2469 - val_accuracy: 0.9327
Epoch 17/50
263/263 [==============================] - 7s 26ms/step - loss: 0.0844 - accuracy: 0.9803 - val_loss: 0.2399 - val_accuracy: 0.9440
Epoch 18/50
263/263 [==============================] - 7s 27ms/step - loss: 0.0657 - accuracy: 0.9865 - val_loss: 0.2712 - val_accuracy: 0.9303
Epoch 19/50
263/263 [==============================] - 7s 25ms/step - loss: 0.0463 - accuracy: 0.9938 - val_loss: 0.2771 - val_accuracy: 0.9403
Epoch 20/50
263/263 [==============================] - 7s 25ms/step - loss: 0.0694 - accuracy: 0.9842 - val_loss: 0.2531 - val_accuracy: 0.9440
Epoch 21/50
263/263 [==============================] - 7s 25ms/step - loss: 0.0579 - accuracy: 0.9887 - val_loss: 0.2975 - val_accuracy: 0.9331
Epoch 22/50
263/263 [==============================] - 7s 26ms/step - loss: 0.0626 - accuracy: 0.9873 - val_loss: 0.2944 - val_accuracy: 0.9312
Epoch 23/50
263/263 [==============================] - 7s 25ms/step - loss: 0.0560 - accuracy: 0.9899 - val_loss: 0.3258 - val_accuracy: 0.9270
Epoch 24/50
263/263 [==============================] - 7s 25ms/step - loss: 0.0398 - accuracy: 0.9961 - val_loss: 0.3283 - val_accuracy: 0.9426
Epoch 25/50
263/263 [==============================] - 7s 25ms/step - loss: 0.0512 - accuracy: 0.9912 - val_loss: 0.2765 - val_accuracy: 0.9384
Epoch 26/50
263/263 [==============================] - 6s 25ms/step - loss: 0.0443 - accuracy: 0.9936 - val_loss: 0.3364 - val_accuracy: 0.9469
Epoch 27/50
263/263 [==============================] - 7s 26ms/step - loss: 0.0699 - accuracy: 0.9843 - val_loss: 0.3219 - val_accuracy: 0.9336
Epoch 28/50
263/263 [==============================] - 7s 26ms/step - loss: 0.0470 - accuracy: 0.9923 - val_loss: 0.2976 - val_accuracy: 0.9436
Epoch 29/50
263/263 [==============================] - 7s 27ms/step - loss: 0.0506 - accuracy: 0.9907 - val_loss: 0.3426 - val_accuracy: 0.9294
Epoch 30/50
263/263 [==============================] - 7s 26ms/step - loss: 0.0512 - accuracy: 0.9904 - val_loss: 0.3651 - val_accuracy: 0.9322
Epoch 31/50
263/263 [==============================] - 7s 25ms/step - loss: 0.0451 - accuracy: 0.9928 - val_loss: 0.3058 - val_accuracy: 0.9360
Epoch 32/50
263/263 [==============================] - 7s 25ms/step - loss: 0.0482 - accuracy: 0.9928 - val_loss: 0.2869 - val_accuracy: 0.9412
Epoch 33/50
263/263 [==============================] - 7s 25ms/step - loss: 0.0499 - accuracy: 0.9916 - val_loss: 0.3573 - val_accuracy: 0.9298
Epoch 34/50
263/263 [==============================] - 7s 25ms/step - loss: 0.0346 - accuracy: 0.9967 - val_loss: 0.3462 - val_accuracy: 0.9417
Epoch 35/50
263/263 [==============================] - 7s 25ms/step - loss: 0.0316 - accuracy: 0.9971 - val_loss: 0.3925 - val_accuracy: 0.9317
Epoch 36/50
263/263 [==============================] - 7s 25ms/step - loss: 0.0537 - accuracy: 0.9895 - val_loss: 0.2978 - val_accuracy: 0.9440
Epoch 37/50
263/263 [==============================] - 7s 26ms/step - loss: 0.0398 - accuracy: 0.9943 - val_loss: 0.3594 - val_accuracy: 0.9275
Epoch 38/50
263/263 [==============================] - 7s 26ms/step - loss: 0.0467 - accuracy: 0.9920 - val_loss: 0.3128 - val_accuracy: 0.9445
Epoch 39/50
263/263 [==============================] - 7s 27ms/step - loss: 0.0313 - accuracy: 0.9974 - val_loss: 0.4037 - val_accuracy: 0.9398
Epoch 40/50
263/263 [==============================] - 7s 26ms/step - loss: 0.0485 - accuracy: 0.9909 - val_loss: 0.3852 - val_accuracy: 0.9369
Epoch 41/50
263/263 [==============================] - 7s 26ms/step - loss: 0.0391 - accuracy: 0.9950 - val_loss: 0.3830 - val_accuracy: 0.9331
Epoch 42/50
263/263 [==============================] - 7s 25ms/step - loss: 0.0416 - accuracy: 0.9930 - val_loss: 0.3671 - val_accuracy: 0.9374
Epoch 43/50
263/263 [==============================] - 7s 25ms/step - loss: 0.0364 - accuracy: 0.9950 - val_loss: 0.3237 - val_accuracy: 0.9474
Epoch 44/50
263/263 [==============================] - 7s 25ms/step - loss: 0.0264 - accuracy: 0.9985 - val_loss: 0.3693 - val_accuracy: 0.9417
Epoch 45/50
263/263 [==============================] - 7s 25ms/step - loss: 0.0433 - accuracy: 0.9932 - val_loss: 0.3646 - val_accuracy: 0.9279
Epoch 46/50
263/263 [==============================] - 7s 26ms/step - loss: 0.0444 - accuracy: 0.9928 - val_loss: 0.3437 - val_accuracy: 0.9431
Epoch 47/50
263/263 [==============================] - 7s 25ms/step - loss: 0.0250 - accuracy: 0.9986 - val_loss: 0.3882 - val_accuracy: 0.9388
Epoch 48/50
263/263 [==============================] - 7s 25ms/step - loss: 0.0246 - accuracy: 0.9980 - val_loss: 0.3886 - val_accuracy: 0.9426
Epoch 49/50
263/263 [==============================] - 7s 26ms/step - loss: 0.0425 - accuracy: 0.9918 - val_loss: 0.3517 - val_accuracy: 0.9507
Epoch 50/50
263/263 [==============================] - 7s 26ms/step - loss: 0.0303 - accuracy: 0.9969 - val_loss: 0.3057 - val_accuracy: 0.9478

CV 5
Epoch 1/50
263/263 [==============================] - 17s 55ms/step - loss: 0.5526 - accuracy: 0.7123 - val_loss: 0.4276 - val_accuracy: 0.8112
Epoch 2/50
263/263 [==============================] - 7s 25ms/step - loss: 0.3423 - accuracy: 0.8639 - val_loss: 0.3358 - val_accuracy: 0.8657
Epoch 3/50
263/263 [==============================] - 7s 25ms/step - loss: 0.2658 - accuracy: 0.8976 - val_loss: 0.2966 - val_accuracy: 0.8824
Epoch 4/50
263/263 [==============================] - 7s 26ms/step - loss: 0.2423 - accuracy: 0.9117 - val_loss: 0.2690 - val_accuracy: 0.9009
Epoch 5/50
263/263 [==============================] - 7s 25ms/step - loss: 0.2077 - accuracy: 0.9260 - val_loss: 0.2571 - val_accuracy: 0.9023
Epoch 6/50
263/263 [==============================] - 7s 25ms/step - loss: 0.1903 - accuracy: 0.9318 - val_loss: 0.2284 - val_accuracy: 0.9222
Epoch 7/50
263/263 [==============================] - 7s 25ms/step - loss: 0.1657 - accuracy: 0.9445 - val_loss: 0.2258 - val_accuracy: 0.9213
Epoch 8/50
263/263 [==============================] - 7s 26ms/step - loss: 0.1652 - accuracy: 0.9432 - val_loss: 0.2013 - val_accuracy: 0.9355
Epoch 9/50
263/263 [==============================] - 7s 28ms/step - loss: 0.1431 - accuracy: 0.9538 - val_loss: 0.2040 - val_accuracy: 0.9331
Epoch 10/50
263/263 [==============================] - 7s 25ms/step - loss: 0.1299 - accuracy: 0.9596 - val_loss: 0.2155 - val_accuracy: 0.9341
Epoch 11/50
263/263 [==============================] - 7s 25ms/step - loss: 0.1153 - accuracy: 0.9677 - val_loss: 0.2186 - val_accuracy: 0.9317
Epoch 12/50
263/263 [==============================] - 7s 25ms/step - loss: 0.1065 - accuracy: 0.9691 - val_loss: 0.2047 - val_accuracy: 0.9360
Epoch 13/50
263/263 [==============================] - 7s 25ms/step - loss: 0.0938 - accuracy: 0.9743 - val_loss: 0.2146 - val_accuracy: 0.9369
Epoch 14/50
263/263 [==============================] - 7s 26ms/step - loss: 0.0882 - accuracy: 0.9766 - val_loss: 0.2535 - val_accuracy: 0.9341
Epoch 15/50
263/263 [==============================] - 7s 25ms/step - loss: 0.0912 - accuracy: 0.9754 - val_loss: 0.2184 - val_accuracy: 0.9369
Epoch 16/50
263/263 [==============================] - 7s 25ms/step - loss: 0.0677 - accuracy: 0.9835 - val_loss: 0.2354 - val_accuracy: 0.9417
Epoch 17/50
263/263 [==============================] - 7s 25ms/step - loss: 0.0715 - accuracy: 0.9815 - val_loss: 0.2516 - val_accuracy: 0.9464
Epoch 18/50
263/263 [==============================] - 7s 26ms/step - loss: 0.0579 - accuracy: 0.9867 - val_loss: 0.2567 - val_accuracy: 0.9412
Epoch 19/50
263/263 [==============================] - 7s 28ms/step - loss: 0.0695 - accuracy: 0.9823 - val_loss: 0.2783 - val_accuracy: 0.9369
Epoch 20/50
263/263 [==============================] - 7s 26ms/step - loss: 0.0583 - accuracy: 0.9846 - val_loss: 0.3258 - val_accuracy: 0.9231
Epoch 21/50
263/263 [==============================] - 7s 25ms/step - loss: 0.0544 - accuracy: 0.9884 - val_loss: 0.2713 - val_accuracy: 0.9431
Epoch 22/50
263/263 [==============================] - 7s 25ms/step - loss: 0.0559 - accuracy: 0.9876 - val_loss: 0.2624 - val_accuracy: 0.9454
Epoch 23/50
263/263 [==============================] - 7s 25ms/step - loss: 0.0464 - accuracy: 0.9901 - val_loss: 0.2678 - val_accuracy: 0.9417
Epoch 24/50
263/263 [==============================] - 7s 26ms/step - loss: 0.0545 - accuracy: 0.9887 - val_loss: 0.2304 - val_accuracy: 0.9435
Epoch 25/50
263/263 [==============================] - 7s 25ms/step - loss: 0.0389 - accuracy: 0.9932 - val_loss: 0.2915 - val_accuracy: 0.9450
Epoch 26/50
263/263 [==============================] - 7s 25ms/step - loss: 0.0637 - accuracy: 0.9851 - val_loss: 0.2919 - val_accuracy: 0.9255
Epoch 27/50
263/263 [==============================] - 7s 25ms/step - loss: 0.0538 - accuracy: 0.9894 - val_loss: 0.2124 - val_accuracy: 0.9435
Epoch 28/50
263/263 [==============================] - 7s 25ms/step - loss: 0.0286 - accuracy: 0.9975 - val_loss: 0.2864 - val_accuracy: 0.9507
Epoch 29/50
263/263 [==============================] - 7s 27ms/step - loss: 0.0464 - accuracy: 0.9887 - val_loss: 0.3532 - val_accuracy: 0.9398
Epoch 30/50
263/263 [==============================] - 7s 28ms/step - loss: 0.0390 - accuracy: 0.9923 - val_loss: 0.2825 - val_accuracy: 0.9492
Epoch 31/50
263/263 [==============================] - 7s 25ms/step - loss: 0.0379 - accuracy: 0.9943 - val_loss: 0.2991 - val_accuracy: 0.9454
Epoch 32/50
263/263 [==============================] - 7s 25ms/step - loss: 0.0388 - accuracy: 0.9931 - val_loss: 0.3238 - val_accuracy: 0.9407
Epoch 33/50
263/263 [==============================] - 7s 26ms/step - loss: 0.0400 - accuracy: 0.9930 - val_loss: 0.2848 - val_accuracy: 0.9488
Epoch 34/50
263/263 [==============================] - 7s 25ms/step - loss: 0.0354 - accuracy: 0.9947 - val_loss: 0.3607 - val_accuracy: 0.9350
Epoch 35/50
263/263 [==============================] - 7s 26ms/step - loss: 0.0479 - accuracy: 0.9909 - val_loss: 0.2716 - val_accuracy: 0.9417
Epoch 36/50
263/263 [==============================] - 7s 25ms/step - loss: 0.0425 - accuracy: 0.9930 - val_loss: 0.2982 - val_accuracy: 0.9412
Epoch 37/50
263/263 [==============================] - 7s 25ms/step - loss: 0.0332 - accuracy: 0.9957 - val_loss: 0.3064 - val_accuracy: 0.9421
Epoch 38/50
263/263 [==============================] - 7s 26ms/step - loss: 0.0391 - accuracy: 0.9930 - val_loss: 0.3159 - val_accuracy: 0.9488
Epoch 39/50
263/263 [==============================] - 7s 25ms/step - loss: 0.0214 - accuracy: 0.9988 - val_loss: 0.3776 - val_accuracy: 0.9473
Epoch 40/50
263/263 [==============================] - 7s 27ms/step - loss: 0.0334 - accuracy: 0.9939 - val_loss: 0.3528 - val_accuracy: 0.9241
Epoch 41/50
263/263 [==============================] - 7s 26ms/step - loss: 0.0408 - accuracy: 0.9907 - val_loss: 0.3231 - val_accuracy: 0.9440
Epoch 42/50
263/263 [==============================] - 7s 25ms/step - loss: 0.0196 - accuracy: 0.9994 - val_loss: 0.3313 - val_accuracy: 0.9464
Epoch 43/50
263/263 [==============================] - 7s 26ms/step - loss: 0.0296 - accuracy: 0.9961 - val_loss: 0.3367 - val_accuracy: 0.9445
Epoch 44/50
263/263 [==============================] - 7s 25ms/step - loss: 0.0429 - accuracy: 0.9903 - val_loss: 0.3703 - val_accuracy: 0.9398
Epoch 45/50
263/263 [==============================] - 7s 25ms/step - loss: 0.0447 - accuracy: 0.9888 - val_loss: 0.3111 - val_accuracy: 0.9417
Epoch 46/50
263/263 [==============================] - 7s 25ms/step - loss: 0.0230 - accuracy: 0.9976 - val_loss: 0.4020 - val_accuracy: 0.9435
Epoch 47/50
263/263 [==============================] - 7s 25ms/step - loss: 0.0436 - accuracy: 0.9903 - val_loss: 0.3020 - val_accuracy: 0.9393
Epoch 48/50
263/263 [==============================] - 7s 25ms/step - loss: 0.0235 - accuracy: 0.9981 - val_loss: 0.3608 - val_accuracy: 0.9507
Epoch 49/50
263/263 [==============================] - 7s 25ms/step - loss: 0.0374 - accuracy: 0.9929 - val_loss: 0.3193 - val_accuracy: 0.9469
Epoch 50/50
263/263 [==============================] - 7s 25ms/step - loss: 0.0223 - accuracy: 0.9980 - val_loss: 0.3957 - val_accuracy: 0.9388
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&quot;evaluasi-model&quot;&gt;Evaluasi Model&lt;/h2&gt;&lt;p&gt;Melihat kualitas model pada setiap split terhadap data validasi dengan akurasi dan f1-scorenya&lt;/p&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;for i in range(split):
    print(f&amp;quot;Split {i + 1} : {acc_scores[i]} acc - {f1_scores[i]} f1&amp;quot;)

print(&amp;quot;\nMean Acc&amp;quot;, sum(acc_scores) / split)
print(&amp;quot;Mean F1 &amp;quot;, sum(f1_scores) / split)
&lt;/code&gt;&lt;/pre&gt;&lt;pre&gt;&lt;code&gt;Split 1 : 0.9577999051683262 acc - 0.9470553242117787 f1
Split 2 : 0.9625414888572783 acc - 0.9533923303834808 f1
Split 3 : 0.9511616880037933 acc - 0.939161252215003 f1
Split 4 : 0.9506875296348981 acc - 0.938751472320377 f1
Split 5 : 0.9506641366223909 acc - 0.937799043062201 f1

Mean Acc 0.9545709496573374
Mean F1  0.943231884438568
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&quot;membuat-submission&quot;&gt;Membuat Submission&lt;/h2&gt;&lt;p&gt;Membuat submission&lt;/p&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;submission = pd.DataFrame({&amp;#x27;id&amp;#x27; : [x.split(&amp;#x27;/&amp;#x27;)[-1].split(&amp;#x27;.&amp;#x27;)[0] for x in new_test_images],
                           &amp;#x27;jenis kelamin&amp;#x27;: np.array((prediksi / split).flatten() &amp;gt;= .5, dtype = &amp;#x27;int&amp;#x27;)})
test = test.merge(submission, on=&amp;quot;id&amp;quot;)
test.head()
&lt;/code&gt;&lt;/pre&gt;&lt;table&gt;&lt;thead&gt;&lt;tr&gt;&lt;th align=&quot;center&quot;&gt;id&lt;/th&gt;&lt;th align=&quot;center&quot;&gt;jenis kelamin&lt;/th&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td align=&quot;center&quot;&gt;005093b2-8c4b-4ed7-91c3-f5f4d50f8d27&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;1&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;center&quot;&gt;0052554e-069e-4c43-beb0-0885e8f7684e&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;1&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;center&quot;&gt;0092b954-1143-4a95-a17b-1edfa6af3b01&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;0&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;center&quot;&gt;009fc28b-fe9b-441d-b8a2-ea8b7ae6ca16&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;0&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;center&quot;&gt;00d0e306-06fe-45d8-ae6c-6f83ab8f7810&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;1&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;&lt;strong&gt;Export csv&lt;/strong&gt;&lt;/p&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;test.to_csv(&amp;quot;submission-gender.csv&amp;quot;, index=False)
&lt;/code&gt;&lt;/pre&gt;&lt;h1 id=&quot;age-detection&quot;&gt;Age Detection&lt;/h1&gt;&lt;p&gt;&lt;a href=&quot;https://www.kaggle.com/code/wahyusetianto/age-bdc-2021&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/Kaggle%20Notebook-blue?logo=kaggle&quot; alt=&quot;kaggle&quot;/&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;Pada tahap semifinal kami ditugaskan untuk melakukan pendeteksian terhadap umur seseorang didalam
foto (&lt;em&gt;regression&lt;/em&gt;). Data yang disediakan merupakan data gambar yang sama pada tahap penyisihan.
Pada tahap &lt;em&gt;data preprocessing&lt;/em&gt; kami melakukan hal yang cukup sama seperti pada tahap penyisihan
namun pada tahap &lt;em&gt;maual filtering&lt;/em&gt; sedikit lebih diketatkan. Model yang digunakan oleh tim kami
dalam menyelesaikan masalah ini adalah &lt;em&gt;tensorflow custom&lt;/em&gt; model dengan arsitektur 6
&lt;code&gt;convolution layer&lt;/code&gt; yang dipadukan dengan &lt;code&gt;max pooling layer&lt;/code&gt; dan &lt;code&gt;batch normalization&lt;/code&gt;, kami
menginkan model yang lebih besar dan lebih dalam dari model yang digunakan pada tahap penyisihan.
Kemudian, ditutup dengan &lt;code&gt;dense layer&lt;/code&gt; berkativasi &lt;em&gt;linear&lt;/em&gt;. Sama seperti tahap penyisihan kami
melakukan &lt;em&gt;ensembel&lt;/em&gt; terhadap hasil prediksi 3 model yang dilatih dengan data yang berbeda
(menggunakan &lt;code&gt;StratifiedKfold&lt;/code&gt;) untuk meningkatkan kualitas prediksi.&lt;/p&gt;&lt;h2 id=&quot;first-thing-first-1&quot;&gt;First Thing First&lt;/h2&gt;&lt;p&gt;Menginstall library yang diperlukan dan mengimport library - library yang akan digunakan serta
menseting variable config yang akan digunakan di dalam notebook ini.&lt;/p&gt;&lt;ol&gt;&lt;li&gt;Menginstal library &lt;code&gt;MTCNN&lt;/code&gt; dan &lt;code&gt;Albumentations&lt;/code&gt;&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;Library &lt;code&gt;MTCNN&lt;/code&gt; dan &lt;code&gt;Albumentations&lt;/code&gt; adalah library yang digunakan untuk preprocessing data gambar
pada notebook ini&lt;/p&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;!pip -q install mtcnn
!pip -q install albumentations --upgrade
&lt;/code&gt;&lt;/pre&gt;&lt;pre&gt;&lt;code&gt;[33mWARNING: Running pip as the &amp;#x27;root&amp;#x27; user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv[0m
[33mWARNING: Running pip as the &amp;#x27;root&amp;#x27; user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv[0m
&lt;/code&gt;&lt;/pre&gt;&lt;ol start=&quot;2&quot;&gt;&lt;li&gt;Importing library&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;Mengimport library yang akan digunakan dalam notebook ini.&lt;/p&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;# Umum
import os, random, re
from tqdm.notebook import tqdm
import numpy as np
import pandas as pd
from PIL import Image

# Tensorflow
import tensorflow as tf
from tensorflow.keras.preprocessing.image import load_img, img_to_array

# Metrics &amp;amp; Splitting data
from sklearn.metrics import *
from sklearn.model_selection import *

# Plotting
import matplotlib.pyplot as plt
import seaborn as sns

# Preprocessing
import cv2
from mtcnn import MTCNN
import albumentations as A

print(&amp;quot;Tensorflow :&amp;quot;, tf.__version__)
&lt;/code&gt;&lt;/pre&gt;&lt;pre&gt;&lt;code&gt;Tensorflow : 2.6.0
&lt;/code&gt;&lt;/pre&gt;&lt;ol start=&quot;3&quot;&gt;&lt;li&gt;Setup &lt;code&gt;CONFIG&lt;/code&gt;&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;Mensetup varible - variable yang digunakan sebagai config pada notebook ini&lt;/p&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;SEED = 2021
SIZE = (200, 200)
BATCH_SIZE = 32
FACE_THRESHOLD = 0.95
FACE_DETECTOR = MTCNN()
&lt;/code&gt;&lt;/pre&gt;&lt;pre&gt;&lt;code&gt;2022-03-25 05:56:07.122802: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-03-25 05:56:07.216440: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-03-25 05:56:07.217282: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-03-25 05:56:07.218923: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-03-25 05:56:07.219966: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-03-25 05:56:07.220994: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-03-25 05:56:07.221947: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-03-25 05:56:08.885974: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-03-25 05:56:08.886927: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-03-25 05:56:08.887719: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-03-25 05:56:08.888401: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 15403 MB memory:  -&amp;gt; device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&quot;dataset-1&quot;&gt;Dataset&lt;/h2&gt;&lt;p&gt;Menggunakan dataset yang telah digunakan oleh tim kami dan melakukan loading agar mendapatkan
informasi &lt;code&gt;path&lt;/code&gt; dari data gambar&lt;/p&gt;&lt;p&gt;Dataset : &lt;a href=&quot;https://www.kaggle.com/wahyusetianto/bdc-2021&quot;&gt;Data SD20210000722&lt;/a&gt;&lt;/p&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;train = pd.read_csv(&amp;quot;../input/bdc-2021/train.csv&amp;quot;)
test = pd.read_csv(&amp;quot;../input/bdc-2021/submission.csv&amp;quot;)
train.head()
&lt;/code&gt;&lt;/pre&gt;&lt;table&gt;&lt;thead&gt;&lt;tr&gt;&lt;th align=&quot;center&quot;&gt;nomor&lt;/th&gt;&lt;th align=&quot;center&quot;&gt;jenis kelamin&lt;/th&gt;&lt;th align=&quot;center&quot;&gt;usia&lt;/th&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td align=&quot;center&quot;&gt;1&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;0&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;27&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;center&quot;&gt;2&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;1&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;24&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;center&quot;&gt;3&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;0&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;29&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;center&quot;&gt;4&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;1&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;23&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;center&quot;&gt;5&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;0&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;20&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;memperjelas &lt;code&gt;path&lt;/code&gt; ke setiap data gambar&lt;/p&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;images = []
labels = []
ages = []
test_images = []

TRAIN_DIR = &amp;quot;../input/bdc-2021/Training&amp;quot;
TEST_DIR = &amp;quot;../input/bdc-2021/Testing&amp;quot;

for no, label, usia in train[[&amp;quot;nomor&amp;quot;, &amp;quot;jenis kelamin&amp;quot;, &amp;quot;usia&amp;quot;]].values:
    TEMP_DIR = os.path.join(TRAIN_DIR, str(no))
    for file in os.listdir(TEMP_DIR):
        file_dir = os.path.join(TEMP_DIR, file)
        if &amp;quot;.ini&amp;quot; not in file_dir:
            images.append(file_dir)
            labels.append(label)
            ages.append(usia)

for no in test.id.values:
    file_dir = os.path.join(TEST_DIR, f&amp;quot;{no}.jpg&amp;quot;)
    if os.path.isfile(file_dir):
        test_images.append(file_dir)
    else:
        test_images.append(None)
        print(file_dir)
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;menampilkan dan mengecek beberapa gambar pada data &lt;code&gt;train&lt;/code&gt;&lt;/p&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;def read(path):
    &amp;quot;&amp;quot;&amp;quot;
    Read data gambar
    &amp;quot;&amp;quot;&amp;quot;
    img = Image.open(path)
    return img

def show_images(list_dir, label = None, age = None, load_image = read, seed = SEED):
    &amp;quot;&amp;quot;&amp;quot;
    Menampilkan Gambar Secara acak sebanyak 5 buah.
    &amp;quot;&amp;quot;&amp;quot;
    random.seed(seed)
    unique = [&amp;quot;init&amp;quot;]
    if label:
        unique = list(set(label))
    fig, axes = plt.subplots(len(unique), 5, figsize = (20, 5 * len(unique)))
    for i in range(len(unique)):
        if i == 0 and unique[i] == &amp;quot;init&amp;quot;:
            data = random.sample(list_dir, 5)
        elif age != None:
            data = random.sample([x for x in zip(list_dir, label, age) if x[1] == unique[i]], 5)
        else:
            data = random.sample([x for x in zip(list_dir, label) if x[1] == unique[i]], 5)
        for j in range(5):
            if unique[0] != &amp;quot;init&amp;quot;:
                img = load_image(data[j][0])
                text = f&amp;#x27;Label : {data[j][1]}&amp;#x27;
                if age != None:
                    text += f&amp;#x27;\nAge : {data[j][2]}&amp;#x27;
                axes[i, j].imshow(img)
                axes[i, j].set_title(text, fontsize = 14)
                axes[i, j].axis(&amp;#x27;off&amp;#x27;)
            else:
                img = load_image(data[j])
                axes[j].imshow(img)
                axes[j].axis(&amp;#x27;off&amp;#x27;)
    fig.tight_layout()
    plt.show()
&lt;/code&gt;&lt;/pre&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;show_images(images, labels, ages, seed=20)
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;span class=&quot;gatsby-resp-image-wrapper&quot; style=&quot;position:relative;display:block;margin-left:auto;margin-right:auto;max-width:630px&quot;&gt;
      &lt;a class=&quot;gatsby-resp-image-link&quot; href=&quot;/static/4c6c8f93bc1f0b93805ebe875e8e8f22/45662/age_output_14_0.png&quot; style=&quot;display:block&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;
    &lt;span class=&quot;gatsby-resp-image-background-image&quot; style=&quot;padding-bottom:50.632911392405056%;position:relative;bottom:0;left:0;background-image:url(&amp;#x27;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAKCAYAAAC0VX7mAAAACXBIWXMAAAsTAAALEwEAmpwYAAADNUlEQVQozwEqA9X8AOfm51liXFhzm5aTdPb190n8+vpK1d7adtHY1nP7+PhPxcW7Hq+yri+2qqou//b0I8rJySC6oqgto5KYMMirtR3m4t9Dx8XDdIp5e3Hi4eFmAK+qqe+hhHH/w6eX/+Li5ML68vDHiZ2T/4SAdv/r39/XwMG3z+Pn4P/Xz8350L+87ebc2drErbf7i3N2/ZBiaN3f1NbGxbOu/6uEf//c19f/AMG1r93Jm4X8x5uH/9jQzLP++/u4g3N5/517bv/p4d3HjoqF1sPGwP+Mioj/ioSA9d7g4OKWobX/cE1N/5draObFv8K7t4V//7aBgPjx8O/6AMfBv+eziW//x6GQ/+no6Lvw8PHAOTZF/zs2Pv/g3+DPUU1K4ZiYk/9UUFP/ZWFi/662uOt9lKT/LCAp/ykiKu+ulpDDnXJu/518f//c1db/ANXIw8rPrZTp1rGY8e3j3KT+//+ow8bT8MfL2u3///+2PkA/mHFaWrhpWmG0lJqeraCioa9wc3vIHRkkygwOGbGciIGnmHt68aOGhuWdmKLlANXKvnaVj4WRjoFxlKeZiWDFycZipKCYlZqYjpNNYFpmh49nRl1nO3JGQjFwOi4eU7SgpQD/6dgA///1AKefnQC2t7xOnpZ+lnxtU46KiYCGALG0sOdjW1P/a2FY/0lKQru1sKvAgXxz/2BUS/9JTkfFYmNIu0pIMP9mWEP/g3pm4aalnXu/taehxrmto8C5rn/Ly82wk4p3/5aRff+nmoD/ALS3ueBdXF3/XV1f/09SS7a9rKS6oJSJ/8uQf/+zi3u/VEo0sYhdRf+thG3/YVpK1ejm39uWkpP/fWhi/3xnXfSjkIq8mXBn/3pkWvurmX39AKOmqOFESlD/SEtS/0dHTLbQwrq7vquo/7B3aP+ffXDAZGZdsph4Zf+beWf/cm5j16WqrNFpb3n/XUlA/3NaSel+d3W7c2Fc/35wbPyypJj+AKKelOF7foT/nqCm/5KRjbexoKy7cGJr/3RcXv9iUFTA1dXOsrGvpf+hlYP/wbmt1h8pNEeIioZWbU4+WTwYBEGXnKKjk5WV/42NjPt2aWD/Ug/2rUnLCKUAAAAASUVORK5CYII=&amp;#x27;);background-size:cover;display:block&quot;&gt;&lt;/span&gt;
  &lt;img class=&quot;gatsby-resp-image-image&quot; alt=&quot;png&quot; title=&quot;png&quot; src=&quot;/static/4c6c8f93bc1f0b93805ebe875e8e8f22/f058b/age_output_14_0.png&quot; srcSet=&quot;/static/4c6c8f93bc1f0b93805ebe875e8e8f22/c26ae/age_output_14_0.png 158w,/static/4c6c8f93bc1f0b93805ebe875e8e8f22/6bdcf/age_output_14_0.png 315w,/static/4c6c8f93bc1f0b93805ebe875e8e8f22/f058b/age_output_14_0.png 630w,/static/4c6c8f93bc1f0b93805ebe875e8e8f22/40601/age_output_14_0.png 945w,/static/4c6c8f93bc1f0b93805ebe875e8e8f22/78612/age_output_14_0.png 1260w,/static/4c6c8f93bc1f0b93805ebe875e8e8f22/45662/age_output_14_0.png 1410w&quot; sizes=&quot;(max-width: 630px) 100vw, 630px&quot; style=&quot;width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0&quot; loading=&quot;lazy&quot; decoding=&quot;async&quot;/&gt;
  &lt;/a&gt;
    &lt;/span&gt;&lt;/p&gt;&lt;h2 id=&quot;preprocess-data-1&quot;&gt;Preprocess Data&lt;/h2&gt;&lt;p&gt;Metode yang digunakan:&lt;/p&gt;&lt;ol&gt;&lt;li&gt;Mengekstrak wajah - wajah yang terdapat pada gambar menjadi gambar - gambar baru dengan label
yang sama dengan menggunakan model &lt;code&gt;MTCNN&lt;/code&gt;&lt;/li&gt;&lt;li&gt;Pada data test jika terdapat dua wajah yang terdeteksi pada satu gambar akan di ambil wajah
dengan tingkat confidence terbesar yang diberikan oleh model &lt;code&gt;MTCNN&lt;/code&gt;.&lt;/li&gt;&lt;li&gt;Jika tidak terdetect wajah pada salah satu gambar maka akan dilakukan crop pada bagian tengah
gambar sehingga gambar berbentuk persegi atau &lt;code&gt;jxj&lt;/code&gt; pixel.&lt;/li&gt;&lt;li&gt;Selanjutnya gambar akan di resize menjadi ukuran &lt;code&gt;256x256&lt;/code&gt; pixel&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;berikut adalah contoh hasil preprocess data gambar.&lt;/p&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;def get_faces(path):
    image = cv2.cvtColor(cv2.imread(path), cv2.COLOR_BGR2RGB)
    faces = FACE_DETECTOR.detect_faces(image)
    return faces

def load_and_preprocess_image(path: str, size = SIZE):
    &amp;quot;&amp;quot;&amp;quot;
    Load &amp;amp; Preprocess data gambar
    &amp;quot;&amp;quot;&amp;quot;
    image = img_to_array(load_img(path))
    faces = [x[&amp;#x27;box&amp;#x27;] for x in get_faces(path) if x[&amp;#x27;confidence&amp;#x27;] &amp;gt; FACE_THRESHOLD]
    if len(faces) &amp;gt; 0:
        x, y, w, h = faces[0]
        image = image[y:y+h, x:x+w]
    img = tf.convert_to_tensor(image, dtype=tf.float32)
    if len(faces) == 0:
        shapes = tf.shape(img)
        h, w = shapes[-3], shapes[-2]
        dim = tf.minimum(h, w)
        img = tf.image.resize_with_crop_or_pad(img, dim, dim)
    img = tf.image.resize(img, size)
    img = tf.cast(img, tf.float32) / 255.0
    return img.numpy()
&lt;/code&gt;&lt;/pre&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;show_images(images, labels, ages, load_image = load_and_preprocess_image, seed=20)
&lt;/code&gt;&lt;/pre&gt;&lt;pre&gt;&lt;code&gt;2022-03-25 05:56:16.153559: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)
2022-03-25 05:56:17.086578: I tensorflow/stream_executor/cuda/cuda_dnn.cc:369] Loaded cuDNN version 8005
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;span class=&quot;gatsby-resp-image-wrapper&quot; style=&quot;position:relative;display:block;margin-left:auto;margin-right:auto;max-width:630px&quot;&gt;
      &lt;a class=&quot;gatsby-resp-image-link&quot; href=&quot;/static/0a8e3f19e96272465bb8fec17f2fdf33/5bd27/age_output_17_1.png&quot; style=&quot;display:block&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;
    &lt;span class=&quot;gatsby-resp-image-background-image&quot; style=&quot;padding-bottom:46.202531645569614%;position:relative;bottom:0;left:0;background-image:url(&amp;#x27;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAJCAYAAAAywQxIAAAACXBIWXMAAAsTAAALEwEAmpwYAAAC5ElEQVQozwHZAib9AHJbTYbJp4+ft450nLCmoYw4Ok+EloF/n8aplZ1pRzSJCwYLhxwSFJ57V0+dWD46h0dLYIldPT2eckQ8nkg3OITHv7+Lo4mJnqSJhp2mlpmEAMCdh/bOq5b/y5+D/5J6cf9pX2nxu6GW/8uvnP+AXET6RzUz9o1waf+0jIH/bVFN9k9DUPpwQz3/nWVY/3FRSvGkjoz/oH12/7uYj/+Vg4XyANelj+HUqJb80aWN+6BxX+nGr6fd1Liv/cifj/ymc1Xl0quf4dqvpP3Cl4r9kG5o4YBZV+WZW1H8nmRY/a55bN26hoHpyIuE+8CMhv2piY/eALGFbfXhsZb/1qOS/5ZpW/5zZl7xwqeh/5NbVf9LLh/5vqOY9c2dk/+ziHv/qZ2X9VM9P/mdY1X/oGRZ/21NS/Gca2b+snBs/657fP/UxsnxAHhZQDaceWBEs5N8Qph/cDgAAAA1UkdERAAAAEIAAAA3xq6kNryjmkN2Z19Cq7/ANgADEzdnRDxDe01HQxIXJzViNzs4XTY2Q4pxd0LZ5+01AHZnXqq7lHvGqoZxw15QR7GFaV6o26OTxt6cicOseGiuBgEAq0AsHsXAlHLElXheq5prV66vbFDFk1xDxWpHO6hfXl+xnX1sxW1HNcQrFgyoAMqqnfPNoo3/zaaT/8GikfzLl4Tv0JJ//9WSf/+weGP4kWhN85BmTv/KmHz/t5WA86uHePiba1v/fVNG/3ZPQO/OvLn816qa/4FVRP9rRjjwANaumujYq53/1aiW/8mnj/HgpZLl6Z2N/9mLeP+0f2rtqn5g6cOAY//OlYL/nn9x6cGkl+2cbmL/gFJF/3NKPOXxzcLx5K+f/6ltXf92TUHlAIVzatLasKPry6KN62FTSNq4g3TP2Ydz7LJrWeuLfHDWfV9H0qNkTeyob2DslIx/0rOtq9auhXvreE9E7FM4Mc+PaFXa2aiT6plmV+xVPzzPAuqbRfqQI68AAAAASUVORK5CYII=&amp;#x27;);background-size:cover;display:block&quot;&gt;&lt;/span&gt;
  &lt;img class=&quot;gatsby-resp-image-image&quot; alt=&quot;png&quot; title=&quot;png&quot; src=&quot;/static/0a8e3f19e96272465bb8fec17f2fdf33/f058b/age_output_17_1.png&quot; srcSet=&quot;/static/0a8e3f19e96272465bb8fec17f2fdf33/c26ae/age_output_17_1.png 158w,/static/0a8e3f19e96272465bb8fec17f2fdf33/6bdcf/age_output_17_1.png 315w,/static/0a8e3f19e96272465bb8fec17f2fdf33/f058b/age_output_17_1.png 630w,/static/0a8e3f19e96272465bb8fec17f2fdf33/40601/age_output_17_1.png 945w,/static/0a8e3f19e96272465bb8fec17f2fdf33/78612/age_output_17_1.png 1260w,/static/0a8e3f19e96272465bb8fec17f2fdf33/5bd27/age_output_17_1.png 1432w&quot; sizes=&quot;(max-width: 630px) 100vw, 630px&quot; style=&quot;width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0&quot; loading=&quot;lazy&quot; decoding=&quot;async&quot;/&gt;
  &lt;/a&gt;
    &lt;/span&gt;&lt;/p&gt;&lt;h2 id=&quot;augmentasi-data-1&quot;&gt;Augmentasi Data&lt;/h2&gt;&lt;p&gt;Melakukan augmentasi untuk memperbanyak data. Metode augmentasi yang digunakan yaitu:&lt;/p&gt;&lt;ol&gt;&lt;li&gt;Horizontal flip&lt;/li&gt;&lt;li&gt;Donwscale kualitas gambar&lt;/li&gt;&lt;li&gt;Random rotate dengan rentang -30 sampai 30 derajad&lt;/li&gt;&lt;li&gt;Shift, scale, dan rotate gambar&lt;/li&gt;&lt;li&gt;Blur&lt;/li&gt;&lt;li&gt;Random brightness&lt;/li&gt;&lt;/ol&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;aug = A.Compose([
    A.HorizontalFlip(p=0.4),
    A.Downscale(scale_min=0.6, scale_max=0.9, p=0.3),
    A.Rotate(limit=(-30,30), p=0.6),
    A.ShiftScaleRotate(shift_limit=(-0.07, 0.07), scale_limit=(-0.05, 0.1), rotate_limit=(-15, 15), p=0.4),
    A.OneOf([
        A.MotionBlur(p=.4),
        A.MedianBlur(blur_limit=3, p=0.4),
        A.Blur(blur_limit=3, p=0.4),
    ], p=0.4),
    A.RandomBrightnessContrast(brightness_limit=(-0.25, 0.15), p=0.4),
])
&lt;/code&gt;&lt;/pre&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;def visualize_aug(path):
    fig, axes = plt.subplots(1, 5, figsize = (20, 5))
    image = load_and_preprocess_image(path)
    axes[0].imshow(image)
    axes[0].axis(&amp;#x27;off&amp;#x27;)
    for i in range(1, 5):
        augmented = aug(image=image)[&amp;#x27;image&amp;#x27;]
        axes[i].imshow(augmented)
        axes[i].axis(&amp;#x27;off&amp;#x27;)
    fig.tight_layout()
    plt.show()
&lt;/code&gt;&lt;/pre&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;random.seed(SEED)
for i in range(3):
    visualize_aug(images[i])
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;span class=&quot;gatsby-resp-image-wrapper&quot; style=&quot;position:relative;display:block;margin-left:auto;margin-right:auto;max-width:630px&quot;&gt;
      &lt;a class=&quot;gatsby-resp-image-link&quot; href=&quot;/static/3855eaee77f20270f295a581a4eff2ca/5bd27/age_output_21_0.png&quot; style=&quot;display:block&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;
    &lt;span class=&quot;gatsby-resp-image-background-image&quot; style=&quot;padding-bottom:20.253164556962027%;position:relative;bottom:0;left:0;background-image:url(&amp;#x27;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAECAYAAACOXx+WAAAACXBIWXMAAAsTAAALEwEAmpwYAAABT0lEQVQY0wFEAbv+AHFJRM7bno3nyJCA5lA1M9ZwRT3LuoNz55tqXOZAJiPSVTQwztCRf+fppJHnZj04zmVCPtLeoY/mxI1951c7OMtwTEbWwYp65tOZieeiamDLAIBZU+rWoZL/y5iK/7Z+cPN4TkbntIV2/6d5bP+pcGHvyIZ069ScjP/kp5X/elBK63dSTu/ZpJT/yZeJ/8SHd+fMkoHzw5SH/9illf+ZaV/nAJJpX+bfoI//3KGR/8OPfu5zTkbiuYBx/7eDdP+4gnHq4qCL5uWlk//tpJH/i19W5oNdVerhoZD/2KCQ/9mei+LZopDu15+P/9GYif+AXFfiADUmJueqfHH/56yd/7mFde8jFBPjgltR/7yHev+rd2fr0JF85/Kwn/+xfXH/KBgZ5zAiI+uuf3T/46ma/8ePfePTnY3v1p6Q/7mHeP89Kyrj1BPHv/8qjUcAAAAASUVORK5CYII=&amp;#x27;);background-size:cover;display:block&quot;&gt;&lt;/span&gt;
  &lt;img class=&quot;gatsby-resp-image-image&quot; alt=&quot;png&quot; title=&quot;png&quot; src=&quot;/static/3855eaee77f20270f295a581a4eff2ca/f058b/age_output_21_0.png&quot; srcSet=&quot;/static/3855eaee77f20270f295a581a4eff2ca/c26ae/age_output_21_0.png 158w,/static/3855eaee77f20270f295a581a4eff2ca/6bdcf/age_output_21_0.png 315w,/static/3855eaee77f20270f295a581a4eff2ca/f058b/age_output_21_0.png 630w,/static/3855eaee77f20270f295a581a4eff2ca/40601/age_output_21_0.png 945w,/static/3855eaee77f20270f295a581a4eff2ca/78612/age_output_21_0.png 1260w,/static/3855eaee77f20270f295a581a4eff2ca/5bd27/age_output_21_0.png 1432w&quot; sizes=&quot;(max-width: 630px) 100vw, 630px&quot; style=&quot;width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0&quot; loading=&quot;lazy&quot; decoding=&quot;async&quot;/&gt;
  &lt;/a&gt;
    &lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span class=&quot;gatsby-resp-image-wrapper&quot; style=&quot;position:relative;display:block;margin-left:auto;margin-right:auto;max-width:630px&quot;&gt;
      &lt;a class=&quot;gatsby-resp-image-link&quot; href=&quot;/static/05a5599d806472efeefce9abc58c9c20/5bd27/age_output_21_1.png&quot; style=&quot;display:block&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;
    &lt;span class=&quot;gatsby-resp-image-background-image&quot; style=&quot;padding-bottom:20.253164556962027%;position:relative;bottom:0;left:0;background-image:url(&amp;#x27;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAECAYAAACOXx+WAAAACXBIWXMAAAsTAAALEwEAmpwYAAABT0lEQVQY0wFEAbv+AGtRSs7FnYvnw5+O5oVqYNaff2/LxqCN56eDdOZcRD/SroZ0zr+Zh+e1k4XnnX5xzmBDNtJ8WkrmfVhG51gzJMtpTkXWr4x85qmJeud0XFPLAIdlXerKoI7/tY6C/7KOgPOoiHrnt5GD/7uSgv+jeGvvnnhp68ehj/+4j4L/x5qJ631XSe9xTED/iWJR/1ExJud0V0/zs45+/557cP+gf3LnALmJeebPn4z/0qGR/8OZie6wi3zixJWH/8ucif+8j33qe1tT5sKSgv/Yp5T/0qOR5pdoV+qOYVD/glVF/08rIuKUbF/uuY18/7qOf/+xinviAIRrYefImYj/3bGf/6GDdO+McWbjx56M/9eqmf+qgnLrm3Bi5513af/LnYv/2rKf55NrWeuSZFP/flJA/1c2KuNzW1Hvp39w/8Sci/+ef2/jXJDF5tuHwV4AAAAASUVORK5CYII=&amp;#x27;);background-size:cover;display:block&quot;&gt;&lt;/span&gt;
  &lt;img class=&quot;gatsby-resp-image-image&quot; alt=&quot;png&quot; title=&quot;png&quot; src=&quot;/static/05a5599d806472efeefce9abc58c9c20/f058b/age_output_21_1.png&quot; srcSet=&quot;/static/05a5599d806472efeefce9abc58c9c20/c26ae/age_output_21_1.png 158w,/static/05a5599d806472efeefce9abc58c9c20/6bdcf/age_output_21_1.png 315w,/static/05a5599d806472efeefce9abc58c9c20/f058b/age_output_21_1.png 630w,/static/05a5599d806472efeefce9abc58c9c20/40601/age_output_21_1.png 945w,/static/05a5599d806472efeefce9abc58c9c20/78612/age_output_21_1.png 1260w,/static/05a5599d806472efeefce9abc58c9c20/5bd27/age_output_21_1.png 1432w&quot; sizes=&quot;(max-width: 630px) 100vw, 630px&quot; style=&quot;width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0&quot; loading=&quot;lazy&quot; decoding=&quot;async&quot;/&gt;
  &lt;/a&gt;
    &lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span class=&quot;gatsby-resp-image-wrapper&quot; style=&quot;position:relative;display:block;margin-left:auto;margin-right:auto;max-width:630px&quot;&gt;
      &lt;a class=&quot;gatsby-resp-image-link&quot; href=&quot;/static/8e4d3a7d5ba58df38d68b22e004e126c/5bd27/age_output_21_2.png&quot; style=&quot;display:block&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;
    &lt;span class=&quot;gatsby-resp-image-background-image&quot; style=&quot;padding-bottom:20.253164556962027%;position:relative;bottom:0;left:0;background-image:url(&amp;#x27;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAECAYAAACOXx+WAAAACXBIWXMAAAsTAAALEwEAmpwYAAABT0lEQVQY0wFEAbv+AIJYOM7Um3DntX9U5kQpF9bOmm7L1J1x55pnPeZKLh3SSy4czqx2Sefnr4DnnG1GzjUdENK7glPm1Z1z559sRstoQynW0Jdt5r2HWudDKRnLAHBKNOqOXUT/nWxK/6tzRPOZZ0fniFtC/6x2UP+3dUHvvYBM65doSP+baU3/hlk865lmPO+hcUz/iFlA/31TO+duRzHzjl1E/5xsSf+mcEPnAHhJMuagZUX/nmNG/6FkOO5wRi/iqm1L/5RcQ/+aXDPqpGQ85ptkR/+fZ0j/akIt5rJxPuqkaEj/oWhI/5BZPOJ4STHunGFD/6BmR/+ycT7iABMKCeduQTD/oWRE/209IO82IBfjVTAi/5xgRf+PVTDrjVY155VaQv9rPSr/OSIY53dCIuuYX0H/hlA7/zAaE+MNBgfvfks4/5tgP/9sPB7jcIql25QfsCAAAAAASUVORK5CYII=&amp;#x27;);background-size:cover;display:block&quot;&gt;&lt;/span&gt;
  &lt;img class=&quot;gatsby-resp-image-image&quot; alt=&quot;png&quot; title=&quot;png&quot; src=&quot;/static/8e4d3a7d5ba58df38d68b22e004e126c/f058b/age_output_21_2.png&quot; srcSet=&quot;/static/8e4d3a7d5ba58df38d68b22e004e126c/c26ae/age_output_21_2.png 158w,/static/8e4d3a7d5ba58df38d68b22e004e126c/6bdcf/age_output_21_2.png 315w,/static/8e4d3a7d5ba58df38d68b22e004e126c/f058b/age_output_21_2.png 630w,/static/8e4d3a7d5ba58df38d68b22e004e126c/40601/age_output_21_2.png 945w,/static/8e4d3a7d5ba58df38d68b22e004e126c/78612/age_output_21_2.png 1260w,/static/8e4d3a7d5ba58df38d68b22e004e126c/5bd27/age_output_21_2.png 1432w&quot; sizes=&quot;(max-width: 630px) 100vw, 630px&quot; style=&quot;width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0&quot; loading=&quot;lazy&quot; decoding=&quot;async&quot;/&gt;
  &lt;/a&gt;
    &lt;/span&gt;&lt;/p&gt;&lt;p&gt;Running preprocessing pada data gambar secara keseluruhan&lt;/p&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;def image_preprocessing(new_dir, images, labels=None):
    if os.path.isdir(new_dir):
        !rm -rf {new_dir}
    os.mkdir(new_dir)

    new_images, new_labels = [], []
    if not labels:
        labels = [None for _ in range(len(images))]

    for path, label in tqdm(zip(images, labels), total=len(images)):
        image = img_to_array(load_img(path))
        if label != None:
            faces = [x[&amp;#x27;box&amp;#x27;] for x in sorted(get_faces(path), key=lambda x: x[&amp;#x27;confidence&amp;#x27;],
                                              reverse=True) if x[&amp;#x27;confidence&amp;#x27;] &amp;gt; FACE_THRESHOLD]
        else:
            faces = [x[&amp;#x27;box&amp;#x27;] for x in sorted(get_faces(path), key=lambda x: x[&amp;#x27;confidence&amp;#x27;], reverse=True)]
        if len(faces) &amp;gt; 0:
            if label != None:
                for j, (x, y, w, h) in enumerate(faces):
                    img = image[y:y+h, x:x+w]
                    img = tf.convert_to_tensor(img, dtype=tf.float32)
                    img = tf.image.resize(img, SIZE)
                    img = tf.cast(img, tf.float32) / 255.0

                    img_dir = os.path.join(new_dir, f&amp;#x27;{j}_{path.split(&amp;quot;/&amp;quot;)[-1]}&amp;#x27;)
                    new_images.append(img_dir)
                    new_labels.append(label)
                    tf.keras.preprocessing.image.save_img(img_dir, img)

                    for k in range(3):
                        augmented = aug(image=img.numpy())[&amp;#x27;image&amp;#x27;]
                        img_dir = os.path.join(new_dir, f&amp;#x27;aug-{k}_{j}_{path.split(&amp;quot;/&amp;quot;)[-1]}&amp;#x27;)
                        new_images.append(img_dir)
                        new_labels.append(label)
                        tf.keras.preprocessing.image.save_img(img_dir, augmented)
            else:
                x, y, w, h = faces[0]
                img = image[y:y+h, x:x+w]
                img = tf.convert_to_tensor(img, dtype=tf.float32)
                img = tf.image.resize(img, SIZE)
                img = tf.cast(img, tf.float32) / 255.0

                img_dir = os.path.join(new_dir, path.split(&amp;#x27;/&amp;#x27;)[-1])
                new_images.append(img_dir)
                new_labels.append(label)
                tf.keras.preprocessing.image.save_img(img_dir, img)
        else :
            img = tf.convert_to_tensor(image, dtype=tf.float32)
            shapes = tf.shape(img)
            h, w = shapes[-3], shapes[-2]
            dim = tf.minimum(h, w)
            img = tf.image.resize_with_crop_or_pad(img, dim, dim)
            img = tf.image.resize(img, SIZE)
            img = tf.cast(img, tf.float32) / 255.0

            img_dir = os.path.join(new_dir, path.split(&amp;#x27;/&amp;#x27;)[-1])
            new_images.append(img_dir)
            new_labels.append(label)
            tf.keras.preprocessing.image.save_img(img_dir, img)

            if label != None:
                for k in range(3):
                    augmented = aug(image=img.numpy())[&amp;#x27;image&amp;#x27;]
                    img_dir = os.path.join(new_dir,  f&amp;#x27;aug-{k}_{path.split(&amp;quot;/&amp;quot;)[-1]}&amp;#x27;)
                    new_images.append(img_dir)
                    new_labels.append(label)
                    tf.keras.preprocessing.image.save_img(img_dir, augmented)

    return new_images, new_labels
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Untuk menghemat waktu running akan di skip bagian ini dan di ganti dengan meload data hasil
preprocess yang sudah di save pada run sebelumnya. Namun jika ingin melakukan preprocess pada run
sekarang maka uncomment code di bawah ini.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Peringatan&lt;/strong&gt; : running block code di bawah memakan waktu sekitar 50 menit dengan GPU Nvidia
Tesla P100-PCIE.&lt;/p&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;# new_train_dir = &amp;quot;./train&amp;quot;
# new_test_dir = &amp;quot;./test&amp;quot;

# random.seed(SEED)
# new_images, new_labels = image_preprocessing(new_train_dir, images, labels)
# new_test_images, _ = image_preprocessing(new_test_dir, test_images)
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt; : Comment dua block kode di bawah jika melakukan preprocess pada run saat ini.&lt;/p&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;preprocessed = pd.read_csv(&amp;quot;../input/bdc-2021/preprocessed-augmented/preprocessed.csv&amp;quot;)
preprocessed.head()
&lt;/code&gt;&lt;/pre&gt;&lt;table&gt;&lt;thead&gt;&lt;tr&gt;&lt;th align=&quot;left&quot;&gt;image&lt;/th&gt;&lt;th align=&quot;center&quot;&gt;label&lt;/th&gt;&lt;th align=&quot;center&quot;&gt;age&lt;/th&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;./train/0_1_3.jpg&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;0&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;27&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;./train/aug-0_0_1_3.jpg&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;0&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;27&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;./train/aug-1_0_1_3.jpg&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;0&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;27&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;./train/aug-2_0_1_3.jpg&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;0&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;27&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;./train/0_1_1.jpg&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;0&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;27&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;h2 id=&quot;filterisasi-data-1&quot;&gt;Filterisasi Data&lt;/h2&gt;&lt;p&gt;Menyaring data yang akan dilatih ke dalam model dengan melakukan filterisasi terhadap data gambar
yang memiliki kualitas gambar yang kurang baik dan misslabel.&lt;/p&gt;&lt;ol&gt;&lt;li&gt;Mendownload list data gambar yang akan dibuang.&lt;/li&gt;&lt;/ol&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;!wget https://raw.githubusercontent.com/Hyuto/bdc-2021/master/excluded-age.txt

with open(&amp;quot;./excluded-age.txt&amp;quot;) as f:
    excluded = [x for x in f.read().split(&amp;quot;\n&amp;quot;) if x != &amp;#x27;&amp;#x27;]

patterns = fr&amp;#x27;{&amp;quot;|&amp;quot;.join(excluded)}&amp;#x27;
&lt;/code&gt;&lt;/pre&gt;&lt;pre&gt;&lt;code&gt;--2022-03-25 05:56:47--  https://raw.githubusercontent.com/Hyuto/bdc-2021/master/excluded-age.txt
Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.111.133, 185.199.110.133, ...
Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.
HTTP request sent, awaiting response... 200 OK
Length: 4407 (4.3K) [text/plain]
Saving to: â€˜excluded-age.txtâ€™

excluded-age.txt    100%[===================&amp;gt;]   4.30K  --.-KB/s    in 0s

2022-03-25 05:56:47 (37.3 MB/s) - â€˜excluded-age.txtâ€™ saved [4407/4407]
&lt;/code&gt;&lt;/pre&gt;&lt;ol start=&quot;2&quot;&gt;&lt;li&gt;Filterisasi data&lt;/li&gt;&lt;/ol&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;preprocessed_dir = &amp;quot;../input/bdc-2021/preprocessed-augmented&amp;quot;
new_images, new_labels, new_ages = [], [], []

for image, label, age in preprocessed[[&amp;quot;image&amp;quot;, &amp;quot;label&amp;quot;, &amp;quot;age&amp;quot;]].values:
    if not re.search(patterns, image):
        new_images.append(os.path.join(preprocessed_dir, image))
        new_labels.append(label)
        new_ages.append(age)

new_test_images = np.asarray([os.path.join(preprocessed_dir, &amp;quot;test&amp;quot;, f&amp;quot;{x}.jpg&amp;quot;) for x in test.id.values])

new_images = np.asarray(new_images)
new_labels = np.asarray(new_labels)
new_ages = np.asarray(new_ages)
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Mengecek distribusi label pada data&lt;/p&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;plt.figure(figsize=(10, 5))
sns.countplot(x=new_ages)
plt.xlabel(&amp;quot;Umur&amp;quot;, fontsize=16)
plt.show()
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;span class=&quot;gatsby-resp-image-wrapper&quot; style=&quot;position:relative;display:block;margin-left:auto;margin-right:auto;max-width:618px&quot;&gt;
      &lt;a class=&quot;gatsby-resp-image-link&quot; href=&quot;/static/a8ce00691bcc1b02f13f9b205db2b6a0/6e6fb/age_output_33_0.png&quot; style=&quot;display:block&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;
    &lt;span class=&quot;gatsby-resp-image-background-image&quot; style=&quot;padding-bottom:51.89873417721519%;position:relative;bottom:0;left:0;background-image:url(&amp;#x27;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAKCAYAAAC0VX7mAAAACXBIWXMAAAsTAAALEwEAmpwYAAACGElEQVQoz4WR70tTURjHz4te9C9EiNxJL4LFQCqEgkUgRU7MFFZLoQY1xpBBkQnRq3oR+G4FQQlRC6KiXjjZnKJJJeburnmR5kJMN3emU1v3pu3eu93nPnF/VHMIfeHLOYfn8Hm+5zkEEUkymTwuSdKcLMt8tdVyhedoit/c+sFrFZWXaupVnlUU5YsgCHGiAxOJRBvWCFAz1qepKA4usliUJfyfJEkqGECO485oplTLoAKAzn0+H4PrE2FYKG4YZ116vcaqDiyVSst/Ero0zUhkUPWNCsYdfPl1DC8MhXA6nzGTaxpaV/5Kh1rALCGE7Emn06fMQCYQzDquikt4Z+o+eocfY4h7j2YjMICmdwFaT26pTghWusjcAHpjQQyMPcPbH2I48m3Jgv5LacF3AlmWbdHMzgCaOTulXIJHEzfAP9oHwfEweKOv4CHHgVQx6ztmCDUz5GZmWqtnspZj8eP0AwyNXMXA+C0MvgtjYPQt3pv6hJGFFcyKCsIuv6yUlTyRf26T2STn3Fpfz/8Sirm15c809tpPn7zw0P7By9QX7aX+6ADtib/J+CJDKzfjk7RveJ5Opjbpak6khUWBilkht/1dLGxkKEv6e3qJr+P83rse975r7tN1507sZzpcDuZs5zGbs93OHGo/zNhbTzbUNTqOHHQ67Y7OrnpHW7eNabrENDV221wHumwXGQ9zpdnd4DnaXP8b5s6VfamEtrYAAAAASUVORK5CYII=&amp;#x27;);background-size:cover;display:block&quot;&gt;&lt;/span&gt;
  &lt;img class=&quot;gatsby-resp-image-image&quot; alt=&quot;png&quot; title=&quot;png&quot; src=&quot;/static/a8ce00691bcc1b02f13f9b205db2b6a0/6e6fb/age_output_33_0.png&quot; srcSet=&quot;/static/a8ce00691bcc1b02f13f9b205db2b6a0/c26ae/age_output_33_0.png 158w,/static/a8ce00691bcc1b02f13f9b205db2b6a0/6bdcf/age_output_33_0.png 315w,/static/a8ce00691bcc1b02f13f9b205db2b6a0/6e6fb/age_output_33_0.png 618w&quot; sizes=&quot;(max-width: 618px) 100vw, 618px&quot; style=&quot;width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0&quot; loading=&quot;lazy&quot; decoding=&quot;async&quot;/&gt;
  &lt;/a&gt;
    &lt;/span&gt;&lt;/p&gt;&lt;p&gt;Jumlah data yang berlabel &lt;code&gt;0&lt;/code&gt; dan &lt;code&gt;1&lt;/code&gt; cenderung sama.&lt;/p&gt;&lt;h2 id=&quot;modelling-1&quot;&gt;Modelling&lt;/h2&gt;&lt;ol&gt;&lt;li&gt;Membuat model dengan arsitektur sebagai berikut&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/Hyuto/bdc-2021/master/assets/arsitektur-model.png&quot; alt=&quot;Arsitektur Model&quot;/&gt;&lt;/p&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;def build_model(kernel_s=(3,3)):
    model = tf.keras.models.Sequential([
        tf.keras.layers.Conv2D(32,kernel_s,activation=&amp;#x27;relu&amp;#x27;,input_shape=(200,200,3),
                            kernel_regularizer=tf.keras.regularizers.l2(0.001),padding=&amp;quot;VALID&amp;quot;),
        tf.keras.layers.BatchNormalization(),
        tf.keras.layers.AveragePooling2D((2,2)),
        tf.keras.layers.Conv2D(64,kernel_s,activation=&amp;#x27;relu&amp;#x27;),
        tf.keras.layers.BatchNormalization(),
        tf.keras.layers.AveragePooling2D((2,2)),
        tf.keras.layers.Conv2D(64,kernel_s,activation=&amp;#x27;relu&amp;#x27;),
        tf.keras.layers.BatchNormalization(),
        tf.keras.layers.AveragePooling2D((2,2)),
        tf.keras.layers.Conv2D(128,kernel_s,activation=&amp;#x27;relu&amp;#x27;),
        tf.keras.layers.BatchNormalization(),
        tf.keras.layers.AveragePooling2D((2,2)),
        tf.keras.layers.Conv2D(128,kernel_s,activation=&amp;#x27;relu&amp;#x27;),
        tf.keras.layers.BatchNormalization(),
        tf.keras.layers.AveragePooling2D((2,2)),
        tf.keras.layers.Conv2D(256,kernel_s,activation=&amp;#x27;relu&amp;#x27;),
        tf.keras.layers.BatchNormalization(),
        tf.keras.layers.AveragePooling2D((2,2)),
        tf.keras.layers.Flatten(),
        tf.keras.layers.Dense(512, activation=&amp;#x27;relu&amp;#x27;,
                              kernel_regularizer=tf.keras.regularizers.l2(5e-4)),
        tf.keras.layers.Dense(1, activation=&amp;#x27;linear&amp;#x27;),
    ])
    model.compile(loss=&amp;quot;mse&amp;quot;, optimizer=&amp;quot;adamax&amp;quot;, metrics=[&amp;#x27;mse&amp;#x27;])
    return model
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;strong&gt;Tensorflow Data&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;Load data gambar menggunakan &lt;code&gt;Tensorflow Data&lt;/code&gt; agar pada saat pelatihan model penggunaan memmori
dapat lebih optimal&lt;/p&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;def decode_image(filename, label=None, image_size=SIZE):
    &amp;quot;&amp;quot;&amp;quot;
    Decode Image from String Path Tensor
    &amp;quot;&amp;quot;&amp;quot;
    bits = tf.io.read_file(filename)
    image = tf.image.decode_jpeg(bits, channels=3)
    image = tf.cast(image, tf.float32) / 255.0
    image = tf.image.resize(image, SIZE)

    if label is None: # if test
        return image
    else:
        return image, label
&lt;/code&gt;&lt;/pre&gt;&lt;ol start=&quot;2&quot;&gt;&lt;li&gt;Training Model&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;Proses ini dilakukan dengan mensplit data menggunakan &lt;code&gt;StratifiedKFold&lt;/code&gt; sebanyak 3 split lalu untuk
setiap split akan dibangun sebuah model untuk dilatih. Setiap model ini akan digunakan untuk
melakukan peramalan terhadap data test. Pada proses trainingnya setiap model akan diukur tingkat
kebaikannya dengan menggunakan metric &lt;code&gt;MSE&lt;/code&gt; lalu dikakukan penyimpanan weight pada model saat
&lt;code&gt;val_mse&lt;/code&gt; berada pada nilai terkecil selama pelatihan model sebanyak &lt;code&gt;150 epochs&lt;/code&gt;.&lt;/p&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;split = 3
prediksi = np.zeros((len(new_test_images), 1), dtype=np.float32)
mse_scores = []

test_dataset = (
    tf.data.Dataset
    .from_tensor_slices((new_test_images))
    .map(decode_image)
    .batch(BATCH_SIZE)
)

cv = StratifiedKFold(n_splits=split, shuffle=True, random_state=SEED)
for i, (train_index, test_index) in enumerate(cv.split(new_images, new_labels)):
    tf.keras.backend.clear_session()
    x_train, x_valid = new_images[train_index], new_images[test_index]
    y_train, y_valid = new_ages[train_index], new_ages[test_index]

    train_dataset = (
        tf.data.Dataset
        .from_tensor_slices((x_train, y_train))
        .map(decode_image, num_parallel_calls=tf.data.AUTOTUNE)
        .cache()
        .repeat()
        .shuffle(1024)
        .batch(BATCH_SIZE)
        .prefetch(tf.data.AUTOTUNE)
    )

    valid_dataset = (
        tf.data.Dataset
        .from_tensor_slices((x_valid, y_valid))
        .map(decode_image, num_parallel_calls=tf.data.AUTOTUNE)
        .batch(BATCH_SIZE)
        .cache()
        .prefetch(tf.data.AUTOTUNE)
    )

    model = build_model()
    checkpoint = tf.keras.callbacks.ModelCheckpoint(f&amp;#x27;{i}_best_model.h5&amp;#x27;, monitor=&amp;#x27;val_mse&amp;#x27;,
                                                save_best_only=True, save_weights_only=True,
                                                mode=&amp;#x27;min&amp;#x27;)
    print(f&amp;quot;\nCV {i+1}&amp;quot;)
    model.fit(train_dataset, epochs=150, validation_data=valid_dataset,
              steps_per_epoch=len(x_train) // BATCH_SIZE,
              callbacks = [checkpoint])
    model.load_weights(f&amp;#x27;{i}_best_model.h5&amp;#x27;)
    mse = mean_squared_error(y_valid, model.predict(valid_dataset))
    mse_scores.append(mse)

    prediksi += model.predict(test_dataset)

    del train_dataset
    del valid_dataset
&lt;/code&gt;&lt;/pre&gt;&lt;pre&gt;&lt;code&gt;CV 1
Epoch 1/150
224/224 [==============================] - 33s 128ms/step - loss: 61.7490 - mse: 61.5615 - val_loss: 30.7318 - val_mse: 30.5421
Epoch 2/150
224/224 [==============================] - 8s 36ms/step - loss: 29.0049 - mse: 28.8152 - val_loss: 29.6056 - val_mse: 29.4154
Epoch 3/150
224/224 [==============================] - 8s 35ms/step - loss: 28.5166 - mse: 28.3266 - val_loss: 29.6574 - val_mse: 29.4673
Epoch 4/150
224/224 [==============================] - 8s 37ms/step - loss: 27.6579 - mse: 27.4678 - val_loss: 29.3373 - val_mse: 29.1470
Epoch 5/150
224/224 [==============================] - 8s 36ms/step - loss: 28.0873 - mse: 27.8968 - val_loss: 31.8402 - val_mse: 31.6497
Epoch 6/150
224/224 [==============================] - 8s 35ms/step - loss: 27.4662 - mse: 27.2757 - val_loss: 30.3539 - val_mse: 30.1629
Epoch 7/150
224/224 [==============================] - 8s 36ms/step - loss: 26.8039 - mse: 26.6133 - val_loss: 31.2905 - val_mse: 31.0997
Epoch 8/150
224/224 [==============================] - 8s 37ms/step - loss: 26.3319 - mse: 26.1414 - val_loss: 29.7415 - val_mse: 29.5506
Epoch 9/150
224/224 [==============================] - 8s 36ms/step - loss: 25.4253 - mse: 25.2351 - val_loss: 27.7372 - val_mse: 27.5469
Epoch 10/150
224/224 [==============================] - 8s 35ms/step - loss: 25.3611 - mse: 25.1712 - val_loss: 33.1380 - val_mse: 32.9477
Epoch 11/150
224/224 [==============================] - 8s 36ms/step - loss: 24.6144 - mse: 24.4248 - val_loss: 29.9749 - val_mse: 29.7853
Epoch 12/150
224/224 [==============================] - 8s 37ms/step - loss: 23.9499 - mse: 23.7608 - val_loss: 29.4134 - val_mse: 29.2245
Epoch 13/150
224/224 [==============================] - 8s 36ms/step - loss: 23.0786 - mse: 22.8898 - val_loss: 27.3832 - val_mse: 27.1947
Epoch 14/150
224/224 [==============================] - 8s 35ms/step - loss: 22.6779 - mse: 22.4899 - val_loss: 33.9357 - val_mse: 33.7476
Epoch 15/150
224/224 [==============================] - 8s 36ms/step - loss: 22.2556 - mse: 22.0682 - val_loss: 27.5627 - val_mse: 27.3751
Epoch 16/150
224/224 [==============================] - 8s 37ms/step - loss: 20.8796 - mse: 20.6928 - val_loss: 33.5097 - val_mse: 33.3230
Epoch 17/150
224/224 [==============================] - 8s 36ms/step - loss: 20.6927 - mse: 20.5065 - val_loss: 32.3534 - val_mse: 32.1673
Epoch 18/150
224/224 [==============================] - 8s 35ms/step - loss: 19.0894 - mse: 18.9040 - val_loss: 28.2080 - val_mse: 28.0223
Epoch 19/150
224/224 [==============================] - 8s 36ms/step - loss: 18.2932 - mse: 18.1084 - val_loss: 28.2822 - val_mse: 28.0978
Epoch 20/150
224/224 [==============================] - 8s 36ms/step - loss: 17.2254 - mse: 17.0413 - val_loss: 32.1861 - val_mse: 32.0021
Epoch 21/150
224/224 [==============================] - 8s 36ms/step - loss: 16.3377 - mse: 16.1545 - val_loss: 26.4559 - val_mse: 26.2731
Epoch 22/150
224/224 [==============================] - 8s 35ms/step - loss: 15.9257 - mse: 15.7434 - val_loss: 28.1726 - val_mse: 27.9902
Epoch 23/150
224/224 [==============================] - 8s 36ms/step - loss: 14.7741 - mse: 14.5924 - val_loss: 24.8542 - val_mse: 24.6724
Epoch 24/150
224/224 [==============================] - 8s 35ms/step - loss: 13.6252 - mse: 13.4443 - val_loss: 23.9924 - val_mse: 23.8113
Epoch 25/150
224/224 [==============================] - 8s 36ms/step - loss: 12.9564 - mse: 12.7763 - val_loss: 27.7363 - val_mse: 27.5563
Epoch 26/150
224/224 [==============================] - 8s 36ms/step - loss: 12.3001 - mse: 12.1208 - val_loss: 29.5507 - val_mse: 29.3713
Epoch 27/150
224/224 [==============================] - 8s 36ms/step - loss: 11.1503 - mse: 10.9718 - val_loss: 24.5016 - val_mse: 24.3226
Epoch 28/150
224/224 [==============================] - 8s 35ms/step - loss: 10.8054 - mse: 10.6275 - val_loss: 22.2076 - val_mse: 22.0297
Epoch 29/150
224/224 [==============================] - 8s 38ms/step - loss: 10.2948 - mse: 10.1178 - val_loss: 26.2559 - val_mse: 26.0787
Epoch 30/150
224/224 [==============================] - 8s 36ms/step - loss: 9.3143 - mse: 9.1377 - val_loss: 21.5713 - val_mse: 21.3949
Epoch 31/150
224/224 [==============================] - 8s 35ms/step - loss: 9.1448 - mse: 8.9690 - val_loss: 23.0846 - val_mse: 22.9089
Epoch 32/150
224/224 [==============================] - 8s 35ms/step - loss: 8.2162 - mse: 8.0408 - val_loss: 20.2005 - val_mse: 20.0250
Epoch 33/150
224/224 [==============================] - 8s 37ms/step - loss: 7.5496 - mse: 7.3748 - val_loss: 23.0445 - val_mse: 22.8701
Epoch 34/150
224/224 [==============================] - 8s 36ms/step - loss: 7.3638 - mse: 7.1897 - val_loss: 20.2571 - val_mse: 20.0833
Epoch 35/150
224/224 [==============================] - 8s 36ms/step - loss: 7.1349 - mse: 6.9613 - val_loss: 17.7547 - val_mse: 17.5815
Epoch 36/150
224/224 [==============================] - 8s 35ms/step - loss: 6.3319 - mse: 6.1589 - val_loss: 20.1022 - val_mse: 19.9290
Epoch 37/150
224/224 [==============================] - 9s 38ms/step - loss: 5.9729 - mse: 5.8007 - val_loss: 19.1848 - val_mse: 19.0123
Epoch 38/150
224/224 [==============================] - 8s 36ms/step - loss: 5.7994 - mse: 5.6276 - val_loss: 17.1907 - val_mse: 17.0189
Epoch 39/150
224/224 [==============================] - 8s 36ms/step - loss: 5.5629 - mse: 5.3916 - val_loss: 19.8856 - val_mse: 19.7144
Epoch 40/150
224/224 [==============================] - 8s 35ms/step - loss: 5.1661 - mse: 4.9953 - val_loss: 18.0269 - val_mse: 17.8563
Epoch 41/150
224/224 [==============================] - 9s 38ms/step - loss: 5.0516 - mse: 4.8812 - val_loss: 19.4839 - val_mse: 19.3134
Epoch 42/150
224/224 [==============================] - 8s 36ms/step - loss: 5.0344 - mse: 4.8644 - val_loss: 19.9552 - val_mse: 19.7853
Epoch 43/150
224/224 [==============================] - 8s 36ms/step - loss: 4.6756 - mse: 4.5061 - val_loss: 20.2364 - val_mse: 20.0670
Epoch 44/150
224/224 [==============================] - 8s 35ms/step - loss: 4.5706 - mse: 4.4015 - val_loss: 17.8152 - val_mse: 17.6461
Epoch 45/150
224/224 [==============================] - 8s 38ms/step - loss: 4.2943 - mse: 4.1256 - val_loss: 17.3017 - val_mse: 17.1329
Epoch 46/150
224/224 [==============================] - 8s 36ms/step - loss: 4.4884 - mse: 4.3201 - val_loss: 19.3418 - val_mse: 19.1733
Epoch 47/150
224/224 [==============================] - 8s 35ms/step - loss: 3.9889 - mse: 3.8208 - val_loss: 15.4163 - val_mse: 15.2484
Epoch 48/150
224/224 [==============================] - 8s 35ms/step - loss: 3.7844 - mse: 3.6168 - val_loss: 16.3840 - val_mse: 16.2162
Epoch 49/150
224/224 [==============================] - 8s 38ms/step - loss: 3.6326 - mse: 3.4654 - val_loss: 16.1904 - val_mse: 16.0234
Epoch 50/150
224/224 [==============================] - 8s 36ms/step - loss: 3.6943 - mse: 3.5277 - val_loss: 16.8564 - val_mse: 16.6901
Epoch 51/150
224/224 [==============================] - 8s 36ms/step - loss: 3.5135 - mse: 3.3473 - val_loss: 15.8205 - val_mse: 15.6544
Epoch 52/150
224/224 [==============================] - 8s 36ms/step - loss: 3.2410 - mse: 3.0751 - val_loss: 16.3657 - val_mse: 16.1998
Epoch 53/150
224/224 [==============================] - 8s 37ms/step - loss: 3.2900 - mse: 3.1245 - val_loss: 16.6403 - val_mse: 16.4746
Epoch 54/150
224/224 [==============================] - 8s 36ms/step - loss: 3.4023 - mse: 3.2373 - val_loss: 17.6016 - val_mse: 17.4366
Epoch 55/150
224/224 [==============================] - 8s 35ms/step - loss: 2.9931 - mse: 2.8283 - val_loss: 14.9742 - val_mse: 14.8091
Epoch 56/150
224/224 [==============================] - 8s 36ms/step - loss: 3.0442 - mse: 2.8796 - val_loss: 15.8485 - val_mse: 15.6838
Epoch 57/150
224/224 [==============================] - 8s 37ms/step - loss: 3.1544 - mse: 2.9902 - val_loss: 15.8307 - val_mse: 15.6672
Epoch 58/150
224/224 [==============================] - 8s 37ms/step - loss: 2.9040 - mse: 2.7403 - val_loss: 16.1261 - val_mse: 15.9625
Epoch 59/150
224/224 [==============================] - 8s 35ms/step - loss: 2.9849 - mse: 2.8215 - val_loss: 14.7911 - val_mse: 14.6273
Epoch 60/150
224/224 [==============================] - 8s 35ms/step - loss: 2.7384 - mse: 2.5753 - val_loss: 16.3628 - val_mse: 16.1993
Epoch 61/150
224/224 [==============================] - 8s 36ms/step - loss: 2.9298 - mse: 2.7670 - val_loss: 14.8591 - val_mse: 14.6961
Epoch 62/150
224/224 [==============================] - 8s 36ms/step - loss: 2.7468 - mse: 2.5842 - val_loss: 14.6470 - val_mse: 14.4843
Epoch 63/150
224/224 [==============================] - 8s 35ms/step - loss: 2.7764 - mse: 2.6143 - val_loss: 16.3168 - val_mse: 16.1545
Epoch 64/150
224/224 [==============================] - 8s 35ms/step - loss: 2.6768 - mse: 2.5149 - val_loss: 18.9223 - val_mse: 18.7606
Epoch 65/150
224/224 [==============================] - 8s 36ms/step - loss: 2.4564 - mse: 2.2949 - val_loss: 14.0731 - val_mse: 13.9115
Epoch 66/150
224/224 [==============================] - 8s 36ms/step - loss: 2.3944 - mse: 2.2331 - val_loss: 14.9244 - val_mse: 14.7632
Epoch 67/150
224/224 [==============================] - 8s 35ms/step - loss: 2.4751 - mse: 2.3141 - val_loss: 14.7471 - val_mse: 14.5859
Epoch 68/150
224/224 [==============================] - 8s 36ms/step - loss: 2.3118 - mse: 2.1511 - val_loss: 14.3834 - val_mse: 14.2226
Epoch 69/150
224/224 [==============================] - 8s 36ms/step - loss: 2.3627 - mse: 2.2021 - val_loss: 14.9056 - val_mse: 14.7448
Epoch 70/150
224/224 [==============================] - 8s 37ms/step - loss: 2.2792 - mse: 2.1191 - val_loss: 14.1932 - val_mse: 14.0330
Epoch 71/150
224/224 [==============================] - 8s 36ms/step - loss: 2.4510 - mse: 2.2912 - val_loss: 14.0827 - val_mse: 13.9231
Epoch 72/150
224/224 [==============================] - 8s 35ms/step - loss: 2.2523 - mse: 2.0929 - val_loss: 14.1287 - val_mse: 13.9692
Epoch 73/150
224/224 [==============================] - 8s 35ms/step - loss: 2.2514 - mse: 2.0923 - val_loss: 13.7133 - val_mse: 13.5544
Epoch 74/150
224/224 [==============================] - 8s 36ms/step - loss: 2.3210 - mse: 2.1622 - val_loss: 17.3318 - val_mse: 17.1733
Epoch 75/150
224/224 [==============================] - 8s 36ms/step - loss: 2.2503 - mse: 2.0919 - val_loss: 14.0075 - val_mse: 13.8494
Epoch 76/150
224/224 [==============================] - 8s 35ms/step - loss: 2.2867 - mse: 2.1285 - val_loss: 13.5774 - val_mse: 13.4197
Epoch 77/150
224/224 [==============================] - 8s 35ms/step - loss: 2.2291 - mse: 2.0713 - val_loss: 14.3507 - val_mse: 14.1927
Epoch 78/150
224/224 [==============================] - 8s 36ms/step - loss: 2.1369 - mse: 1.9792 - val_loss: 14.5678 - val_mse: 14.4101
Epoch 79/150
224/224 [==============================] - 8s 37ms/step - loss: 2.0839 - mse: 1.9266 - val_loss: 15.1398 - val_mse: 14.9826
Epoch 80/150
224/224 [==============================] - 8s 35ms/step - loss: 1.9892 - mse: 1.8324 - val_loss: 13.6943 - val_mse: 13.5377
Epoch 81/150
224/224 [==============================] - 8s 35ms/step - loss: 2.0517 - mse: 1.8954 - val_loss: 13.5791 - val_mse: 13.4233
Epoch 82/150
224/224 [==============================] - 8s 36ms/step - loss: 1.9524 - mse: 1.7966 - val_loss: 14.3110 - val_mse: 14.1550
Epoch 83/150
224/224 [==============================] - 8s 37ms/step - loss: 1.9200 - mse: 1.7646 - val_loss: 14.2576 - val_mse: 14.1017
Epoch 84/150
224/224 [==============================] - 8s 36ms/step - loss: 1.8745 - mse: 1.7194 - val_loss: 13.8831 - val_mse: 13.7282
Epoch 85/150
224/224 [==============================] - 8s 35ms/step - loss: 1.8472 - mse: 1.6926 - val_loss: 14.2939 - val_mse: 14.1394
Epoch 86/150
224/224 [==============================] - 8s 36ms/step - loss: 1.9061 - mse: 1.7519 - val_loss: 14.5727 - val_mse: 14.4188
Epoch 87/150
224/224 [==============================] - 8s 35ms/step - loss: 1.8161 - mse: 1.6623 - val_loss: 17.0817 - val_mse: 16.9279
Epoch 88/150
224/224 [==============================] - 8s 37ms/step - loss: 1.8448 - mse: 1.6914 - val_loss: 13.2618 - val_mse: 13.1085
Epoch 89/150
224/224 [==============================] - 8s 35ms/step - loss: 1.7935 - mse: 1.6404 - val_loss: 13.3743 - val_mse: 13.2208
Epoch 90/150
224/224 [==============================] - 8s 36ms/step - loss: 1.7868 - mse: 1.6341 - val_loss: 13.1837 - val_mse: 13.0313
Epoch 91/150
224/224 [==============================] - 8s 35ms/step - loss: 1.7573 - mse: 1.6050 - val_loss: 14.3036 - val_mse: 14.1513
Epoch 92/150
224/224 [==============================] - 8s 36ms/step - loss: 1.7767 - mse: 1.6246 - val_loss: 14.7067 - val_mse: 14.5548
Epoch 93/150
224/224 [==============================] - 8s 36ms/step - loss: 1.7215 - mse: 1.5699 - val_loss: 13.1582 - val_mse: 13.0069
Epoch 94/150
224/224 [==============================] - 8s 36ms/step - loss: 1.6420 - mse: 1.4908 - val_loss: 13.1509 - val_mse: 12.9996
Epoch 95/150
224/224 [==============================] - 8s 35ms/step - loss: 1.6509 - mse: 1.5001 - val_loss: 12.8322 - val_mse: 12.6814
Epoch 96/150
224/224 [==============================] - 8s 35ms/step - loss: 1.5931 - mse: 1.4429 - val_loss: 12.7185 - val_mse: 12.5683
Epoch 97/150
224/224 [==============================] - 9s 38ms/step - loss: 1.7260 - mse: 1.5760 - val_loss: 13.0205 - val_mse: 12.8709
Epoch 98/150
224/224 [==============================] - 8s 36ms/step - loss: 1.6453 - mse: 1.4957 - val_loss: 12.6698 - val_mse: 12.5202
Epoch 99/150
224/224 [==============================] - 8s 35ms/step - loss: 1.5136 - mse: 1.3644 - val_loss: 15.1047 - val_mse: 14.9557
Epoch 100/150
224/224 [==============================] - 8s 35ms/step - loss: 1.5625 - mse: 1.4138 - val_loss: 16.6367 - val_mse: 16.4880
Epoch 101/150
224/224 [==============================] - 8s 37ms/step - loss: 1.5482 - mse: 1.3999 - val_loss: 13.8176 - val_mse: 13.6693
Epoch 102/150
224/224 [==============================] - 8s 37ms/step - loss: 1.6382 - mse: 1.4902 - val_loss: 13.7476 - val_mse: 13.5999
Epoch 103/150
224/224 [==============================] - 8s 35ms/step - loss: 1.5542 - mse: 1.4065 - val_loss: 15.5675 - val_mse: 15.4197
Epoch 104/150
224/224 [==============================] - 8s 35ms/step - loss: 1.5140 - mse: 1.3667 - val_loss: 14.7609 - val_mse: 14.6142
Epoch 105/150
224/224 [==============================] - 8s 35ms/step - loss: 1.6224 - mse: 1.4753 - val_loss: 13.6264 - val_mse: 13.4792
Epoch 106/150
224/224 [==============================] - 9s 39ms/step - loss: 1.4618 - mse: 1.3152 - val_loss: 13.3492 - val_mse: 13.2023
Epoch 107/150
224/224 [==============================] - 8s 35ms/step - loss: 1.5253 - mse: 1.3792 - val_loss: 16.6365 - val_mse: 16.4902
Epoch 108/150
224/224 [==============================] - 8s 35ms/step - loss: 1.6125 - mse: 1.4665 - val_loss: 13.5693 - val_mse: 13.4238
Epoch 109/150
224/224 [==============================] - 8s 36ms/step - loss: 1.4243 - mse: 1.2788 - val_loss: 13.4972 - val_mse: 13.3518
Epoch 110/150
224/224 [==============================] - 8s 36ms/step - loss: 1.4297 - mse: 1.2846 - val_loss: 12.8761 - val_mse: 12.7304
Epoch 111/150
224/224 [==============================] - 8s 36ms/step - loss: 1.4686 - mse: 1.3236 - val_loss: 13.0742 - val_mse: 12.9294
Epoch 112/150
224/224 [==============================] - 8s 35ms/step - loss: 1.5276 - mse: 1.3829 - val_loss: 12.9483 - val_mse: 12.8037
Epoch 113/150
224/224 [==============================] - 8s 35ms/step - loss: 1.3553 - mse: 1.2111 - val_loss: 12.7453 - val_mse: 12.6016
Epoch 114/150
224/224 [==============================] - 8s 38ms/step - loss: 1.4306 - mse: 1.2870 - val_loss: 13.7208 - val_mse: 13.5774
Epoch 115/150
224/224 [==============================] - 8s 37ms/step - loss: 1.4006 - mse: 1.2573 - val_loss: 13.4571 - val_mse: 13.3140
Epoch 116/150
224/224 [==============================] - 8s 35ms/step - loss: 1.4423 - mse: 1.2994 - val_loss: 13.2137 - val_mse: 13.0711
Epoch 117/150
224/224 [==============================] - 8s 35ms/step - loss: 1.3852 - mse: 1.2426 - val_loss: 12.7637 - val_mse: 12.6217
Epoch 118/150
224/224 [==============================] - 8s 35ms/step - loss: 1.3654 - mse: 1.2234 - val_loss: 13.1005 - val_mse: 12.9587
Epoch 119/150
224/224 [==============================] - 8s 36ms/step - loss: 1.3657 - mse: 1.2239 - val_loss: 14.5568 - val_mse: 14.4148
Epoch 120/150
224/224 [==============================] - 8s 35ms/step - loss: 1.3499 - mse: 1.2087 - val_loss: 15.5286 - val_mse: 15.3876
Epoch 121/150
224/224 [==============================] - 8s 35ms/step - loss: 1.3829 - mse: 1.2420 - val_loss: 12.7452 - val_mse: 12.6042
Epoch 122/150
224/224 [==============================] - 8s 35ms/step - loss: 1.4210 - mse: 1.2804 - val_loss: 12.6050 - val_mse: 12.4644
Epoch 123/150
224/224 [==============================] - 9s 39ms/step - loss: 1.3179 - mse: 1.1776 - val_loss: 12.3164 - val_mse: 12.1762
Epoch 124/150
224/224 [==============================] - 8s 35ms/step - loss: 1.2820 - mse: 1.1421 - val_loss: 13.6001 - val_mse: 13.4604
Epoch 125/150
224/224 [==============================] - 8s 36ms/step - loss: 1.3649 - mse: 1.2253 - val_loss: 12.9455 - val_mse: 12.8059
Epoch 126/150
224/224 [==============================] - 8s 35ms/step - loss: 1.1833 - mse: 1.0442 - val_loss: 12.7510 - val_mse: 12.6117
Epoch 127/150
224/224 [==============================] - 8s 36ms/step - loss: 1.2822 - mse: 1.1435 - val_loss: 12.4807 - val_mse: 12.3421
Epoch 128/150
224/224 [==============================] - 8s 36ms/step - loss: 1.2425 - mse: 1.1043 - val_loss: 12.2466 - val_mse: 12.1088
Epoch 129/150
224/224 [==============================] - 8s 35ms/step - loss: 1.2899 - mse: 1.1521 - val_loss: 13.6597 - val_mse: 13.5218
Epoch 130/150
224/224 [==============================] - 8s 35ms/step - loss: 1.2488 - mse: 1.1112 - val_loss: 14.3895 - val_mse: 14.2520
Epoch 131/150
224/224 [==============================] - 9s 38ms/step - loss: 1.2578 - mse: 1.1206 - val_loss: 12.1461 - val_mse: 12.0086
Epoch 132/150
224/224 [==============================] - 8s 36ms/step - loss: 1.2285 - mse: 1.0916 - val_loss: 13.5352 - val_mse: 13.3985
Epoch 133/150
224/224 [==============================] - 8s 35ms/step - loss: 1.2124 - mse: 1.0759 - val_loss: 13.3632 - val_mse: 13.2268
Epoch 134/150
224/224 [==============================] - 8s 35ms/step - loss: 1.1782 - mse: 1.0421 - val_loss: 12.7990 - val_mse: 12.6631
Epoch 135/150
224/224 [==============================] - 8s 36ms/step - loss: 1.2040 - mse: 1.0683 - val_loss: 12.4254 - val_mse: 12.2898
Epoch 136/150
224/224 [==============================] - 8s 36ms/step - loss: 1.1161 - mse: 0.9807 - val_loss: 12.3986 - val_mse: 12.2635
Epoch 137/150
224/224 [==============================] - 8s 35ms/step - loss: 1.2211 - mse: 1.0862 - val_loss: 12.3183 - val_mse: 12.1836
Epoch 138/150
224/224 [==============================] - 8s 35ms/step - loss: 1.1373 - mse: 1.0029 - val_loss: 11.9633 - val_mse: 11.8288
Epoch 139/150
224/224 [==============================] - 8s 37ms/step - loss: 1.1589 - mse: 1.0248 - val_loss: 13.3995 - val_mse: 13.2656
Epoch 140/150
224/224 [==============================] - 8s 38ms/step - loss: 1.0995 - mse: 0.9659 - val_loss: 13.9962 - val_mse: 13.8627
Epoch 141/150
224/224 [==============================] - 8s 35ms/step - loss: 1.1864 - mse: 1.0531 - val_loss: 12.6507 - val_mse: 12.5173
Epoch 142/150
224/224 [==============================] - 8s 35ms/step - loss: 1.1720 - mse: 1.0391 - val_loss: 12.6915 - val_mse: 12.5582
Epoch 143/150
224/224 [==============================] - 8s 36ms/step - loss: 1.1724 - mse: 1.0396 - val_loss: 13.1602 - val_mse: 13.0276
Epoch 144/150
224/224 [==============================] - 8s 36ms/step - loss: 1.0312 - mse: 0.8989 - val_loss: 12.0131 - val_mse: 11.8810
Epoch 145/150
224/224 [==============================] - 8s 35ms/step - loss: 1.1703 - mse: 1.0385 - val_loss: 12.4866 - val_mse: 12.3547
Epoch 146/150
224/224 [==============================] - 8s 36ms/step - loss: 1.1232 - mse: 0.9916 - val_loss: 13.0720 - val_mse: 12.9402
Epoch 147/150
224/224 [==============================] - 8s 36ms/step - loss: 1.0162 - mse: 0.8849 - val_loss: 12.3481 - val_mse: 12.2168
Epoch 148/150
224/224 [==============================] - 8s 37ms/step - loss: 1.2107 - mse: 1.0798 - val_loss: 13.0532 - val_mse: 12.9225
Epoch 149/150
224/224 [==============================] - 8s 37ms/step - loss: 1.0977 - mse: 0.9673 - val_loss: 12.4154 - val_mse: 12.2850
Epoch 150/150
224/224 [==============================] - 8s 36ms/step - loss: 1.1413 - mse: 1.0112 - val_loss: 13.1140 - val_mse: 12.9839

CV 2
Epoch 1/150
224/224 [==============================] - 19s 76ms/step - loss: 65.3573 - mse: 65.1696 - val_loss: 28.1471 - val_mse: 27.9575
Epoch 2/150
224/224 [==============================] - 8s 35ms/step - loss: 29.3658 - mse: 29.1761 - val_loss: 29.2328 - val_mse: 29.0427
Epoch 3/150
224/224 [==============================] - 8s 36ms/step - loss: 28.6027 - mse: 28.4125 - val_loss: 31.0996 - val_mse: 30.9088
Epoch 4/150
224/224 [==============================] - 8s 35ms/step - loss: 29.1083 - mse: 28.9178 - val_loss: 34.1321 - val_mse: 33.9410
Epoch 5/150
224/224 [==============================] - 9s 38ms/step - loss: 28.6279 - mse: 28.4375 - val_loss: 28.0483 - val_mse: 27.8579
Epoch 6/150
224/224 [==============================] - 8s 36ms/step - loss: 27.8268 - mse: 27.6364 - val_loss: 30.3272 - val_mse: 30.1364
Epoch 7/150
224/224 [==============================] - 8s 36ms/step - loss: 27.4756 - mse: 27.2852 - val_loss: 27.7932 - val_mse: 27.6030
Epoch 8/150
224/224 [==============================] - 8s 36ms/step - loss: 27.9995 - mse: 27.8094 - val_loss: 33.2439 - val_mse: 33.0532
Epoch 9/150
224/224 [==============================] - 8s 35ms/step - loss: 26.4406 - mse: 26.2508 - val_loss: 27.6317 - val_mse: 27.4419
Epoch 10/150
224/224 [==============================] - 8s 35ms/step - loss: 27.0694 - mse: 26.8799 - val_loss: 39.3276 - val_mse: 39.1379
Epoch 11/150
224/224 [==============================] - 8s 36ms/step - loss: 26.0196 - mse: 25.8306 - val_loss: 49.3599 - val_mse: 49.1708
Epoch 12/150
224/224 [==============================] - 8s 36ms/step - loss: 25.0996 - mse: 24.9111 - val_loss: 28.7126 - val_mse: 28.5241
Epoch 13/150
224/224 [==============================] - 8s 36ms/step - loss: 24.3809 - mse: 24.1927 - val_loss: 31.1182 - val_mse: 30.9302
Epoch 14/150
224/224 [==============================] - 9s 38ms/step - loss: 23.6208 - mse: 23.4332 - val_loss: 41.3011 - val_mse: 41.1134
Epoch 15/150
224/224 [==============================] - 8s 36ms/step - loss: 23.6479 - mse: 23.4610 - val_loss: 48.9201 - val_mse: 48.7333
Epoch 16/150
224/224 [==============================] - 8s 35ms/step - loss: 22.8764 - mse: 22.6903 - val_loss: 30.3477 - val_mse: 30.1613
Epoch 17/150
224/224 [==============================] - 8s 35ms/step - loss: 22.2860 - mse: 22.1007 - val_loss: 28.8005 - val_mse: 28.6153
Epoch 18/150
224/224 [==============================] - 8s 35ms/step - loss: 21.1264 - mse: 20.9420 - val_loss: 29.7282 - val_mse: 29.5435
Epoch 19/150
224/224 [==============================] - 8s 35ms/step - loss: 21.0464 - mse: 20.8629 - val_loss: 25.0158 - val_mse: 24.8325
Epoch 20/150
224/224 [==============================] - 8s 36ms/step - loss: 19.8903 - mse: 19.7076 - val_loss: 25.2338 - val_mse: 25.0511
Epoch 21/150
224/224 [==============================] - 8s 35ms/step - loss: 18.7813 - mse: 18.5994 - val_loss: 31.8400 - val_mse: 31.6579
Epoch 22/150
224/224 [==============================] - 8s 37ms/step - loss: 17.8403 - mse: 17.6595 - val_loss: 26.4239 - val_mse: 26.2432
Epoch 23/150
224/224 [==============================] - 8s 38ms/step - loss: 17.1947 - mse: 17.0150 - val_loss: 25.4340 - val_mse: 25.2549
Epoch 24/150
224/224 [==============================] - 8s 36ms/step - loss: 16.2822 - mse: 16.1036 - val_loss: 26.5665 - val_mse: 26.3882
Epoch 25/150
224/224 [==============================] - 8s 35ms/step - loss: 15.4414 - mse: 15.2638 - val_loss: 23.3567 - val_mse: 23.1793
Epoch 26/150
224/224 [==============================] - 8s 35ms/step - loss: 13.9219 - mse: 13.7453 - val_loss: 24.3640 - val_mse: 24.1876
Epoch 27/150
224/224 [==============================] - 8s 36ms/step - loss: 13.3928 - mse: 13.2171 - val_loss: 20.1073 - val_mse: 19.9319
Epoch 28/150
224/224 [==============================] - 8s 36ms/step - loss: 12.5223 - mse: 12.3475 - val_loss: 20.8471 - val_mse: 20.6727
Epoch 29/150
224/224 [==============================] - 8s 35ms/step - loss: 11.6667 - mse: 11.4927 - val_loss: 23.1861 - val_mse: 23.0123
Epoch 30/150
224/224 [==============================] - 8s 35ms/step - loss: 10.6564 - mse: 10.4831 - val_loss: 20.3115 - val_mse: 20.1384
Epoch 31/150
224/224 [==============================] - 8s 38ms/step - loss: 10.3281 - mse: 10.1554 - val_loss: 22.1086 - val_mse: 21.9360
Epoch 32/150
224/224 [==============================] - 8s 38ms/step - loss: 9.5491 - mse: 9.3771 - val_loss: 19.1791 - val_mse: 19.0072
Epoch 33/150
224/224 [==============================] - 8s 35ms/step - loss: 8.7044 - mse: 8.5329 - val_loss: 19.8548 - val_mse: 19.6835
Epoch 34/150
224/224 [==============================] - 8s 35ms/step - loss: 8.0894 - mse: 7.9184 - val_loss: 20.3238 - val_mse: 20.1524
Epoch 35/150
224/224 [==============================] - 8s 35ms/step - loss: 7.1951 - mse: 7.0247 - val_loss: 17.5049 - val_mse: 17.3345
Epoch 36/150
224/224 [==============================] - 8s 36ms/step - loss: 7.1404 - mse: 6.9704 - val_loss: 17.7885 - val_mse: 17.6183
Epoch 37/150
224/224 [==============================] - 8s 35ms/step - loss: 6.5653 - mse: 6.3958 - val_loss: 19.3679 - val_mse: 19.1984
Epoch 38/150
224/224 [==============================] - 8s 35ms/step - loss: 6.4107 - mse: 6.2417 - val_loss: 17.2135 - val_mse: 17.0444
Epoch 39/150
224/224 [==============================] - 8s 35ms/step - loss: 6.1884 - mse: 6.0199 - val_loss: 16.5921 - val_mse: 16.4238
Epoch 40/150
224/224 [==============================] - 9s 39ms/step - loss: 5.4442 - mse: 5.2759 - val_loss: 14.3250 - val_mse: 14.1564
Epoch 41/150
224/224 [==============================] - 8s 37ms/step - loss: 5.2603 - mse: 5.0921 - val_loss: 17.0472 - val_mse: 16.8791
Epoch 42/150
224/224 [==============================] - 8s 35ms/step - loss: 4.9224 - mse: 4.7544 - val_loss: 14.8110 - val_mse: 14.6435
Epoch 43/150
224/224 [==============================] - 8s 35ms/step - loss: 4.9235 - mse: 4.7560 - val_loss: 18.1798 - val_mse: 18.0118
Epoch 44/150
224/224 [==============================] - 8s 36ms/step - loss: 4.6185 - mse: 4.4513 - val_loss: 15.4470 - val_mse: 15.2796
Epoch 45/150
224/224 [==============================] - 8s 35ms/step - loss: 4.2175 - mse: 4.0505 - val_loss: 15.4949 - val_mse: 15.3278
Epoch 46/150
224/224 [==============================] - 8s 35ms/step - loss: 4.2992 - mse: 4.1323 - val_loss: 14.1135 - val_mse: 13.9465
Epoch 47/150
224/224 [==============================] - 8s 35ms/step - loss: 4.3648 - mse: 4.1981 - val_loss: 15.0115 - val_mse: 14.8443
Epoch 48/150
224/224 [==============================] - 8s 36ms/step - loss: 3.7658 - mse: 3.5993 - val_loss: 13.3013 - val_mse: 13.1348
Epoch 49/150
224/224 [==============================] - 8s 37ms/step - loss: 3.9163 - mse: 3.7499 - val_loss: 14.3093 - val_mse: 14.1428
Epoch 50/150
224/224 [==============================] - 8s 37ms/step - loss: 3.6224 - mse: 3.4561 - val_loss: 15.0223 - val_mse: 14.8557
Epoch 51/150
224/224 [==============================] - 8s 35ms/step - loss: 3.4034 - mse: 3.2373 - val_loss: 13.6526 - val_mse: 13.4867
Epoch 52/150
224/224 [==============================] - 8s 36ms/step - loss: 3.7152 - mse: 3.5497 - val_loss: 13.6527 - val_mse: 13.4867
Epoch 53/150
224/224 [==============================] - 8s 36ms/step - loss: 3.2484 - mse: 3.0832 - val_loss: 15.9726 - val_mse: 15.8067
Epoch 54/150
224/224 [==============================] - 8s 35ms/step - loss: 3.2659 - mse: 3.1010 - val_loss: 15.1650 - val_mse: 15.0005
Epoch 55/150
224/224 [==============================] - 8s 35ms/step - loss: 3.3276 - mse: 3.1630 - val_loss: 15.2027 - val_mse: 15.0377
Epoch 56/150
224/224 [==============================] - 8s 36ms/step - loss: 3.2592 - mse: 3.0948 - val_loss: 13.8045 - val_mse: 13.6397
Epoch 57/150
224/224 [==============================] - 8s 35ms/step - loss: 3.3118 - mse: 3.1474 - val_loss: 13.1203 - val_mse: 12.9557
Epoch 58/150
224/224 [==============================] - 8s 38ms/step - loss: 3.0461 - mse: 2.8820 - val_loss: 13.5162 - val_mse: 13.3521
Epoch 59/150
224/224 [==============================] - 8s 37ms/step - loss: 2.7827 - mse: 2.6187 - val_loss: 13.0337 - val_mse: 12.8702
Epoch 60/150
224/224 [==============================] - 8s 36ms/step - loss: 2.7146 - mse: 2.5513 - val_loss: 13.6579 - val_mse: 13.4941
Epoch 61/150
224/224 [==============================] - 8s 35ms/step - loss: 2.8320 - mse: 2.6688 - val_loss: 12.1502 - val_mse: 11.9869
Epoch 62/150
224/224 [==============================] - 8s 35ms/step - loss: 2.7925 - mse: 2.6295 - val_loss: 12.8631 - val_mse: 12.6999
Epoch 63/150
224/224 [==============================] - 8s 35ms/step - loss: 2.7468 - mse: 2.5842 - val_loss: 12.7770 - val_mse: 12.6141
Epoch 64/150
224/224 [==============================] - 8s 36ms/step - loss: 2.7268 - mse: 2.5644 - val_loss: 13.1902 - val_mse: 13.0281
Epoch 65/150
224/224 [==============================] - 8s 35ms/step - loss: 2.4073 - mse: 2.2452 - val_loss: 12.3041 - val_mse: 12.1420
Epoch 66/150
224/224 [==============================] - 8s 36ms/step - loss: 2.4849 - mse: 2.3231 - val_loss: 12.0085 - val_mse: 11.8470
Epoch 67/150
224/224 [==============================] - 8s 36ms/step - loss: 2.6172 - mse: 2.4557 - val_loss: 12.2735 - val_mse: 12.1115
Epoch 68/150
224/224 [==============================] - 9s 39ms/step - loss: 2.5151 - mse: 2.3537 - val_loss: 12.9417 - val_mse: 12.7807
Epoch 69/150
224/224 [==============================] - 8s 35ms/step - loss: 2.4275 - mse: 2.2666 - val_loss: 11.9198 - val_mse: 11.7590
Epoch 70/150
224/224 [==============================] - 8s 36ms/step - loss: 2.2098 - mse: 2.0492 - val_loss: 11.7847 - val_mse: 11.6242
Epoch 71/150
224/224 [==============================] - 8s 35ms/step - loss: 2.2341 - mse: 2.0740 - val_loss: 12.6661 - val_mse: 12.5060
Epoch 72/150
224/224 [==============================] - 8s 36ms/step - loss: 2.0994 - mse: 1.9397 - val_loss: 12.4913 - val_mse: 12.3318
Epoch 73/150
224/224 [==============================] - 8s 35ms/step - loss: 2.1504 - mse: 1.9912 - val_loss: 13.0752 - val_mse: 12.9161
Epoch 74/150
224/224 [==============================] - 8s 35ms/step - loss: 2.2619 - mse: 2.1029 - val_loss: 12.4034 - val_mse: 12.2440
Epoch 75/150
224/224 [==============================] - 8s 35ms/step - loss: 1.9073 - mse: 1.7486 - val_loss: 11.9525 - val_mse: 11.7942
Epoch 76/150
224/224 [==============================] - 8s 38ms/step - loss: 2.0578 - mse: 1.8997 - val_loss: 13.6006 - val_mse: 13.4427
Epoch 77/150
224/224 [==============================] - 8s 38ms/step - loss: 2.0992 - mse: 1.9414 - val_loss: 11.7928 - val_mse: 11.6350
Epoch 78/150
224/224 [==============================] - 8s 35ms/step - loss: 1.9002 - mse: 1.7430 - val_loss: 11.8577 - val_mse: 11.7004
Epoch 79/150
224/224 [==============================] - 8s 35ms/step - loss: 2.0017 - mse: 1.8449 - val_loss: 12.0755 - val_mse: 11.9184
Epoch 80/150
224/224 [==============================] - 8s 36ms/step - loss: 1.9485 - mse: 1.7919 - val_loss: 14.5831 - val_mse: 14.4261
Epoch 81/150
224/224 [==============================] - 8s 36ms/step - loss: 2.0927 - mse: 1.9365 - val_loss: 12.0999 - val_mse: 11.9441
Epoch 82/150
224/224 [==============================] - 8s 35ms/step - loss: 1.8610 - mse: 1.7054 - val_loss: 11.6716 - val_mse: 11.5159
Epoch 83/150
224/224 [==============================] - 8s 35ms/step - loss: 1.9042 - mse: 1.7489 - val_loss: 13.2938 - val_mse: 13.1384
Epoch 84/150
224/224 [==============================] - 8s 36ms/step - loss: 1.8856 - mse: 1.7307 - val_loss: 11.7056 - val_mse: 11.5505
Epoch 85/150
224/224 [==============================] - 8s 36ms/step - loss: 1.9696 - mse: 1.8150 - val_loss: 12.0150 - val_mse: 11.8603
Epoch 86/150
224/224 [==============================] - 8s 37ms/step - loss: 1.8740 - mse: 1.7197 - val_loss: 10.9867 - val_mse: 10.8324
Epoch 87/150
224/224 [==============================] - 8s 37ms/step - loss: 1.6808 - mse: 1.5270 - val_loss: 11.7345 - val_mse: 11.5810
Epoch 88/150
224/224 [==============================] - 8s 36ms/step - loss: 1.8247 - mse: 1.6714 - val_loss: 11.3200 - val_mse: 11.1666
Epoch 89/150
224/224 [==============================] - 8s 35ms/step - loss: 1.8358 - mse: 1.6826 - val_loss: 12.5673 - val_mse: 12.4144
Epoch 90/150
224/224 [==============================] - 8s 35ms/step - loss: 1.7825 - mse: 1.6297 - val_loss: 12.4812 - val_mse: 12.3287
Epoch 91/150
224/224 [==============================] - 8s 35ms/step - loss: 1.7067 - mse: 1.5544 - val_loss: 11.1053 - val_mse: 10.9532
Epoch 92/150
224/224 [==============================] - 8s 37ms/step - loss: 1.6025 - mse: 1.4507 - val_loss: 11.8117 - val_mse: 11.6597
Epoch 93/150
224/224 [==============================] - 8s 35ms/step - loss: 1.7225 - mse: 1.5710 - val_loss: 12.4930 - val_mse: 12.3412
Epoch 94/150
224/224 [==============================] - 8s 35ms/step - loss: 1.6865 - mse: 1.5353 - val_loss: 11.7851 - val_mse: 11.6341
Epoch 95/150
224/224 [==============================] - 8s 35ms/step - loss: 1.6675 - mse: 1.5167 - val_loss: 11.8057 - val_mse: 11.6548
Epoch 96/150
224/224 [==============================] - 8s 36ms/step - loss: 1.5344 - mse: 1.3842 - val_loss: 11.2643 - val_mse: 11.1138
Epoch 97/150
224/224 [==============================] - 8s 35ms/step - loss: 1.6088 - mse: 1.4590 - val_loss: 12.0023 - val_mse: 11.8530
Epoch 98/150
224/224 [==============================] - 8s 36ms/step - loss: 1.5693 - mse: 1.4200 - val_loss: 11.5944 - val_mse: 11.4453
Epoch 99/150
224/224 [==============================] - 8s 35ms/step - loss: 1.6738 - mse: 1.5248 - val_loss: 14.6320 - val_mse: 14.4829
Epoch 100/150
224/224 [==============================] - 8s 36ms/step - loss: 1.5276 - mse: 1.3790 - val_loss: 11.4561 - val_mse: 11.3079
Epoch 101/150
224/224 [==============================] - 8s 36ms/step - loss: 1.5250 - mse: 1.3767 - val_loss: 11.6983 - val_mse: 11.5500
Epoch 102/150
224/224 [==============================] - 8s 37ms/step - loss: 1.5038 - mse: 1.3561 - val_loss: 12.5257 - val_mse: 12.3778
Epoch 103/150
224/224 [==============================] - 9s 38ms/step - loss: 1.4619 - mse: 1.3145 - val_loss: 12.2082 - val_mse: 12.0607
Epoch 104/150
224/224 [==============================] - 8s 36ms/step - loss: 1.4726 - mse: 1.3258 - val_loss: 13.5575 - val_mse: 13.4107
Epoch 105/150
224/224 [==============================] - 8s 36ms/step - loss: 1.4210 - mse: 1.2747 - val_loss: 11.5041 - val_mse: 11.3582
Epoch 106/150
224/224 [==============================] - 8s 36ms/step - loss: 1.4426 - mse: 1.2967 - val_loss: 11.4231 - val_mse: 11.2772
Epoch 107/150
224/224 [==============================] - 8s 36ms/step - loss: 1.5604 - mse: 1.4150 - val_loss: 11.1475 - val_mse: 11.0020
Epoch 108/150
224/224 [==============================] - 8s 35ms/step - loss: 1.4207 - mse: 1.2757 - val_loss: 11.0739 - val_mse: 10.9290
Epoch 109/150
224/224 [==============================] - 8s 36ms/step - loss: 1.4701 - mse: 1.3255 - val_loss: 12.1092 - val_mse: 11.9646
Epoch 110/150
224/224 [==============================] - 8s 35ms/step - loss: 1.4152 - mse: 1.2710 - val_loss: 12.7533 - val_mse: 12.6094
Epoch 111/150
224/224 [==============================] - 8s 35ms/step - loss: 1.4174 - mse: 1.2737 - val_loss: 11.2918 - val_mse: 11.1483
Epoch 112/150
224/224 [==============================] - 8s 35ms/step - loss: 1.4003 - mse: 1.2571 - val_loss: 11.1652 - val_mse: 11.0220
Epoch 113/150
224/224 [==============================] - 8s 36ms/step - loss: 1.3647 - mse: 1.2219 - val_loss: 11.5964 - val_mse: 11.4539
Epoch 114/150
224/224 [==============================] - 8s 35ms/step - loss: 1.4116 - mse: 1.2693 - val_loss: 11.5399 - val_mse: 11.3977
Epoch 115/150
224/224 [==============================] - 8s 35ms/step - loss: 1.4232 - mse: 1.2811 - val_loss: 12.5575 - val_mse: 12.4162
Epoch 116/150
224/224 [==============================] - 8s 35ms/step - loss: 1.3567 - mse: 1.2151 - val_loss: 10.9896 - val_mse: 10.8481
Epoch 117/150
224/224 [==============================] - 8s 36ms/step - loss: 1.2817 - mse: 1.1406 - val_loss: 11.7565 - val_mse: 11.6150
Epoch 118/150
224/224 [==============================] - 8s 35ms/step - loss: 1.3111 - mse: 1.1704 - val_loss: 11.2981 - val_mse: 11.1572
Epoch 119/150
224/224 [==============================] - 8s 37ms/step - loss: 1.3256 - mse: 1.1853 - val_loss: 11.6880 - val_mse: 11.5479
Epoch 120/150
224/224 [==============================] - 9s 38ms/step - loss: 1.3133 - mse: 1.1735 - val_loss: 12.7000 - val_mse: 12.5605
Epoch 121/150
224/224 [==============================] - 9s 39ms/step - loss: 1.3123 - mse: 1.1729 - val_loss: 11.2099 - val_mse: 11.0710
Epoch 122/150
224/224 [==============================] - 8s 36ms/step - loss: 1.2696 - mse: 1.1308 - val_loss: 11.3391 - val_mse: 11.2004
Epoch 123/150
224/224 [==============================] - 8s 35ms/step - loss: 1.2533 - mse: 1.1146 - val_loss: 11.4851 - val_mse: 11.3464
Epoch 124/150
224/224 [==============================] - 8s 35ms/step - loss: 1.2198 - mse: 1.0816 - val_loss: 10.9624 - val_mse: 10.8247
Epoch 125/150
224/224 [==============================] - 8s 36ms/step - loss: 1.2487 - mse: 1.1109 - val_loss: 11.6655 - val_mse: 11.5277
Epoch 126/150
224/224 [==============================] - 8s 35ms/step - loss: 1.1385 - mse: 1.0012 - val_loss: 11.2689 - val_mse: 11.1318
Epoch 127/150
224/224 [==============================] - 8s 35ms/step - loss: 1.2927 - mse: 1.1557 - val_loss: 11.6734 - val_mse: 11.5367
Epoch 128/150
224/224 [==============================] - 8s 35ms/step - loss: 1.2782 - mse: 1.1417 - val_loss: 11.2034 - val_mse: 11.0673
Epoch 129/150
224/224 [==============================] - 8s 36ms/step - loss: 1.1859 - mse: 1.0500 - val_loss: 11.2717 - val_mse: 11.1359
Epoch 130/150
224/224 [==============================] - 8s 35ms/step - loss: 1.2046 - mse: 1.0692 - val_loss: 11.1139 - val_mse: 10.9783
Epoch 131/150
224/224 [==============================] - 8s 35ms/step - loss: 1.2607 - mse: 1.1255 - val_loss: 11.3920 - val_mse: 11.2569
Epoch 132/150
224/224 [==============================] - 8s 36ms/step - loss: 1.2480 - mse: 1.1131 - val_loss: 11.2044 - val_mse: 11.0699
Epoch 133/150
224/224 [==============================] - 8s 36ms/step - loss: 1.2024 - mse: 1.0679 - val_loss: 11.1364 - val_mse: 11.0022
Epoch 134/150
224/224 [==============================] - 8s 35ms/step - loss: 1.1733 - mse: 1.0392 - val_loss: 11.2258 - val_mse: 11.0918
Epoch 135/150
224/224 [==============================] - 8s 35ms/step - loss: 1.1914 - mse: 1.0577 - val_loss: 11.8171 - val_mse: 11.6834
Epoch 136/150
224/224 [==============================] - 8s 37ms/step - loss: 1.1680 - mse: 1.0348 - val_loss: 10.7287 - val_mse: 10.5955
Epoch 137/150
224/224 [==============================] - 9s 39ms/step - loss: 1.0499 - mse: 0.9171 - val_loss: 11.0255 - val_mse: 10.8930
Epoch 138/150
224/224 [==============================] - 8s 37ms/step - loss: 1.1165 - mse: 0.9842 - val_loss: 10.7928 - val_mse: 10.6606
Epoch 139/150
224/224 [==============================] - 8s 35ms/step - loss: 1.1238 - mse: 0.9919 - val_loss: 11.3765 - val_mse: 11.2445
Epoch 140/150
224/224 [==============================] - 8s 35ms/step - loss: 1.1821 - mse: 1.0502 - val_loss: 11.1137 - val_mse: 10.9820
Epoch 141/150
224/224 [==============================] - 8s 36ms/step - loss: 1.1555 - mse: 1.0241 - val_loss: 11.4838 - val_mse: 11.3524
Epoch 142/150
224/224 [==============================] - 8s 35ms/step - loss: 1.0895 - mse: 0.9584 - val_loss: 10.9140 - val_mse: 10.7830
Epoch 143/150
224/224 [==============================] - 8s 35ms/step - loss: 1.0979 - mse: 0.9674 - val_loss: 10.7904 - val_mse: 10.6601
Epoch 144/150
224/224 [==============================] - 8s 36ms/step - loss: 1.0964 - mse: 0.9662 - val_loss: 11.4072 - val_mse: 11.2765
Epoch 145/150
224/224 [==============================] - 8s 37ms/step - loss: 1.0877 - mse: 0.9580 - val_loss: 10.6479 - val_mse: 10.5180
Epoch 146/150
224/224 [==============================] - 8s 35ms/step - loss: 1.0476 - mse: 0.9184 - val_loss: 11.0730 - val_mse: 10.9440
Epoch 147/150
224/224 [==============================] - 8s 35ms/step - loss: 1.0473 - mse: 0.9184 - val_loss: 10.6226 - val_mse: 10.4938
Epoch 148/150
224/224 [==============================] - 8s 36ms/step - loss: 1.0564 - mse: 0.9283 - val_loss: 11.1643 - val_mse: 11.0358
Epoch 149/150
224/224 [==============================] - 8s 36ms/step - loss: 1.0350 - mse: 0.9073 - val_loss: 11.4436 - val_mse: 11.3160
Epoch 150/150
224/224 [==============================] - 8s 35ms/step - loss: 1.0246 - mse: 0.8972 - val_loss: 11.5455 - val_mse: 11.4183

CV 3
Epoch 1/150
224/224 [==============================] - 18s 71ms/step - loss: 64.0630 - mse: 63.8744 - val_loss: 26.6925 - val_mse: 26.5021
Epoch 2/150
224/224 [==============================] - 9s 38ms/step - loss: 29.4283 - mse: 29.2377 - val_loss: 34.2601 - val_mse: 34.0695
Epoch 3/150
224/224 [==============================] - 8s 38ms/step - loss: 29.4174 - mse: 29.2265 - val_loss: 28.0861 - val_mse: 27.8946
Epoch 4/150
224/224 [==============================] - 8s 36ms/step - loss: 28.4510 - mse: 28.2600 - val_loss: 26.5930 - val_mse: 26.4016
Epoch 5/150
224/224 [==============================] - 8s 36ms/step - loss: 27.8727 - mse: 27.6817 - val_loss: 27.8358 - val_mse: 27.6445
Epoch 6/150
224/224 [==============================] - 8s 35ms/step - loss: 27.9634 - mse: 27.7724 - val_loss: 27.2859 - val_mse: 27.0946
Epoch 7/150
224/224 [==============================] - 8s 35ms/step - loss: 27.6240 - mse: 27.4331 - val_loss: 27.5211 - val_mse: 27.3298
Epoch 8/150
224/224 [==============================] - 8s 35ms/step - loss: 26.3959 - mse: 26.2051 - val_loss: 27.7426 - val_mse: 27.5517
Epoch 9/150
224/224 [==============================] - 8s 36ms/step - loss: 26.6406 - mse: 26.4502 - val_loss: 26.5138 - val_mse: 26.3230
Epoch 10/150
224/224 [==============================] - 8s 36ms/step - loss: 25.9079 - mse: 25.7175 - val_loss: 28.1014 - val_mse: 27.9106
Epoch 11/150
224/224 [==============================] - 8s 35ms/step - loss: 24.3350 - mse: 24.1449 - val_loss: 25.6038 - val_mse: 25.4136
Epoch 12/150
224/224 [==============================] - 8s 35ms/step - loss: 24.6982 - mse: 24.5084 - val_loss: 26.5788 - val_mse: 26.3889
Epoch 13/150
224/224 [==============================] - 8s 36ms/step - loss: 23.1910 - mse: 23.0016 - val_loss: 24.8441 - val_mse: 24.6543
Epoch 14/150
224/224 [==============================] - 8s 35ms/step - loss: 23.6291 - mse: 23.4398 - val_loss: 25.2168 - val_mse: 25.0273
Epoch 15/150
224/224 [==============================] - 8s 35ms/step - loss: 22.2427 - mse: 22.0537 - val_loss: 26.8532 - val_mse: 26.6637
Epoch 16/150
224/224 [==============================] - 8s 35ms/step - loss: 21.1323 - mse: 20.9439 - val_loss: 22.7553 - val_mse: 22.5667
Epoch 17/150
224/224 [==============================] - 8s 36ms/step - loss: 20.0823 - mse: 19.8943 - val_loss: 27.0159 - val_mse: 26.8283
Epoch 18/150
224/224 [==============================] - 8s 35ms/step - loss: 19.6289 - mse: 19.4414 - val_loss: 25.6060 - val_mse: 25.4181
Epoch 19/150
224/224 [==============================] - 8s 37ms/step - loss: 18.8640 - mse: 18.6768 - val_loss: 23.9811 - val_mse: 23.7938
Epoch 20/150
224/224 [==============================] - 8s 37ms/step - loss: 17.5019 - mse: 17.3152 - val_loss: 23.4232 - val_mse: 23.2359
Epoch 21/150
224/224 [==============================] - 9s 38ms/step - loss: 16.7936 - mse: 16.6070 - val_loss: 21.8684 - val_mse: 21.6816
Epoch 22/150
224/224 [==============================] - 8s 35ms/step - loss: 15.4916 - mse: 15.3054 - val_loss: 24.5574 - val_mse: 24.3707
Epoch 23/150
224/224 [==============================] - 8s 35ms/step - loss: 15.1660 - mse: 14.9803 - val_loss: 24.3920 - val_mse: 24.2063
Epoch 24/150
224/224 [==============================] - 8s 35ms/step - loss: 13.5094 - mse: 13.3240 - val_loss: 20.6964 - val_mse: 20.5112
Epoch 25/150
224/224 [==============================] - 8s 36ms/step - loss: 12.5674 - mse: 12.3825 - val_loss: 22.1065 - val_mse: 21.9214
Epoch 26/150
224/224 [==============================] - 8s 36ms/step - loss: 12.0910 - mse: 11.9066 - val_loss: 23.7337 - val_mse: 23.5489
Epoch 27/150
224/224 [==============================] - 8s 35ms/step - loss: 10.8528 - mse: 10.6691 - val_loss: 24.0450 - val_mse: 23.8611
Epoch 28/150
224/224 [==============================] - 8s 35ms/step - loss: 10.2645 - mse: 10.0814 - val_loss: 20.7816 - val_mse: 20.5984
Epoch 29/150
224/224 [==============================] - 8s 36ms/step - loss: 9.6208 - mse: 9.4381 - val_loss: 19.1426 - val_mse: 18.9602
Epoch 30/150
224/224 [==============================] - 8s 36ms/step - loss: 8.8887 - mse: 8.7066 - val_loss: 17.4184 - val_mse: 17.2360
Epoch 31/150
224/224 [==============================] - 8s 35ms/step - loss: 8.4120 - mse: 8.2304 - val_loss: 18.7839 - val_mse: 18.6022
Epoch 32/150
224/224 [==============================] - 8s 35ms/step - loss: 7.5865 - mse: 7.4057 - val_loss: 18.3398 - val_mse: 18.1587
Epoch 33/150
224/224 [==============================] - 8s 36ms/step - loss: 7.2515 - mse: 7.0711 - val_loss: 20.1611 - val_mse: 19.9804
Epoch 34/150
224/224 [==============================] - 8s 35ms/step - loss: 6.7897 - mse: 6.6100 - val_loss: 18.9088 - val_mse: 18.7289
Epoch 35/150
224/224 [==============================] - 8s 35ms/step - loss: 6.6665 - mse: 6.4874 - val_loss: 16.1534 - val_mse: 15.9744
Epoch 36/150
224/224 [==============================] - 8s 36ms/step - loss: 6.0128 - mse: 5.8343 - val_loss: 15.9078 - val_mse: 15.7294
Epoch 37/150
224/224 [==============================] - 9s 39ms/step - loss: 5.7833 - mse: 5.6053 - val_loss: 15.5125 - val_mse: 15.3348
Epoch 38/150
224/224 [==============================] - 9s 38ms/step - loss: 5.4244 - mse: 5.2471 - val_loss: 17.7890 - val_mse: 17.6120
Epoch 39/150
224/224 [==============================] - 8s 35ms/step - loss: 5.4565 - mse: 5.2794 - val_loss: 17.1997 - val_mse: 17.0227
Epoch 40/150
224/224 [==============================] - 8s 36ms/step - loss: 4.9225 - mse: 4.7460 - val_loss: 15.0919 - val_mse: 14.9153
Epoch 41/150
224/224 [==============================] - 8s 35ms/step - loss: 4.9830 - mse: 4.8072 - val_loss: 15.4808 - val_mse: 15.3051
Epoch 42/150
224/224 [==============================] - 8s 36ms/step - loss: 4.4458 - mse: 4.2707 - val_loss: 15.5269 - val_mse: 15.3520
Epoch 43/150
224/224 [==============================] - 8s 36ms/step - loss: 4.2957 - mse: 4.1210 - val_loss: 15.9580 - val_mse: 15.7833
Epoch 44/150
224/224 [==============================] - 8s 35ms/step - loss: 4.0142 - mse: 3.8400 - val_loss: 15.2207 - val_mse: 15.0463
Epoch 45/150
224/224 [==============================] - 8s 36ms/step - loss: 4.3365 - mse: 4.1628 - val_loss: 15.8201 - val_mse: 15.6467
Epoch 46/150
224/224 [==============================] - 8s 36ms/step - loss: 3.8578 - mse: 3.6847 - val_loss: 15.2412 - val_mse: 15.0683
Epoch 47/150
224/224 [==============================] - 8s 35ms/step - loss: 3.7620 - mse: 3.5895 - val_loss: 14.1555 - val_mse: 13.9830
Epoch 48/150
224/224 [==============================] - 8s 35ms/step - loss: 3.8934 - mse: 3.7213 - val_loss: 15.6474 - val_mse: 15.4752
Epoch 49/150
224/224 [==============================] - 8s 35ms/step - loss: 3.4903 - mse: 3.3185 - val_loss: 15.4174 - val_mse: 15.2458
Epoch 50/150
224/224 [==============================] - 8s 36ms/step - loss: 3.4422 - mse: 3.2710 - val_loss: 15.7668 - val_mse: 15.5958
Epoch 51/150
224/224 [==============================] - 8s 35ms/step - loss: 3.3569 - mse: 3.1860 - val_loss: 16.0617 - val_mse: 15.8912
Epoch 52/150
224/224 [==============================] - 8s 36ms/step - loss: 3.0938 - mse: 2.9237 - val_loss: 13.7458 - val_mse: 13.5757
Epoch 53/150
224/224 [==============================] - 8s 35ms/step - loss: 3.1558 - mse: 2.9861 - val_loss: 14.4210 - val_mse: 14.2512
Epoch 54/150
224/224 [==============================] - 9s 39ms/step - loss: 3.0781 - mse: 2.9087 - val_loss: 14.7467 - val_mse: 14.5774
Epoch 55/150
224/224 [==============================] - 8s 38ms/step - loss: 3.0496 - mse: 2.8805 - val_loss: 14.8904 - val_mse: 14.7213
Epoch 56/150
224/224 [==============================] - 8s 37ms/step - loss: 3.0996 - mse: 2.9307 - val_loss: 15.6446 - val_mse: 15.4757
Epoch 57/150
224/224 [==============================] - 8s 35ms/step - loss: 2.9118 - mse: 2.7434 - val_loss: 13.1821 - val_mse: 13.0135
Epoch 58/150
224/224 [==============================] - 8s 36ms/step - loss: 2.8906 - mse: 2.7224 - val_loss: 12.9174 - val_mse: 12.7497
Epoch 59/150
224/224 [==============================] - 8s 35ms/step - loss: 2.6946 - mse: 2.5268 - val_loss: 12.3487 - val_mse: 12.1810
Epoch 60/150
224/224 [==============================] - 8s 35ms/step - loss: 2.5319 - mse: 2.3644 - val_loss: 12.1731 - val_mse: 12.0057
Epoch 61/150
224/224 [==============================] - 8s 35ms/step - loss: 2.6043 - mse: 2.4372 - val_loss: 15.4212 - val_mse: 15.2545
Epoch 62/150
224/224 [==============================] - 8s 37ms/step - loss: 2.5601 - mse: 2.3935 - val_loss: 12.8057 - val_mse: 12.6388
Epoch 63/150
224/224 [==============================] - 8s 35ms/step - loss: 2.3505 - mse: 2.1841 - val_loss: 13.0155 - val_mse: 12.8490
Epoch 64/150
224/224 [==============================] - 8s 35ms/step - loss: 2.3990 - mse: 2.2330 - val_loss: 12.7303 - val_mse: 12.5643
Epoch 65/150
224/224 [==============================] - 8s 35ms/step - loss: 2.4691 - mse: 2.3035 - val_loss: 12.3747 - val_mse: 12.2093
Epoch 66/150
224/224 [==============================] - 8s 36ms/step - loss: 2.4088 - mse: 2.2437 - val_loss: 12.0684 - val_mse: 11.9034
Epoch 67/150
224/224 [==============================] - 8s 36ms/step - loss: 2.4236 - mse: 2.2590 - val_loss: 12.8413 - val_mse: 12.6765
Epoch 68/150
224/224 [==============================] - 8s 35ms/step - loss: 2.2897 - mse: 2.1253 - val_loss: 13.4463 - val_mse: 13.2819
Epoch 69/150
224/224 [==============================] - 8s 36ms/step - loss: 2.3749 - mse: 2.2110 - val_loss: 13.5427 - val_mse: 13.3785
Epoch 70/150
224/224 [==============================] - 8s 37ms/step - loss: 2.2597 - mse: 2.0960 - val_loss: 12.5643 - val_mse: 12.4007
Epoch 71/150
224/224 [==============================] - 8s 36ms/step - loss: 2.3428 - mse: 2.1794 - val_loss: 13.7561 - val_mse: 13.5924
Epoch 72/150
224/224 [==============================] - 8s 37ms/step - loss: 2.1722 - mse: 2.0090 - val_loss: 11.5905 - val_mse: 11.4273
Epoch 73/150
224/224 [==============================] - 8s 38ms/step - loss: 2.1391 - mse: 1.9763 - val_loss: 11.9685 - val_mse: 11.8059
Epoch 74/150
224/224 [==============================] - 8s 37ms/step - loss: 2.0919 - mse: 1.9297 - val_loss: 12.5889 - val_mse: 12.4265
Epoch 75/150
224/224 [==============================] - 8s 35ms/step - loss: 2.1351 - mse: 1.9733 - val_loss: 14.2824 - val_mse: 14.1204
Epoch 76/150
224/224 [==============================] - 8s 35ms/step - loss: 2.1083 - mse: 1.9469 - val_loss: 13.1298 - val_mse: 12.9677
Epoch 77/150
224/224 [==============================] - 8s 35ms/step - loss: 1.8941 - mse: 1.7329 - val_loss: 11.3037 - val_mse: 11.1425
Epoch 78/150
224/224 [==============================] - 8s 36ms/step - loss: 1.9437 - mse: 1.7829 - val_loss: 11.3940 - val_mse: 11.2332
Epoch 79/150
224/224 [==============================] - 8s 35ms/step - loss: 1.9223 - mse: 1.7620 - val_loss: 11.9912 - val_mse: 11.8307
Epoch 80/150
224/224 [==============================] - 8s 35ms/step - loss: 1.9754 - mse: 1.8155 - val_loss: 11.2821 - val_mse: 11.1221
Epoch 81/150
224/224 [==============================] - 8s 36ms/step - loss: 1.8252 - mse: 1.6657 - val_loss: 10.9588 - val_mse: 10.7997
Epoch 82/150
224/224 [==============================] - 8s 35ms/step - loss: 1.9253 - mse: 1.7664 - val_loss: 11.1801 - val_mse: 11.0211
Epoch 83/150
224/224 [==============================] - 8s 36ms/step - loss: 1.8684 - mse: 1.7099 - val_loss: 12.1733 - val_mse: 12.0147
Epoch 84/150
224/224 [==============================] - 8s 35ms/step - loss: 1.9635 - mse: 1.8054 - val_loss: 11.2523 - val_mse: 11.0945
Epoch 85/150
224/224 [==============================] - 8s 35ms/step - loss: 1.7985 - mse: 1.6408 - val_loss: 11.2183 - val_mse: 11.0607
Epoch 86/150
224/224 [==============================] - 8s 35ms/step - loss: 1.7919 - mse: 1.6345 - val_loss: 11.7500 - val_mse: 11.5930
Epoch 87/150
224/224 [==============================] - 8s 36ms/step - loss: 1.7628 - mse: 1.6060 - val_loss: 12.0103 - val_mse: 11.8535
Epoch 88/150
224/224 [==============================] - 8s 35ms/step - loss: 1.6884 - mse: 1.5320 - val_loss: 11.5823 - val_mse: 11.4260
Epoch 89/150
224/224 [==============================] - 8s 37ms/step - loss: 1.7441 - mse: 1.5880 - val_loss: 12.1187 - val_mse: 11.9621
Epoch 90/150
224/224 [==============================] - 8s 38ms/step - loss: 1.5883 - mse: 1.4325 - val_loss: 10.9943 - val_mse: 10.8388
Epoch 91/150
224/224 [==============================] - 9s 38ms/step - loss: 1.6915 - mse: 1.5364 - val_loss: 11.4551 - val_mse: 11.3000
Epoch 92/150
224/224 [==============================] - 8s 37ms/step - loss: 1.7625 - mse: 1.6077 - val_loss: 11.2858 - val_mse: 11.1313
Epoch 93/150
224/224 [==============================] - 8s 36ms/step - loss: 1.6273 - mse: 1.4731 - val_loss: 11.4359 - val_mse: 11.2818
Epoch 94/150
224/224 [==============================] - 8s 35ms/step - loss: 1.5213 - mse: 1.3675 - val_loss: 11.1534 - val_mse: 10.9998
Epoch 95/150
224/224 [==============================] - 8s 36ms/step - loss: 1.6361 - mse: 1.4826 - val_loss: 11.3452 - val_mse: 11.1915
Epoch 96/150
224/224 [==============================] - 8s 35ms/step - loss: 1.5515 - mse: 1.3982 - val_loss: 11.1500 - val_mse: 10.9971
Epoch 97/150
224/224 [==============================] - 8s 35ms/step - loss: 1.6958 - mse: 1.5428 - val_loss: 12.1573 - val_mse: 12.0046
Epoch 98/150
224/224 [==============================] - 8s 35ms/step - loss: 1.5670 - mse: 1.4146 - val_loss: 10.9905 - val_mse: 10.8385
Epoch 99/150
224/224 [==============================] - 8s 36ms/step - loss: 1.5122 - mse: 1.3602 - val_loss: 11.3538 - val_mse: 11.2017
Epoch 100/150
224/224 [==============================] - 8s 36ms/step - loss: 1.6371 - mse: 1.4853 - val_loss: 11.2140 - val_mse: 11.0620
Epoch 101/150
224/224 [==============================] - 8s 36ms/step - loss: 1.5231 - mse: 1.3717 - val_loss: 14.4119 - val_mse: 14.2602
Epoch 102/150
224/224 [==============================] - 8s 35ms/step - loss: 1.6083 - mse: 1.4572 - val_loss: 11.3764 - val_mse: 11.2251
Epoch 103/150
224/224 [==============================] - 8s 36ms/step - loss: 1.5144 - mse: 1.3636 - val_loss: 11.2236 - val_mse: 11.0731
Epoch 104/150
224/224 [==============================] - 8s 36ms/step - loss: 1.5400 - mse: 1.3896 - val_loss: 11.4890 - val_mse: 11.3387
Epoch 105/150
224/224 [==============================] - 8s 35ms/step - loss: 1.4505 - mse: 1.3006 - val_loss: 11.2601 - val_mse: 11.1102
Epoch 106/150
224/224 [==============================] - 8s 35ms/step - loss: 1.5095 - mse: 1.3599 - val_loss: 11.1270 - val_mse: 10.9777
Epoch 107/150
224/224 [==============================] - 8s 37ms/step - loss: 1.5124 - mse: 1.3630 - val_loss: 11.1053 - val_mse: 10.9561
Epoch 108/150
224/224 [==============================] - 8s 38ms/step - loss: 1.5025 - mse: 1.3535 - val_loss: 12.2559 - val_mse: 12.1069
Epoch 109/150
224/224 [==============================] - 8s 38ms/step - loss: 1.4327 - mse: 1.2839 - val_loss: 10.8802 - val_mse: 10.7319
Epoch 110/150
224/224 [==============================] - 8s 37ms/step - loss: 1.4092 - mse: 1.2609 - val_loss: 11.0747 - val_mse: 10.9264
Epoch 111/150
224/224 [==============================] - 8s 36ms/step - loss: 1.5996 - mse: 1.4516 - val_loss: 11.4316 - val_mse: 11.2834
Epoch 112/150
224/224 [==============================] - 8s 36ms/step - loss: 1.3918 - mse: 1.2442 - val_loss: 10.8782 - val_mse: 10.7307
Epoch 113/150
224/224 [==============================] - 8s 35ms/step - loss: 1.3540 - mse: 1.2068 - val_loss: 11.4977 - val_mse: 11.3506
Epoch 114/150
224/224 [==============================] - 8s 35ms/step - loss: 1.3172 - mse: 1.1705 - val_loss: 10.8232 - val_mse: 10.6767
Epoch 115/150
224/224 [==============================] - 8s 36ms/step - loss: 1.3692 - mse: 1.2228 - val_loss: 11.7551 - val_mse: 11.6090
Epoch 116/150
224/224 [==============================] - 8s 35ms/step - loss: 1.3928 - mse: 1.2468 - val_loss: 12.2676 - val_mse: 12.1218
Epoch 117/150
224/224 [==============================] - 8s 35ms/step - loss: 1.3450 - mse: 1.1995 - val_loss: 11.1970 - val_mse: 11.0514
Epoch 118/150
224/224 [==============================] - 8s 35ms/step - loss: 1.2837 - mse: 1.1385 - val_loss: 10.5228 - val_mse: 10.3775
Epoch 119/150
224/224 [==============================] - 8s 36ms/step - loss: 1.3284 - mse: 1.1836 - val_loss: 11.2209 - val_mse: 11.0758
Epoch 120/150
224/224 [==============================] - 8s 36ms/step - loss: 1.2486 - mse: 1.1041 - val_loss: 12.6120 - val_mse: 12.4678
Epoch 121/150
224/224 [==============================] - 8s 35ms/step - loss: 1.2956 - mse: 1.1513 - val_loss: 10.9564 - val_mse: 10.8120
Epoch 122/150
224/224 [==============================] - 8s 35ms/step - loss: 1.2670 - mse: 1.1231 - val_loss: 10.9909 - val_mse: 10.8475
Epoch 123/150
224/224 [==============================] - 8s 36ms/step - loss: 1.3466 - mse: 1.2031 - val_loss: 12.3124 - val_mse: 12.1688
Epoch 124/150
224/224 [==============================] - 8s 36ms/step - loss: 1.2514 - mse: 1.1084 - val_loss: 11.5158 - val_mse: 11.3726
Epoch 125/150
224/224 [==============================] - 8s 37ms/step - loss: 1.3053 - mse: 1.1625 - val_loss: 12.8301 - val_mse: 12.6878
Epoch 126/150
224/224 [==============================] - 8s 38ms/step - loss: 1.3052 - mse: 1.1628 - val_loss: 10.9073 - val_mse: 10.7649
Epoch 127/150
224/224 [==============================] - 9s 39ms/step - loss: 1.2388 - mse: 1.0966 - val_loss: 11.4785 - val_mse: 11.3366
Epoch 128/150
224/224 [==============================] - 8s 37ms/step - loss: 1.2563 - mse: 1.1145 - val_loss: 10.6045 - val_mse: 10.4630
Epoch 129/150
224/224 [==============================] - 8s 35ms/step - loss: 1.2183 - mse: 1.0769 - val_loss: 10.6817 - val_mse: 10.5405
Epoch 130/150
224/224 [==============================] - 8s 35ms/step - loss: 1.1939 - mse: 1.0530 - val_loss: 12.4597 - val_mse: 12.3189
Epoch 131/150
224/224 [==============================] - 8s 36ms/step - loss: 1.1827 - mse: 1.0422 - val_loss: 10.6724 - val_mse: 10.5316
Epoch 132/150
224/224 [==============================] - 8s 36ms/step - loss: 1.2082 - mse: 1.0679 - val_loss: 10.5794 - val_mse: 10.4394
Epoch 133/150
224/224 [==============================] - 8s 35ms/step - loss: 1.2416 - mse: 1.1018 - val_loss: 11.2244 - val_mse: 11.0844
Epoch 134/150
224/224 [==============================] - 8s 35ms/step - loss: 1.1432 - mse: 1.0036 - val_loss: 10.7517 - val_mse: 10.6118
Epoch 135/150
224/224 [==============================] - 8s 36ms/step - loss: 1.2332 - mse: 1.0941 - val_loss: 11.1508 - val_mse: 11.0118
Epoch 136/150
224/224 [==============================] - 8s 36ms/step - loss: 1.2319 - mse: 1.0931 - val_loss: 10.5052 - val_mse: 10.3665
Epoch 137/150
224/224 [==============================] - 8s 35ms/step - loss: 1.1782 - mse: 1.0399 - val_loss: 11.5313 - val_mse: 11.3930
Epoch 138/150
224/224 [==============================] - 8s 35ms/step - loss: 1.1512 - mse: 1.0132 - val_loss: 10.8653 - val_mse: 10.7272
Epoch 139/150
224/224 [==============================] - 8s 35ms/step - loss: 1.2196 - mse: 1.0818 - val_loss: 10.6396 - val_mse: 10.5019
Epoch 140/150
224/224 [==============================] - 8s 36ms/step - loss: 1.1287 - mse: 0.9913 - val_loss: 11.1701 - val_mse: 11.0326
Epoch 141/150
224/224 [==============================] - 8s 35ms/step - loss: 1.2792 - mse: 1.1420 - val_loss: 11.4076 - val_mse: 11.2706
Epoch 142/150
224/224 [==============================] - 8s 35ms/step - loss: 1.0997 - mse: 0.9629 - val_loss: 11.0357 - val_mse: 10.8989
Epoch 143/150
224/224 [==============================] - 8s 36ms/step - loss: 1.1597 - mse: 1.0233 - val_loss: 11.1121 - val_mse: 10.9759
Epoch 144/150
224/224 [==============================] - 9s 39ms/step - loss: 1.2361 - mse: 1.0999 - val_loss: 11.1620 - val_mse: 11.0257
Epoch 145/150
224/224 [==============================] - 9s 38ms/step - loss: 1.0967 - mse: 0.9609 - val_loss: 11.5128 - val_mse: 11.3772
Epoch 146/150
224/224 [==============================] - 9s 38ms/step - loss: 1.1463 - mse: 1.0109 - val_loss: 11.3373 - val_mse: 11.2021
Epoch 147/150
224/224 [==============================] - 8s 36ms/step - loss: 1.1424 - mse: 1.0073 - val_loss: 10.6591 - val_mse: 10.5244
Epoch 148/150
224/224 [==============================] - 8s 36ms/step - loss: 1.0930 - mse: 0.9583 - val_loss: 11.1429 - val_mse: 11.0083
Epoch 149/150
224/224 [==============================] - 8s 36ms/step - loss: 1.1404 - mse: 1.0060 - val_loss: 10.8703 - val_mse: 10.7360
Epoch 150/150
224/224 [==============================] - 8s 36ms/step - loss: 1.1294 - mse: 0.9954 - val_loss: 10.6106 - val_mse: 10.4765
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&quot;evaluasi-model-1&quot;&gt;Evaluasi Model&lt;/h2&gt;&lt;p&gt;Melihat kualitas model pada setiap &lt;code&gt;split&lt;/code&gt; terhadap data validasi dengan &lt;code&gt;MSE&lt;/code&gt;-nya&lt;/p&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;for i in range(split):
    print(f&amp;quot;Split {i + 1} : {mse_scores[i]} mse&amp;quot;)

print(&amp;quot;\nMean MSE : &amp;quot;, sum(mse_scores) / split)
&lt;/code&gt;&lt;/pre&gt;&lt;pre&gt;&lt;code&gt;Split 1 : 11.828768063097979 mse
Split 2 : 10.493811792385397 mse
Split 3 : 10.366531025167129 mse

Mean MSE :  10.896370293550168
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&quot;membuat-submission-1&quot;&gt;Membuat Submission&lt;/h2&gt;&lt;p&gt;Membuat submission&lt;/p&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;submission = pd.DataFrame({&amp;#x27;id&amp;#x27; :[x.split(&amp;#x27;/&amp;#x27;)[-1].split(&amp;#x27;.&amp;#x27;)[0] for x in new_test_images],
                           &amp;#x27;usia&amp;#x27;: np.round(prediksi.flatten() / split).astype(int)})
test = test.merge(submission, on=&amp;quot;id&amp;quot;)
test.head()
&lt;/code&gt;&lt;/pre&gt;&lt;table&gt;&lt;thead&gt;&lt;tr&gt;&lt;th align=&quot;center&quot;&gt;id&lt;/th&gt;&lt;th align=&quot;center&quot;&gt;usia&lt;/th&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td align=&quot;center&quot;&gt;005093b2-8c4b-4ed7-91c3-f5f4d50f8d27&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;25&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;center&quot;&gt;0052554e-069e-4c43-beb0-0885e8f7684e&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;31&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;center&quot;&gt;0092b954-1143-4a95-a17b-1edfa6af3b01&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;28&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;center&quot;&gt;009fc28b-fe9b-441d-b8a2-ea8b7ae6ca16&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;25&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;center&quot;&gt;00d0e306-06fe-45d8-ae6c-6f83ab8f7810&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;28&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;&lt;strong&gt;Melihat distribusi hasil prediksi&lt;/strong&gt;&lt;/p&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;plt.figure(figsize=(10, 5))
sns.countplot(x=test[&amp;quot;usia&amp;quot;])
plt.xlabel(&amp;quot;Umur&amp;quot;, fontsize=16)
plt.show()
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;span class=&quot;gatsby-resp-image-wrapper&quot; style=&quot;position:relative;display:block;margin-left:auto;margin-right:auto;max-width:612px&quot;&gt;
      &lt;a class=&quot;gatsby-resp-image-link&quot; href=&quot;/static/9cbb01fad9058dbd81f0a26166f2ea58/8c76f/age_output_46_0.png&quot; style=&quot;display:block&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;
    &lt;span class=&quot;gatsby-resp-image-background-image&quot; style=&quot;padding-bottom:52.53164556962025%;position:relative;bottom:0;left:0;background-image:url(&amp;#x27;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAALCAYAAAB/Ca1DAAAACXBIWXMAAAsTAAALEwEAmpwYAAACQ0lEQVQoz41RXU8TQRSdJ3+Zf4LGxIgFlUQLStcm+AXx0cQQaJHy0T6ItqQEGmlLlwpmCxiqfBRcFHBx28UKSR8UukM7nb3X7NAilph4kpOZyb333HPvEFVVmyilGdM035mmqdRZplTJFfeV5O575ah0rFCTKufjDZynlK4Ui8Ueks/nPdgACyxx7h0dopTux/8FpTRKdF2/CwAIAAwAuGVZHBG5UVT58HIv9yz4eebHHi9Xq9yO/4MVW7BUKoWJpmmS/TgNIHCrKrpltGnsnHLg/cVRvBIbQlqt1NzDBWcAIIpM0xy3R74Hp0kcwBKXnd0kht52Ys+sC93pIWxJBnH88wYWjo/qAme8IKjremetEbcsLjI+rAbwxWQT9qQ60J32C0FnYhJXDwr1xHPCDYKGYQiHYHF7d7CzHoK51GMIxFuhW3aBO+2HlpkgtKdiMJ/T7RnAhgV/4Y+gpml1h2VEZAsxN4tF21hw5hbrlu8wd9rPmhMB1qUkmTM+y2b3DGbnIULtRAYIJzXB1ySXNx5AlWH5MIfln99xMSZhYtqFwcRN7JZdYuTmRAC7FBlb4zJKc2sY2TrAXyWOnAPaWxKrB/HLCbKylr28v7U5ovqf9C2NSb6pYUf/q5c3BvpCV73usNPXFn024Aj1em9HxgavhSPe6+HkgBTJ+AJvdrzqet6X2y70G1/y3gO9MPrtq+YkiCjI5RFS2RgkH+Nt5NPGUzKx1E76Nh+Rh9kxESeEXOpYXCaebIE83zwh6W08qz3P33iV54pwwrp7AAAAAElFTkSuQmCC&amp;#x27;);background-size:cover;display:block&quot;&gt;&lt;/span&gt;
  &lt;img class=&quot;gatsby-resp-image-image&quot; alt=&quot;png&quot; title=&quot;png&quot; src=&quot;/static/9cbb01fad9058dbd81f0a26166f2ea58/8c76f/age_output_46_0.png&quot; srcSet=&quot;/static/9cbb01fad9058dbd81f0a26166f2ea58/c26ae/age_output_46_0.png 158w,/static/9cbb01fad9058dbd81f0a26166f2ea58/6bdcf/age_output_46_0.png 315w,/static/9cbb01fad9058dbd81f0a26166f2ea58/8c76f/age_output_46_0.png 612w&quot; sizes=&quot;(max-width: 612px) 100vw, 612px&quot; style=&quot;width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0&quot; loading=&quot;lazy&quot; decoding=&quot;async&quot;/&gt;
  &lt;/a&gt;
    &lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Export csv&lt;/strong&gt;&lt;/p&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;test.to_csv(&amp;quot;submission-age.csv&amp;quot;, index=False)
&lt;/code&gt;&lt;/pre&gt;&lt;h1 id=&quot;kesimpulan&quot;&gt;Kesimpulan&lt;/h1&gt;&lt;p&gt;Berikut adalah beberapa kesimpulan yang kami dapat dalam menyelesaikan tantangan yang diberikan:&lt;/p&gt;&lt;ol&gt;&lt;li&gt;&lt;p&gt;Proses &lt;em&gt;feature selection&lt;/em&gt; untuk mengambil hanya bagian wajah dari gambar merupakan hal
terpenting untuk membuat model lebih terpusat kearah wajah untuk mengklasifikasi wajah dan
mendeteksi umur.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;em&gt;Filterisasi&lt;/em&gt; secara manual sangat efektif untuk menyaring &lt;em&gt;noise&lt;/em&gt; (data yang tidak baik),
mengingat&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;garbage in, garbage out&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;data yang buruk (kualitas rendah) akan memberikan hasil yang kurang baik juga. Namun sepertinya
para juri kurang setuju untuk tahapan ini karena dilakukan secara manual mengingat
pengaplikasiannya pada &lt;em&gt;Big Data&lt;/em&gt;.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;em&gt;Augmentasi&lt;/em&gt; memberikan kesempatan bagi model untuk belajar lebih banyak.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;em&gt;Ensembel&lt;/em&gt; memberikan hasil score yang lebih stabil dibandingan menggunakan model tunggal
pada peramalan data test.&lt;/p&gt;&lt;/li&gt;&lt;/ol&gt;</content:encoded></item><item><title><![CDATA[Digit Recognizer]]></title><description><![CDATA[MNIST Digit Recognizer using Tensorflow.js]]></description><link>https://Hyuto.github.io/showcase/digit-recognizer</link><guid isPermaLink="false">https://Hyuto.github.io/showcase/digit-recognizer</guid><pubDate>Sun, 10 Jan 2021 03:00:00 GMT</pubDate><content:encoded>
                        &lt;div&gt;
                          &lt;h2&gt;Digit Recognizer&lt;/h2&gt;
                          &lt;p&gt;MNIST Digit Recognizer using Tensorflow.js&lt;/p&gt;
                        &lt;/div&gt;
                      </content:encoded></item><item><title><![CDATA[Hoax Classification - BDC Satria Data 2020]]></title><description><![CDATA[
 Beberapa bulan yang lalu penulis dan teman - teman yang tergabung dalam tim  Catatan Cakrawala 
mengikuti lomba  Big Data Competition  diâ€¦]]></description><link>https://Hyuto.github.io/blog/bdc-satria-data-2020/</link><guid isPermaLink="false">https://Hyuto.github.io/blog/bdc-satria-data-2020/</guid><pubDate>Fri, 16 Oct 2020 17:28:47 GMT</pubDate><content:encoded>&lt;p&gt;&lt;a href=&quot;https://github.com/Hyuto/BDC-Satria-Data&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/Github-black?logo=github&quot; alt=&quot;Github&quot;/&gt;&lt;/a&gt;
&lt;a href=&quot;https://www.kaggle.com/wahyusetianto/bdc-main-notebook&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/Kaggle-blue?logo=kaggle&quot; alt=&quot;kaggle&quot;/&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;Beberapa bulan yang lalu penulis dan teman - teman yang tergabung dalam tim &lt;strong&gt;Catatan Cakrawala&lt;/strong&gt;
mengikuti lomba &lt;code&gt;Big Data Competition&lt;/code&gt; di acara &lt;code&gt;Satria Data 2020&lt;/code&gt; yang diselenggarakan oleh &lt;em&gt;PUSPRESNAS&lt;/em&gt;
yang bekerjasama dengan &lt;em&gt;IPB University&lt;/em&gt;. &lt;em&gt;Alhamdulillah&lt;/em&gt; kami mendapatkan posisi kedua dalam lomba
tersebut. Dapat berpartisipasi bahkan dapat menjadi juara adalah pengalaman yang sangat berkesan bagi
penulis, walaupun belum dapat keluar sebagai juara utama penulis mendapatkan banyak pembelajaran dan
pengalaman yang berkesan.&lt;/p&gt;&lt;p&gt;Berikut ini adalah kode dan langkah - langkah yang kelompok &lt;strong&gt;Catatan Cakrawala&lt;/strong&gt; gunakan pada saat
mengikuti lomba &lt;code&gt;BDC Satria Data 2020&lt;/code&gt;.&lt;/p&gt;&lt;h2 id=&quot;author&quot;&gt;Author&lt;/h2&gt;&lt;ul&gt;&lt;li&gt;&lt;a href=&quot;https://www.linkedin.com/in/dimas-k-jati/&quot;&gt;Dimas Kuncoro Jati&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href=&quot;https://www.linkedin.com/in/muhammadamanda/&quot;&gt;Muhammad Amanda&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href=&quot;https://www.linkedin.com/in/wahyu-setianto/&quot;&gt;Wahyu Setianto&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;h2 id=&quot;running-environment&quot;&gt;Running Environment&lt;/h2&gt;&lt;ul&gt;&lt;li&gt;Platform : Kaggle&lt;/li&gt;&lt;li&gt;Accelerator : TPU&lt;/li&gt;&lt;/ul&gt;&lt;table&gt;&lt;thead&gt;&lt;tr&gt;&lt;th align=&quot;left&quot;&gt;Data&lt;/th&gt;&lt;th align=&quot;left&quot;&gt;Keterangan&lt;/th&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://www.kaggle.com/wahyusetianto/data-bdc&quot;&gt;BDC - Satria Data Catatan Cakrawala&lt;/a&gt;&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;Data original dan data yang telah di preprocess oleh tim Catatan Cakrawala&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://www.kaggle.com/pencarikebahagiaan/modelku&quot;&gt;Catatan Cakrawala Model&lt;/a&gt; [Optional]&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;Model terbaik yang telah di train tim Catatan Cakrawala&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;h2 id=&quot;overview&quot;&gt;Overview&lt;/h2&gt;&lt;p&gt;Melakukan pendeteksian apakah suatu berita adalah hoax atau bukan hoax (&lt;em&gt;binary classification&lt;/em&gt;).
Data merupakan berita yang di crawl dari website &lt;a href=&quot;http://turnbackhoax.id/&quot;&gt;turnbackhoax.id&lt;/a&gt; yang
berupa teks dan juga gambar. Data kemudian dipreproses dan di&lt;em&gt;fit&lt;/em&gt; ke model. Model yang digunakan
untuk data teks adalah &lt;em&gt;BERT&lt;/em&gt; dan untuk data gambar digunakan model &lt;em&gt;EfficientNet&lt;/em&gt;. Setelah di&lt;em&gt;fit&lt;/em&gt;
kemudian model digunakan untuk melakukan pemrediksian terhadap data test. hasil prediksi dari kedua
model kemudian diensembel untuk mendapatkan hasil yang maksimal. Dari hasil ensembel yang berupa
peluang akan ditetapkan nilai &lt;em&gt;threshold&lt;/em&gt; kemudia nilai tersebut digunakan untuk menentukan apakah
hasil prediksi(peluang) masuk ke kelas &lt;code&gt;0&lt;/code&gt; atau &lt;code&gt;1&lt;/code&gt;.&lt;/p&gt;&lt;h2 id=&quot;first-things-first&quot;&gt;First Things First&lt;/h2&gt;&lt;ul&gt;&lt;li&gt;Install &amp;amp; Load Library&lt;/li&gt;&lt;li&gt;Global SEED-ing&lt;/li&gt;&lt;li&gt;Keeping Google Cloud Storage PATH&lt;/li&gt;&lt;/ul&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;# Install Package yang Dibutuhkan
!pip install -q --upgrade pip
!pip install -q efficientnet  # EfficientNet
!pip -q install sastrawi      # Sastrawi

# Imports
import os, random, re, string, emoji
from timeit import default_timer
from tqdm.notebook import tqdm

# Kaggle Datasets for checking GCS
from kaggle_datasets import KaggleDatasets

# Scientific tools
import numpy as np
import pandas as pd

# Plotting tools
import matplotlib.pyplot as plt
import seaborn as sns
plt.style.use(&amp;quot;seaborn-notebook&amp;quot;)

# Interactive
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots

# Image processing
from PIL import Image
from skimage.transform import rotate

# Tensorflow and Keras
import tensorflow as tf
from tensorflow.keras.preprocessing.image import load_img, img_to_array

# Scikit Learn
from sklearn.utils import shuffle
from sklearn.metrics import *
from sklearn.model_selection import *
from sklearn.feature_extraction.text import CountVectorizer

# SEED ALL
SEED = 42

os.environ[&amp;#x27;PYTHONHASHSEED&amp;#x27;] = str(SEED)
random.seed(SEED)
np.random.seed(SEED)

os.environ[&amp;#x27;TF_DETERMINISTIC_OPS&amp;#x27;] = str(SEED)
tf.random.set_seed(SEED)

# GCS PATH
GCS_PATH = KaggleDatasets().get_gcs_path(&amp;#x27;data-bdc&amp;#x27;)

# Out
print(f&amp;#x27;Using Tensorflow Version       : {tf.__version__}&amp;#x27;)
print(f&amp;#x27;Google Cloud Storage Data Path : {GCS_PATH}&amp;#x27;)
&lt;/code&gt;&lt;/pre&gt;&lt;pre&gt;&lt;code&gt;Using Tensorflow Version       : 2.2.0
Google Cloud Storage Data Path : gs://kds-7a3b4bb00789754aa5925da7c7a9217df698684ced337b42a9790257
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&quot;preprocessing-data&quot;&gt;Preprocessing Data&lt;/h2&gt;&lt;p&gt;Melakukan preprocessing data sebelum diolah&lt;/p&gt;&lt;h3 id=&quot;load-dataset&quot;&gt;Load Dataset&lt;/h3&gt;&lt;p&gt;Load dataset ke memory&lt;/p&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;train = pd.read_excel(&amp;#x27;../input/data-bdc/Data BDC - Satria Data 2020/Data Latih/Data Latih BDC.xlsx&amp;#x27;)
test = pd.read_excel(&amp;#x27;../input/data-bdc/Data BDC - Satria Data 2020/Data Uji/Data Uji BDC.xlsx&amp;#x27;)
train.head()
&lt;/code&gt;&lt;/pre&gt;&lt;table&gt;&lt;thead&gt;&lt;tr&gt;&lt;th&gt;ID&lt;/th&gt;&lt;th&gt;label&lt;/th&gt;&lt;th&gt;tanggal&lt;/th&gt;&lt;th&gt;judul&lt;/th&gt;&lt;th&gt;narasi&lt;/th&gt;&lt;th&gt;nama file gambar&lt;/th&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;71&lt;/td&gt;&lt;td&gt;1&lt;/td&gt;&lt;td&gt;2020-08-17 00:00:00&lt;/td&gt;&lt;td&gt;Pemakaian Masker Menyebabkan Penyakit Legionna...&lt;/td&gt;&lt;td&gt;A caller to a radio talk show recently shared ...&lt;/td&gt;&lt;td&gt;71.jpg&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;461&lt;/td&gt;&lt;td&gt;1&lt;/td&gt;&lt;td&gt;2020-07-17 00:00:00&lt;/td&gt;&lt;td&gt;Instruksi Gubernur Jateng tentang penilangan ...&lt;/td&gt;&lt;td&gt;Yth.Seluruh Anggota Grup Sesuai Instruksi Gube...&lt;/td&gt;&lt;td&gt;461.png&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;495&lt;/td&gt;&lt;td&gt;1&lt;/td&gt;&lt;td&gt;2020-07-13 00:00:00&lt;/td&gt;&lt;td&gt;Foto Jim Rohn: Jokowi adalah presiden terbaik ...&lt;/td&gt;&lt;td&gt;Jokowi adalah presiden terbaik dlm sejarah ban...&lt;/td&gt;&lt;td&gt;495.png&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;550&lt;/td&gt;&lt;td&gt;1&lt;/td&gt;&lt;td&gt;2020-07-08 00:00:00&lt;/td&gt;&lt;td&gt;ini bukan politik, tapi kenyataan Pak Jokowi b...&lt;/td&gt;&lt;td&gt;Maaf Mas2 dan Mbak2, ini bukan politik, tapi k...&lt;/td&gt;&lt;td&gt;550.png&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;681&lt;/td&gt;&lt;td&gt;1&lt;/td&gt;&lt;td&gt;2020-06-24 00:00:00&lt;/td&gt;&lt;td&gt;Foto Kadrun kalo lihat foto ini panas dingin&lt;/td&gt;&lt;td&gt;Kadrun kalo lihat foto ini panas dingin . .&lt;/td&gt;&lt;td&gt;681.jpg&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;h3 id=&quot;download-fungsi-tambahan&quot;&gt;Download Fungsi Tambahan&lt;/h3&gt;&lt;p&gt;Mendownload fungsi yang sudah dibuat dari responsitory kami.&lt;/p&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;!wget -q https://raw.githubusercontent.com/Hyuto/BDC-Satria-Data/master/Preprocess%20code/RPU.py
!wget -q https://raw.githubusercontent.com/Hyuto/BDC-Satria-Data/master/Preprocess%20code/Preprocess.py
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&quot;data-gambar&quot;&gt;Data Gambar&lt;/h2&gt;&lt;p&gt;Melakukan preprocessing pada data gambar&lt;/p&gt;&lt;h3 id=&quot;sesuaikan-path-data-gambar&quot;&gt;Sesuaikan Path data gambar&lt;/h3&gt;&lt;p&gt;Menyesuaikan direktori data gambar&lt;/p&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;# Train &amp;amp; Test Image PATH
TRAIN_PATH = &amp;#x27;../input/data-bdc/Data BDC - Satria Data 2020/Data Latih/File Gambar Data Latih/&amp;#x27;
TEST_PATH = &amp;#x27;../input/data-bdc/Data BDC - Satria Data 2020/Data Uji/File Gambar Data Uji/&amp;#x27;
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;tambahkan Path ke nama file gambar&lt;/p&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;# Apply to nama file
TRAIN_IMG = [TRAIN_PATH + x for x in train[&amp;#x27;nama file gambar&amp;#x27;].values]
TEST_IMG = [TEST_PATH + x for x in test[&amp;#x27;nama file gambar&amp;#x27;].values]
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&quot;checking-missing-file&quot;&gt;Checking Missing File&lt;/h3&gt;&lt;p&gt;Mengecek apakah ada file yang hilang / tidak bisa terbaca pada direktori gambar.&lt;/p&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;def check_missing(files, return_missing_ID = True):
    &amp;quot;&amp;quot;&amp;quot;
    Mengecek keberadaan data gambar berdasarkan direktori
    &amp;quot;&amp;quot;&amp;quot;
    missing = []
    for file in files:
        if not os.path.isfile(file):
            missing.append(file)
    print(f&amp;#x27;[INFO] Missing {len(missing)} file&amp;#x27;)
    if return_missing_ID:
        return sorted([int(x.split(&amp;#x27;/&amp;#x27;)[-1][:-4]) for x in missing])
&lt;/code&gt;&lt;/pre&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;# Missing pada data TRAIN
missing_train = check_missing(TRAIN_IMG)
missing_train
&lt;/code&gt;&lt;/pre&gt;&lt;pre&gt;&lt;code&gt;[INFO] Missing 9 file

[48121, 275477, 343052, 367583, 555990, 697754, 697955, 742855, 743885]
&lt;/code&gt;&lt;/pre&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;# Missing pada data TEST
missing_test = check_missing(TEST_IMG)
missing_test
&lt;/code&gt;&lt;/pre&gt;&lt;pre&gt;&lt;code&gt;[INFO] Missing 2 file

[690192, 693499]
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Terdapat 9 file gambar pada data train dan 2 file gambar pada data test yang hilang. Hal ini diduga karena nama file berbeda pada direktori gambar dengan nama file yang ada pada dataframe. Maka dari itu kami melakukan pengecekan nama file pada direktori gambar dengan menyamakan &lt;code&gt;ID&lt;/code&gt;-nya.&lt;/p&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;def fixing_extensions(missing, path):
    &amp;quot;&amp;quot;&amp;quot;
    Membenarkan ekstensi file gambar yang di anggap hilang
    dari direktori.
    &amp;quot;&amp;quot;&amp;quot;
    res = []
    for miss in missing:
        fixed = False
        list_dir = os.listdir(path)
        for i in range(len(list_dir)):
            if miss == int(list_dir[i][:-4]):
                fixed = True
                res.append((miss, list_dir[i]))
                break
        if not fixed:
            res.append((miss, &amp;#x27;404&amp;#x27;)) # Not Found
    return res
&lt;/code&gt;&lt;/pre&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;# Apply function
fixed_missing_train = fixing_extensions(missing_train, TRAIN_PATH)
fixed_missing_test = fixing_extensions(missing_test, TEST_PATH)
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Setelah nama file yang hilang dibenarkan, apply nama file yang benar ke main dataframe.&lt;/p&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;# Apply to train DF
for Id, filename in fixed_missing_train:
    index = train.ID.tolist().index(Id)
    train.loc[index, &amp;#x27;nama file gambar&amp;#x27;] = filename

# Apply to test DF
for Id, filename in fixed_missing_test:
    index = test.ID.tolist().index(Id)
    test.loc[index, &amp;#x27;nama file gambar&amp;#x27;] = filename
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Spesifikasi ulang nama file gambar&lt;/p&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;TRAIN_X = [TRAIN_PATH + x for x in train[&amp;#x27;nama file gambar&amp;#x27;].values if x != &amp;#x27;404&amp;#x27;]
TRAIN_y = train.label.values
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&quot;menampilkan-data-gambar&quot;&gt;Menampilkan Data Gambar&lt;/h3&gt;&lt;p&gt;Menampilkan beberapa data gambar yang ada&lt;/p&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;def read_and_resize(path):
    &amp;quot;&amp;quot;&amp;quot;
    Read &amp;amp; Resize data gambar
    &amp;quot;&amp;quot;&amp;quot;
    img = Image.open(path)
    img.resize((256, 256), Image.ANTIALIAS)
    return img

def show_images(list_dir, label, load_image = read_and_resize, seed = SEED):
    &amp;quot;&amp;quot;&amp;quot;
    Menampilkan Gambar Secara acak berdasarkan kelasnya
    masing - masing sebanyak 5 buah.
    &amp;quot;&amp;quot;&amp;quot;
    random.seed(seed)
    data_0 = random.sample([x for x in zip(list_dir, label) if x[1] == 0], 5)
    data_1 = random.sample([x for x in zip(list_dir, label) if x[1] == 1], 5)
    fig, axes = plt.subplots(2, 5, figsize = (20, 10))
    for i in range(2):
        if i == 0:
            data = data_0
        else:
            data = data_1
        for j in range(5):
            img = load_image(data[j][0])
            axes[i, j].imshow(img)
            axes[i, j].set_title(f&amp;#x27;Label : {data[j][1]}&amp;#x27;, fontsize = 14)
            axes[i, j].axis(&amp;#x27;off&amp;#x27;)
    fig.tight_layout()
    plt.show()
&lt;/code&gt;&lt;/pre&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;show_images(TRAIN_X, TRAIN_y)
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;span class=&quot;gatsby-resp-image-wrapper&quot; style=&quot;position:relative;display:block;margin-left:auto;margin-right:auto;max-width:630px&quot;&gt;
      &lt;a class=&quot;gatsby-resp-image-link&quot; href=&quot;/static/545399b05d7ca1491d61ceacd714411c/5bd27/output_27_0.png&quot; style=&quot;display:block&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;
    &lt;span class=&quot;gatsby-resp-image-background-image&quot; style=&quot;padding-bottom:50%;position:relative;bottom:0;left:0;background-image:url(&amp;#x27;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAKCAYAAAC0VX7mAAAACXBIWXMAAAsSAAALEgHS3X78AAADGUlEQVQoz12SX0xTBxyFf4PhY8GVF+OyhwUDM6iIhOKWGgnEwBJd4txMo4LRJXuZkDmdzogsW5wErYiKDixEENIKSKAFApYJiAW04ril/0vh1tLbll4Y3YXWFuQsNfFlJ/neTr6ch0MsyxKAd9y4XUM7s3Kysz7fo0pLz2xN37U7w84hdWTUsFfzxCBtVk9mvO/GcC+tEuedp3WAloVlMhqmiCwWS9P4+HgfwzBPAFRu2S45vi0nD6mZUuQfOFw0xwX1DtssPF4eVqefDUdXrwCojq5Gby2urF0MBBZlc5z3hIt1nbRbrYdiwpDBMAWGMWCB5+1pmdIjKelZ+CRlK3ILvpKF3oT10y43Xrs86yFB8LTdu7dWf60Cg0PPIIRCYb+fB88vwMN5YTSa18lsNrsZhonGhCvCv2Np0v3F0uztKJR8CtnXhUVRwFlXo4b85+swOfhQj6LGqir/kdcNDi1Eom8mvZy/2+fzDwaFlWGfn28nk8n0Gcuy2+x2+04AmxM2frw/JT0rkLZD8k9mzheFAEqVdzTyWw11lUbXchmATQCSAIjDQSGpt/zCh6Gq7+Kf/3k/3mB7HU/Hjh2lYFAgm9VGcvk1MlvNMufsbNfC0pI6HI3kl589I9L9baox2dgq10TXR68mJ+gV84Ii3DTNDD0mzfkL9KysjEYU9TTj4YhEiUnXh5/qmpSqNiUR/cTa5mvnXAE8H9aDNbrPWuz27KdjFlgcHADkNzS3fH+3rvqmWftIbtW0/qI+VXr6YVHxrx2X/yh3ergSEovF61crf8P5cyWIi/vAqdcOVLUolGi8cxcdivpT4dXIjo6bv0OrvP8WQF5ndw+n7evApH4QvW2qtZfqToz192CsW4N53g8SJydP7duX6yssyA8mbNjQU3+7rvrIQRkuXzqNzubaEiEUydC9MMPhdMcW5t64WqH+9ksJd+aHg6yqRTHqGOofMekGDKN9j6d8AX6ARCKRyOZwJz5o6UqOnbW1XVvVoGhCU2Mj/tJqS6adMxleP4/FJQEmm0tySdFPtYe+SSCieJwrjaP/5T/2SQSXGKzf4QAAAABJRU5ErkJggg==&amp;#x27;);background-size:cover;display:block&quot;&gt;&lt;/span&gt;
  &lt;img class=&quot;gatsby-resp-image-image&quot; alt=&quot;png&quot; title=&quot;png&quot; src=&quot;/static/545399b05d7ca1491d61ceacd714411c/f058b/output_27_0.png&quot; srcSet=&quot;/static/545399b05d7ca1491d61ceacd714411c/c26ae/output_27_0.png 158w,/static/545399b05d7ca1491d61ceacd714411c/6bdcf/output_27_0.png 315w,/static/545399b05d7ca1491d61ceacd714411c/f058b/output_27_0.png 630w,/static/545399b05d7ca1491d61ceacd714411c/40601/output_27_0.png 945w,/static/545399b05d7ca1491d61ceacd714411c/78612/output_27_0.png 1260w,/static/545399b05d7ca1491d61ceacd714411c/5bd27/output_27_0.png 1432w&quot; sizes=&quot;(max-width: 630px) 100vw, 630px&quot; style=&quot;width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0&quot; loading=&quot;lazy&quot; decoding=&quot;async&quot;/&gt;
  &lt;/a&gt;
    &lt;/span&gt;&lt;/p&gt;&lt;p&gt;Dapat dilihat gambar memiliki resolusi yang &lt;code&gt;berbeda - beda&lt;/code&gt;.&lt;/p&gt;&lt;h3 id=&quot;preprocess-and-resizing&quot;&gt;Preprocess and Resizing&lt;/h3&gt;&lt;p&gt;Untuk mengatasi data gambar dengan resolusi yang berbeda - beda maka dilakukan dengan metode croping pada bagian &lt;code&gt;center&lt;/code&gt; gambar dan &lt;code&gt;resizing&lt;/code&gt; gambar tersebut ke ukuran yang di tetapkan.&lt;/p&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;def load_and_preprocess_image(path: str, size = [256, 256]):
    &amp;quot;&amp;quot;&amp;quot;
    Load &amp;amp; Preprocess data gambar
    &amp;quot;&amp;quot;&amp;quot;
    image = img_to_array(load_img(path))
    img = tf.convert_to_tensor(image, dtype=tf.float32)
    shapes = tf.shape(img)
    h, w = shapes[-3], shapes[-2]
    dim = tf.minimum(h, w)
    img = tf.image.resize_with_crop_or_pad(img, dim, dim)
    img = tf.image.resize(img, size)
    img = tf.cast(img, tf.float32) / 255.0
    return img.numpy()
&lt;/code&gt;&lt;/pre&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;show_images(TRAIN_X, TRAIN_y, load_and_preprocess_image)
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;span class=&quot;gatsby-resp-image-wrapper&quot; style=&quot;position:relative;display:block;margin-left:auto;margin-right:auto;max-width:630px&quot;&gt;
      &lt;a class=&quot;gatsby-resp-image-link&quot; href=&quot;/static/f23197e17aa8023fc2c7acd396292c4d/5bd27/output_31_0.png&quot; style=&quot;display:block&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;
    &lt;span class=&quot;gatsby-resp-image-background-image&quot; style=&quot;padding-bottom:44.93670886075949%;position:relative;bottom:0;left:0;background-image:url(&amp;#x27;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAJCAYAAAAywQxIAAAACXBIWXMAAAsSAAALEgHS3X78AAAC2UlEQVQozwXBiU9SARwA4PdntGWWeWTLI9O5LNfMsUJdp7C0tWqaa820LZ2r2eHWMRWzrWbOqSW4hitX3hfCQhP1oY/34AEiJALv6IE8OQIFj1/fh5AkKdFoNDI9QXxZxnTvY09kN2bkXpBl5gp6zgguvVPNGpoJwx8pStilynlbMwC07OztSXeiUene7m4jx3lanRQj9fJ8j9fjbUJIkgwRBAFWqxUMpHEnMT3Hl5YjgPj0s5B6On+LMFiDLM0C7/MDzXiCRtK0ZVsxg3V9Hfz/Qj6GYXb8/gBsb0cB0+EBBMdxGsMw0Ov1+2r1jCc2+ZQj4XgKxByJg2MpJ1m7y0WptRaYHluEVcsaVVdazFYV5kHt43qgWLfDvrbmYWh6PxzeBkJvcCJms3nDaDSCzboKpNkSSMzKY6/lJkBJ9gG4ei6Zd7g33Z8l30FS0wCYZcOt+vSa/91cB1Njk7C56WMdDioQ3IrCdnQXAsEwhywtLRdgGCbGcZ1obmGx6HxhnrDsZoH4zfOHohf1lYUWm71gbBwT95OYaMXlKyAtpkK7wykOR6Jinl4XLil+FpHKabEZw0U6vUmIJCQmDXv5AGq12RdjDh6ecJj5gU1vWDs1qFy0EY5R3usfYv5yWtgHNBwODT1peDna2SVB9epB1CTvGOi6fWdCVf0I7SkrRycVykEkKzMjaCKXYHi4DzIzUiKoSssP9KugXdIGM2PTIa1y3N9ZUwG9r56Bh+P8TooJWfBZsJp1sG608OqJkciabRXm+r8BoV3wIfFxsayoWAhXLudDemryxpCs3VlVUQndbW9BMSjn5lWTdOvTKuj92AK0y0WLS29xvbIm+DUrh9aOZuekvGOD93Kg+CoDHbpAITGHYrnye3ehqvIBHE1I5j+0dFPFF69DbfV9mBroc7PeCKtZjQDlAwgFfGxuWpK7VJgJNwRpUCLIppZH5Pwez4Gm/wfocZL5D4xx/ne1mvD3AAAAAElFTkSuQmCC&amp;#x27;);background-size:cover;display:block&quot;&gt;&lt;/span&gt;
  &lt;img class=&quot;gatsby-resp-image-image&quot; alt=&quot;png&quot; title=&quot;png&quot; src=&quot;/static/f23197e17aa8023fc2c7acd396292c4d/f058b/output_31_0.png&quot; srcSet=&quot;/static/f23197e17aa8023fc2c7acd396292c4d/c26ae/output_31_0.png 158w,/static/f23197e17aa8023fc2c7acd396292c4d/6bdcf/output_31_0.png 315w,/static/f23197e17aa8023fc2c7acd396292c4d/f058b/output_31_0.png 630w,/static/f23197e17aa8023fc2c7acd396292c4d/40601/output_31_0.png 945w,/static/f23197e17aa8023fc2c7acd396292c4d/78612/output_31_0.png 1260w,/static/f23197e17aa8023fc2c7acd396292c4d/5bd27/output_31_0.png 1432w&quot; sizes=&quot;(max-width: 630px) 100vw, 630px&quot; style=&quot;width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0&quot; loading=&quot;lazy&quot; decoding=&quot;async&quot;/&gt;
  &lt;/a&gt;
    &lt;/span&gt;&lt;/p&gt;&lt;h3 id=&quot;image-augmentation&quot;&gt;Image Augmentation&lt;/h3&gt;&lt;p&gt;Karena data yang ada tidak seimbang maka perlu dilakukan penambahan data. Metode yang digunakan untuk menambahkan data yaitu augmentasi data gambar pada kelas yang sedikit dengan melakukan &lt;code&gt;random rotation&lt;/code&gt;.&lt;/p&gt;&lt;p&gt;Metode Augmentasi yang digunakan.&lt;/p&gt;&lt;pre&gt;&lt;code&gt;Rotasi Acak dengan nilai rotasi di random pada kisaran -70 sd 70 derajad
&lt;/code&gt;&lt;/pre&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;data_augmentation = lambda x : (
    rotate(x, random.randint(-70, 70), mode=&amp;#x27;reflect&amp;#x27;)
)
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Menampilkan hasil dari augmentasi.&lt;/p&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;def AUG_test(X):
    &amp;quot;&amp;quot;&amp;quot;
    Plot gambar dengan fungsi Augmentasi
    &amp;quot;&amp;quot;&amp;quot;
    X = load_and_preprocess_image(X)
    fig, axes = plt.subplots(1, 5, figsize = (20,10))
    axes[0].imshow(X)
    axes[0].set_title(&amp;#x27;Actual&amp;#x27;, fontsize = 14)
    axes[0].axis(&amp;#x27;off&amp;#x27;)
    for i in range(1, 5):
        aug = data_augmentation(X)
        axes[i].imshow(aug)
        axes[i].set_title(&amp;#x27;Augmented&amp;#x27;, fontsize = 14)
        axes[i].axis(&amp;#x27;off&amp;#x27;)
    fig.tight_layout()
    return plt.show()
&lt;/code&gt;&lt;/pre&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;for i in random.sample(TRAIN_X, 2):
    AUG_test(i)
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;span class=&quot;gatsby-resp-image-wrapper&quot; style=&quot;position:relative;display:block;margin-left:auto;margin-right:auto;max-width:630px&quot;&gt;
      &lt;a class=&quot;gatsby-resp-image-link&quot; href=&quot;/static/6272eec8c1a27d92cf905df980bff40f/5bd27/output_36_0.png&quot; style=&quot;display:block&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;
    &lt;span class=&quot;gatsby-resp-image-background-image&quot; style=&quot;padding-bottom:21.51898734177215%;position:relative;bottom:0;left:0;background-image:url(&amp;#x27;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAECAYAAACOXx+WAAAACXBIWXMAAAsSAAALEgHS3X78AAABRklEQVQY0wXB20vCUBwA4P15FeFDNwqCXoKILtBLF+zF1yISKcMeAtGofOlFFpaWGArLhZcm1nS6Y9uZ21md1Gkz5n59H6OTr4SqamVCzFITKelyKc8qUrFSfReFbs9K8XwuoTZKglCpCgDuPQxxqlR4EWT0WQHXZT/EWlqSmm8GMQXHcVgGt0RKDAVGjg06btlLs1ME1wtg6BhUFfeWF+aoIr4CMTAoqkYjF6Fe16iDpmH4oZTYHdU22ggsqwMmMQiTScUJx94Ayj+CbmDraGdVy0ROANw+KLpO/Xvr5PkyCAADaCBE1uY99CkcABh1oa23NZ9318pch4AiAX6tb42ZHJ/gNmbG0Jl3U04+JIvx22g2EjhEzp8lm6bOx6Ln3FXQj1xngDSscIm7GH+6vyXLtSoa9ml2cdpTPN5ekcMHPiRKcu4fWZADu740UWMAAAAASUVORK5CYII=&amp;#x27;);background-size:cover;display:block&quot;&gt;&lt;/span&gt;
  &lt;img class=&quot;gatsby-resp-image-image&quot; alt=&quot;png&quot; title=&quot;png&quot; src=&quot;/static/6272eec8c1a27d92cf905df980bff40f/f058b/output_36_0.png&quot; srcSet=&quot;/static/6272eec8c1a27d92cf905df980bff40f/c26ae/output_36_0.png 158w,/static/6272eec8c1a27d92cf905df980bff40f/6bdcf/output_36_0.png 315w,/static/6272eec8c1a27d92cf905df980bff40f/f058b/output_36_0.png 630w,/static/6272eec8c1a27d92cf905df980bff40f/40601/output_36_0.png 945w,/static/6272eec8c1a27d92cf905df980bff40f/78612/output_36_0.png 1260w,/static/6272eec8c1a27d92cf905df980bff40f/5bd27/output_36_0.png 1432w&quot; sizes=&quot;(max-width: 630px) 100vw, 630px&quot; style=&quot;width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0&quot; loading=&quot;lazy&quot; decoding=&quot;async&quot;/&gt;
  &lt;/a&gt;
    &lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span class=&quot;gatsby-resp-image-wrapper&quot; style=&quot;position:relative;display:block;margin-left:auto;margin-right:auto;max-width:630px&quot;&gt;
      &lt;a class=&quot;gatsby-resp-image-link&quot; href=&quot;/static/36ff25b583392546d2970760c1d635cc/5bd27/output_36_1.png&quot; style=&quot;display:block&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;
    &lt;span class=&quot;gatsby-resp-image-background-image&quot; style=&quot;padding-bottom:21.51898734177215%;position:relative;bottom:0;left:0;background-image:url(&amp;#x27;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAECAYAAACOXx+WAAAACXBIWXMAAAsSAAALEgHS3X78AAABQklEQVQY0wXBTUsCQRgAYP9Xx6hbp+gDT2FYQVIkSCEFHroUZERIUpduQQRBB8lC0oOBpO3M7uzXzM7uzOq64mpb6MHb2/MkDNOuqpquYpXgXtCv25RVENZ0BWFiWrSmanrVsigZhF3CGXprfVZrtmUSBWGd6GbFpk5dQaqmIJVglVQSlLKYUQa+78NoFM1c7kXc4eB5AoIgmBimFTPmwG88hofHUlwuH066vgeWRUEjeiQ8b+ZyDg7nIKQfJXCnERkWBi4ccKgyfX+9D1X9G/r9HgxDGWvEiITLoYO/YGtzKTrIrMaIKDAcDIAyGj4/XU6R1oHhcAA/4zBMFM/TrcJxSuaOUmI3u44y6cVmPrch84UdUbo7a9s2bbmOLq+Ke3J7ZaG1tjzXvrk9FZO/sewGsplOzqPsflJcXJ/Ij8bL5z8cvPtEvFtgDgAAAABJRU5ErkJggg==&amp;#x27;);background-size:cover;display:block&quot;&gt;&lt;/span&gt;
  &lt;img class=&quot;gatsby-resp-image-image&quot; alt=&quot;png&quot; title=&quot;png&quot; src=&quot;/static/36ff25b583392546d2970760c1d635cc/f058b/output_36_1.png&quot; srcSet=&quot;/static/36ff25b583392546d2970760c1d635cc/c26ae/output_36_1.png 158w,/static/36ff25b583392546d2970760c1d635cc/6bdcf/output_36_1.png 315w,/static/36ff25b583392546d2970760c1d635cc/f058b/output_36_1.png 630w,/static/36ff25b583392546d2970760c1d635cc/40601/output_36_1.png 945w,/static/36ff25b583392546d2970760c1d635cc/78612/output_36_1.png 1260w,/static/36ff25b583392546d2970760c1d635cc/5bd27/output_36_1.png 1432w&quot; sizes=&quot;(max-width: 630px) 100vw, 630px&quot; style=&quot;width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0&quot; loading=&quot;lazy&quot; decoding=&quot;async&quot;/&gt;
  &lt;/a&gt;
    &lt;/span&gt;&lt;/p&gt;&lt;h3 id=&quot;up-sampling-data&quot;&gt;Up Sampling Data&lt;/h3&gt;&lt;p&gt;Mengapply Preprocess &amp;amp; Augmentasi ke data gambar dan menyimpan gambar pada direktori baru.
Menggunakan Fungsi tambahan yang sudah di download pada file &lt;code&gt;RPU.py&lt;/code&gt;&lt;/p&gt;&lt;p&gt;Fungsi akan mengembalikan List direktori gambar dan juga labelnya.&lt;/p&gt;&lt;p&gt;&lt;code&gt;ApplyAUG -&amp;gt; IMAGE_DIR, Label&lt;/code&gt;&lt;/p&gt;&lt;p&gt;Contoh Penggunaan&lt;/p&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;from RPU import ApplyAUG

TRAIN_X, TRAIN_y = (
       ApplyAUG(TRAIN_X,               # List atau Array direktori dari gambar
                TRAIN_y,               # List atau Array kelas(label) dari TRAIN_X [One Hot Encoding]
                PATH,                  # Direktori data gambar
                up_sample_ratio = 1,   # Rasio Up Sample
                up_sample_class = &amp;#x27;0&amp;#x27;, # Spesifikasi Class yang akan di Up Sample
                data_aug = data_augmentation,  # Fungsi Augmentasi
                LP = load_and_preprocess_image # Fungsi Load &amp;amp; Preprocess
                )
)
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Atau bisa juga menggunakan shell&lt;/p&gt;&lt;pre&gt;&lt;code&gt;!python RPU.py PATH SIZE TEST_SIZE UP_SAMPLES UP_SAMPLE_CLASS
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Namun pada kernel ini tidak akan di gunakan fungsi tersebut. Melaikan menggunakan hasil dari fungsi tersebut, karena kernel ini di run pada TPU &amp;amp; TPU tidak dapat membaca file diluar dari &lt;code&gt;GCS&lt;/code&gt; (lokal direktori).&lt;/p&gt;&lt;p&gt;Note : Up Sample data pada Notebook ini didapatkan dengan command berikut&lt;/p&gt;&lt;pre&gt;&lt;code&gt;!python RPU.py &amp;#x27;../input/data-bdc/Data BDC - Satria Data 2020/Data Latih/File Gambar Data Latih/&amp;#x27; \
                SIZE:512 TEST_SIZE:0.15 UP_SAMPLES:0.5-1-2 UP_SAMPLE_CLASS:0
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&quot;data-text&quot;&gt;Data Text&lt;/h2&gt;&lt;p&gt;Melakukan preprocessing pada data text.&lt;/p&gt;&lt;p&gt;Load fungsi pada file &lt;code&gt;Preprocess.py&lt;/code&gt; terlebih dahulu&lt;/p&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;from Preprocess import *
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Pada notebook ini pengolahan data text dilakukan menggunakan &lt;code&gt;judul&lt;/code&gt; dan &lt;code&gt;narasi&lt;/code&gt;, sehingga perlu dilakukan penyatuan kolom &lt;code&gt;judul&lt;/code&gt; dan kolom &lt;code&gt;narasi&lt;/code&gt; dipisahkan dengan spasi.&lt;/p&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;train[&amp;#x27;text&amp;#x27;] = (train[&amp;#x27;judul&amp;#x27;] + &amp;#x27; &amp;#x27; + train[&amp;#x27;narasi&amp;#x27;]).apply(lambda x : x.lower())
test[&amp;#x27;text&amp;#x27;] = (test[&amp;#x27;judul&amp;#x27;] + &amp;#x27; &amp;#x27; + test[&amp;#x27;narasi&amp;#x27;]).apply(lambda x : x.lower())
train.head()
&lt;/code&gt;&lt;/pre&gt;&lt;table&gt;&lt;thead&gt;&lt;tr&gt;&lt;th&gt;ID&lt;/th&gt;&lt;th&gt;label&lt;/th&gt;&lt;th&gt;tanggal&lt;/th&gt;&lt;th&gt;judul&lt;/th&gt;&lt;th&gt;narasi&lt;/th&gt;&lt;th&gt;nama file gambar&lt;/th&gt;&lt;th&gt;text&lt;/th&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;71&lt;/td&gt;&lt;td&gt;1&lt;/td&gt;&lt;td&gt;2020-08-17 00:00:00&lt;/td&gt;&lt;td&gt;Pemakaian Masker Menyebabkan Penyakit Legionna...&lt;/td&gt;&lt;td&gt;A caller to a radio talk show recently shared ...&lt;/td&gt;&lt;td&gt;71.jpg&lt;/td&gt;&lt;td&gt;pemakaian masker menyebabkan penyakit legionna...&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;461&lt;/td&gt;&lt;td&gt;1&lt;/td&gt;&lt;td&gt;2020-07-17 00:00:00&lt;/td&gt;&lt;td&gt;Instruksi Gubernur Jateng tentang penilangan ...&lt;/td&gt;&lt;td&gt;Yth.Seluruh Anggota Grup Sesuai Instruksi Gube...&lt;/td&gt;&lt;td&gt;461.png&lt;/td&gt;&lt;td&gt;instruksi gubernur jateng tentang penilangan ...&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;495&lt;/td&gt;&lt;td&gt;1&lt;/td&gt;&lt;td&gt;2020-07-13 00:00:00&lt;/td&gt;&lt;td&gt;Foto Jim Rohn: Jokowi adalah presiden terbaik ...&lt;/td&gt;&lt;td&gt;Jokowi adalah presiden terbaik dlm sejarah ban...&lt;/td&gt;&lt;td&gt;495.png&lt;/td&gt;&lt;td&gt;foto jim rohn: jokowi adalah presiden terbaik ...&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;550&lt;/td&gt;&lt;td&gt;1&lt;/td&gt;&lt;td&gt;2020-07-08 00:00:00&lt;/td&gt;&lt;td&gt;ini bukan politik, tapi kenyataan Pak Jokowi b...&lt;/td&gt;&lt;td&gt;Maaf Mas2 dan Mbak2, ini bukan politik, tapi k...&lt;/td&gt;&lt;td&gt;550.png&lt;/td&gt;&lt;td&gt;ini bukan politik, tapi kenyataan pak jokowi b...&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;681&lt;/td&gt;&lt;td&gt;1&lt;/td&gt;&lt;td&gt;2020-06-24 00:00:00&lt;/td&gt;&lt;td&gt;Foto Kadrun kalo lihat foto ini panas dingin&lt;/td&gt;&lt;td&gt;Kadrun kalo lihat foto ini panas dingin . .&lt;/td&gt;&lt;td&gt;681.jpg&lt;/td&gt;&lt;td&gt;foto kadrun kalo lihat foto ini panas dingin k...&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;h3 id=&quot;mengecek-duplicated-values&quot;&gt;Mengecek Duplicated Values&lt;/h3&gt;&lt;p&gt;Mengecek keberadaan data yang berduplikasi.&lt;/p&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;train[train.duplicated(subset=[&amp;#x27;text&amp;#x27;], keep=False)]
&lt;/code&gt;&lt;/pre&gt;&lt;table&gt;&lt;thead&gt;&lt;tr&gt;&lt;th align=&quot;center&quot;&gt;ID&lt;/th&gt;&lt;th align=&quot;center&quot;&gt;label&lt;/th&gt;&lt;th align=&quot;left&quot;&gt;tanggal&lt;/th&gt;&lt;th align=&quot;left&quot;&gt;judul&lt;/th&gt;&lt;th align=&quot;center&quot;&gt;narasi&lt;/th&gt;&lt;th align=&quot;left&quot;&gt;nama file gambar&lt;/th&gt;&lt;th&gt;text&lt;/th&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td align=&quot;center&quot;&gt;130974&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;0&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;2015-11-28 00:00:00&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;Jokowi Lebih Memilih Helikopter Buatan Luar Ne...&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;INSTING MAKELAR\n \n AKU Awalnya kaget, membac...&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;130974.png&lt;/td&gt;&lt;td&gt;jokowi lebih memilih helikopter buatan luar ne...&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;center&quot;&gt;140123&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;1&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;2020-06-02 00:00:00&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;Foto Sekarang malesiya sapu habis penduduk asi...&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;Perhatian perhatian Sekarang malesiya makin da...&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;140123.jpg&lt;/td&gt;&lt;td&gt;foto sekarang malesiya sapu habis penduduk asi...&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;center&quot;&gt;188319&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;0&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;2015-11-28 00:00:00&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;Jokowi Lebih Memilih Helikopter Buatan Luar Ne...&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;INSTING MAKELAR\n \n AKU Awalnya kaget, membac...&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;188319.png&lt;/td&gt;&lt;td&gt;jokowi lebih memilih helikopter buatan luar ne...&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;center&quot;&gt;312152&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;1&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;2020-06-02 00:00:00&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;Foto Sekarang malesiya sapu habis penduduk asi...&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;Perhatian perhatian Sekarang malesiya makin da...&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;312152.jpg&lt;/td&gt;&lt;td&gt;foto sekarang malesiya sapu habis penduduk asi...&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;center&quot;&gt;898927&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;1&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;2018-03-12 00:00:00&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;Soal Bocornya NIK dan Nomor KK Sekarang dilemp...&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;Siapa yang disalahkan ?\n Saling lempar tanggu...&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;898927.jpg&lt;/td&gt;&lt;td&gt;soal bocornya nik dan nomor kk sekarang dilemp...&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;center&quot;&gt;923503&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;1&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;2018-03-12 00:00:00&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;Soal Bocornya NIK dan Nomor KK Sekarang dilemp...&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;Siapa yang disalahkan ?\n Saling lempar tanggu...&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;923503.png&lt;/td&gt;&lt;td&gt;soal bocornya nik dan nomor kk sekarang dilemp...&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;Menghapus data yang berduplikasi&lt;/p&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;# Drop duplicates row
train = train.drop_duplicates(subset =&amp;quot;text&amp;quot;).reset_index()
&lt;/code&gt;&lt;/pre&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;def clean_up(arr):
    r&amp;quot;&amp;quot;&amp;quot;
    Cleanup \n and lowering text
    &amp;quot;&amp;quot;&amp;quot;
    for i in range(len(arr)):
        arr[i] = arr[i].lower()
        arr[i] = re.sub(&amp;#x27;\n&amp;#x27;, &amp;#x27; &amp;#x27;, arr[i])
        arr[i] = &amp;#x27; &amp;#x27;.join(arr[i].split())
    return arr
&lt;/code&gt;&lt;/pre&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;# Init text
train_text = clean_up(train.text.values)
test_text = clean_up(test.text.values)
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&quot;mengekstrack-feature-dari-text&quot;&gt;Mengekstrack Feature dari Text&lt;/h3&gt;&lt;p&gt;Mengekstrack feature seperti &lt;code&gt;URL&lt;/code&gt;, &lt;code&gt;Hashtag&lt;/code&gt;, &lt;code&gt;Tag&lt;/code&gt;, dan &lt;code&gt;Emoji&lt;/code&gt; dari text untuk melihat informasi dan frekuensi kemunculannya dalam setiap kelas pada data&lt;/p&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;%%time
FE = FeatureExtraction()
FE_test = FeatureExtraction()
FE.fit(train_text, train.label.values)
FE_test.fit(test_text)
&lt;/code&gt;&lt;/pre&gt;&lt;pre&gt;&lt;code&gt;CPU times: user 6.09 s, sys: 18 ms, total: 6.11 s
Wall time: 6.11 s
&lt;/code&gt;&lt;/pre&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;FE.get_table(&amp;#x27;urls&amp;#x27;, return_prop = True)
&lt;/code&gt;&lt;/pre&gt;&lt;table&gt;&lt;thead&gt;&lt;tr&gt;&lt;th align=&quot;left&quot;&gt;urls&lt;/th&gt;&lt;th align=&quot;center&quot;&gt;0&lt;/th&gt;&lt;th align=&quot;center&quot;&gt;1&lt;/th&gt;&lt;th align=&quot;right&quot;&gt;frekuensi&lt;/th&gt;&lt;th align=&quot;right&quot;&gt;max_prop&lt;/th&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;kompas.com&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;0.333333&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;0.666667&lt;/td&gt;&lt;td align=&quot;right&quot;&gt;3&lt;/td&gt;&lt;td align=&quot;right&quot;&gt;0.666667&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;viva.co.id&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;0.000000&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;1.000000&lt;/td&gt;&lt;td align=&quot;right&quot;&gt;2&lt;/td&gt;&lt;td align=&quot;right&quot;&gt;1.000000&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;seword.com&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;0.500000&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;0.500000&lt;/td&gt;&lt;td align=&quot;right&quot;&gt;2&lt;/td&gt;&lt;td align=&quot;right&quot;&gt;0.500000&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;opishposh.com&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;0.500000&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;0.500000&lt;/td&gt;&lt;td align=&quot;right&quot;&gt;2&lt;/td&gt;&lt;td align=&quot;right&quot;&gt;0.500000&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://t.co/jvjxc0jhmp&quot;&gt;https://t.co/jvjxc0jhmp&lt;/a&gt;&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;0.000000&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;1.000000&lt;/td&gt;&lt;td align=&quot;right&quot;&gt;1&lt;/td&gt;&lt;td align=&quot;right&quot;&gt;1.000000&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;...&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;...&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;...&lt;/td&gt;&lt;td align=&quot;right&quot;&gt;...&lt;/td&gt;&lt;td align=&quot;right&quot;&gt;...&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;http://poskotanews.com/%E2%80%A6/pa-212-batalkan-acara&quot;&gt;http://poskotanews.com/â€¦/pa-212-batalkan-acara&lt;/a&gt;...&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;0.000000&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;1.000000&lt;/td&gt;&lt;td align=&quot;right&quot;&gt;1&lt;/td&gt;&lt;td align=&quot;right&quot;&gt;1.000000&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;http://www.piyunganonline.co&quot;&gt;www.piyunganonline.co&lt;/a&gt;&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;0.000000&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;1.000000&lt;/td&gt;&lt;td align=&quot;right&quot;&gt;1&lt;/td&gt;&lt;td align=&quot;right&quot;&gt;1.000000&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://goo.gl/2xz5yn&quot;&gt;https://goo.gl/2xz5yn&lt;/a&gt;&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;0.000000&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;1.000000&lt;/td&gt;&lt;td align=&quot;right&quot;&gt;1&lt;/td&gt;&lt;td align=&quot;right&quot;&gt;1.000000&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;corona-virus-map.com.exe.&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;1.000000&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;0.000000&lt;/td&gt;&lt;td align=&quot;right&quot;&gt;1&lt;/td&gt;&lt;td align=&quot;right&quot;&gt;1.000000&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://sekolahnews.com/daftar-passing-grade-u&quot;&gt;https://sekolahnews.com/daftar-passing-grade-u&lt;/a&gt;...&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;0.000000&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;1.000000&lt;/td&gt;&lt;td align=&quot;right&quot;&gt;1&lt;/td&gt;&lt;td align=&quot;right&quot;&gt;1.000000&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;pre&gt;&lt;code&gt;85 rows Ã— 5 columns
&lt;/code&gt;&lt;/pre&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;FE.get_table(&amp;#x27;hashtags&amp;#x27;, return_prop = True)
&lt;/code&gt;&lt;/pre&gt;&lt;table&gt;&lt;thead&gt;&lt;tr&gt;&lt;th align=&quot;center&quot;&gt;hashtags&lt;/th&gt;&lt;th align=&quot;center&quot;&gt;0&lt;/th&gt;&lt;th align=&quot;center&quot;&gt;1&lt;/th&gt;&lt;th align=&quot;right&quot;&gt;frekuensi&lt;/th&gt;&lt;th align=&quot;right&quot;&gt;max_prop&lt;/th&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td align=&quot;center&quot;&gt;#2019gantipresiden&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;0.133333&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;0.866667&lt;/td&gt;&lt;td align=&quot;right&quot;&gt;15&lt;/td&gt;&lt;td align=&quot;right&quot;&gt;0.866667&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;center&quot;&gt;#covid19&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;0.000000&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;1.000000&lt;/td&gt;&lt;td align=&quot;right&quot;&gt;5&lt;/td&gt;&lt;td align=&quot;right&quot;&gt;1.000000&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;center&quot;&gt;#2019&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;0.000000&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;1.000000&lt;/td&gt;&lt;td align=&quot;right&quot;&gt;4&lt;/td&gt;&lt;td align=&quot;right&quot;&gt;1.000000&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;center&quot;&gt;#coronavirus&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;0.000000&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;1.000000&lt;/td&gt;&lt;td align=&quot;right&quot;&gt;3&lt;/td&gt;&lt;td align=&quot;right&quot;&gt;1.000000&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;center&quot;&gt;#stayhome&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;0.000000&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;1.000000&lt;/td&gt;&lt;td align=&quot;right&quot;&gt;3&lt;/td&gt;&lt;td align=&quot;right&quot;&gt;1.000000&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;center&quot;&gt;...&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;...&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;...&lt;/td&gt;&lt;td align=&quot;right&quot;&gt;...&lt;/td&gt;&lt;td align=&quot;right&quot;&gt;...&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;center&quot;&gt;#sekolahnews&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;0.000000&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;1.000000&lt;/td&gt;&lt;td align=&quot;right&quot;&gt;1&lt;/td&gt;&lt;td align=&quot;right&quot;&gt;1.000000&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;center&quot;&gt;#wkwkland&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;0.000000&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;1.000000&lt;/td&gt;&lt;td align=&quot;right&quot;&gt;1&lt;/td&gt;&lt;td align=&quot;right&quot;&gt;1.000000&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;center&quot;&gt;#abusendal&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;0.000000&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;1.000000&lt;/td&gt;&lt;td align=&quot;right&quot;&gt;1&lt;/td&gt;&lt;td align=&quot;right&quot;&gt;1.000000&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;center&quot;&gt;#sayapancasila&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;1.000000&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;0.000000&lt;/td&gt;&lt;td align=&quot;right&quot;&gt;1&lt;/td&gt;&lt;td align=&quot;right&quot;&gt;1.000000&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;center&quot;&gt;#sayaindonesia&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;1.000000&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;0.000000&lt;/td&gt;&lt;td align=&quot;right&quot;&gt;1&lt;/td&gt;&lt;td align=&quot;right&quot;&gt;1.000000&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;pre&gt;&lt;code&gt;356 rows Ã— 5 columns
&lt;/code&gt;&lt;/pre&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;FE.get_table(&amp;#x27;tags&amp;#x27;, return_prop = True)
&lt;/code&gt;&lt;/pre&gt;&lt;table&gt;&lt;thead&gt;&lt;tr&gt;&lt;th align=&quot;center&quot;&gt;tags&lt;/th&gt;&lt;th align=&quot;center&quot;&gt;0&lt;/th&gt;&lt;th align=&quot;center&quot;&gt;1&lt;/th&gt;&lt;th align=&quot;right&quot;&gt;frekuensi&lt;/th&gt;&lt;th align=&quot;right&quot;&gt;max_prop&lt;/th&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td align=&quot;center&quot;&gt;@jokowi&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;0.111111&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;0.888889&lt;/td&gt;&lt;td align=&quot;right&quot;&gt;9&lt;/td&gt;&lt;td align=&quot;right&quot;&gt;0.888889&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;center&quot;&gt;@aniesbaswedan&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;0.000000&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;1.000000&lt;/td&gt;&lt;td align=&quot;right&quot;&gt;6&lt;/td&gt;&lt;td align=&quot;right&quot;&gt;1.000000&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;center&quot;&gt;@prabowo&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;0.166667&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;0.833333&lt;/td&gt;&lt;td align=&quot;right&quot;&gt;6&lt;/td&gt;&lt;td align=&quot;right&quot;&gt;0.833333&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;center&quot;&gt;@fadlizon&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;0.200000&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;0.800000&lt;/td&gt;&lt;td align=&quot;right&quot;&gt;5&lt;/td&gt;&lt;td align=&quot;right&quot;&gt;0.800000&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;center&quot;&gt;@netizentofa&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;0.000000&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;1.000000&lt;/td&gt;&lt;td align=&quot;right&quot;&gt;2&lt;/td&gt;&lt;td align=&quot;right&quot;&gt;1.000000&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;center&quot;&gt;...&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;...&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;...&lt;/td&gt;&lt;td align=&quot;right&quot;&gt;...&lt;/td&gt;&lt;td align=&quot;right&quot;&gt;...&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;center&quot;&gt;@kemenhub151&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;0.000000&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;1.000000&lt;/td&gt;&lt;td align=&quot;right&quot;&gt;1&lt;/td&gt;&lt;td align=&quot;right&quot;&gt;1.000000&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;center&quot;&gt;@ricky_hf&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;0.000000&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;1.000000&lt;/td&gt;&lt;td align=&quot;right&quot;&gt;1&lt;/td&gt;&lt;td align=&quot;right&quot;&gt;1.000000&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;center&quot;&gt;@kai121&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;0.000000&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;1.000000&lt;/td&gt;&lt;td align=&quot;right&quot;&gt;1&lt;/td&gt;&lt;td align=&quot;right&quot;&gt;1.000000&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;center&quot;&gt;@ankertwiter&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;0.000000&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;1.000000&lt;/td&gt;&lt;td align=&quot;right&quot;&gt;1&lt;/td&gt;&lt;td align=&quot;right&quot;&gt;1.000000&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;center&quot;&gt;@commuterline&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;0.000000&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;1.000000&lt;/td&gt;&lt;td align=&quot;right&quot;&gt;1&lt;/td&gt;&lt;td align=&quot;right&quot;&gt;1.000000&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;pre&gt;&lt;code&gt;147 rows Ã— 5 columns
&lt;/code&gt;&lt;/pre&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;FE.get_table(&amp;#x27;emojis&amp;#x27;, return_prop = True)
&lt;/code&gt;&lt;/pre&gt;&lt;table&gt;&lt;thead&gt;&lt;tr&gt;&lt;th align=&quot;center&quot;&gt;emojis&lt;/th&gt;&lt;th align=&quot;center&quot;&gt;0&lt;/th&gt;&lt;th align=&quot;center&quot;&gt;1&lt;/th&gt;&lt;th align=&quot;right&quot;&gt;frekuensi&lt;/th&gt;&lt;th align=&quot;right&quot;&gt;max_prop&lt;/th&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td align=&quot;center&quot;&gt;ðŸ˜‚&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;0.076923&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;0.923077&lt;/td&gt;&lt;td align=&quot;right&quot;&gt;13&lt;/td&gt;&lt;td align=&quot;right&quot;&gt;0.923077&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;center&quot;&gt;ðŸ˜­&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;0.142857&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;0.857143&lt;/td&gt;&lt;td align=&quot;right&quot;&gt;7&lt;/td&gt;&lt;td align=&quot;right&quot;&gt;0.857143&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;center&quot;&gt;ðŸ™&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;0.285714&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;0.714286&lt;/td&gt;&lt;td align=&quot;right&quot;&gt;7&lt;/td&gt;&lt;td align=&quot;right&quot;&gt;0.714286&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;center&quot;&gt;ðŸ˜¢&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;0.166667&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;0.833333&lt;/td&gt;&lt;td align=&quot;right&quot;&gt;6&lt;/td&gt;&lt;td align=&quot;right&quot;&gt;0.833333&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;center&quot;&gt;ðŸ˜&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;0.000000&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;1.000000&lt;/td&gt;&lt;td align=&quot;right&quot;&gt;5&lt;/td&gt;&lt;td align=&quot;right&quot;&gt;1.000000&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;center&quot;&gt;...&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;...&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;...&lt;/td&gt;&lt;td align=&quot;right&quot;&gt;...&lt;/td&gt;&lt;td align=&quot;right&quot;&gt;...&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;center&quot;&gt;ðŸ‡²ðŸ‡¨&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;0.000000&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;1.000000&lt;/td&gt;&lt;td align=&quot;right&quot;&gt;1&lt;/td&gt;&lt;td align=&quot;right&quot;&gt;1.000000&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;center&quot;&gt;â¬&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;0.000000&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;1.000000&lt;/td&gt;&lt;td align=&quot;right&quot;&gt;1&lt;/td&gt;&lt;td align=&quot;right&quot;&gt;1.000000&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;center&quot;&gt;ðŸ˜›&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;0.000000&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;1.000000&lt;/td&gt;&lt;td align=&quot;right&quot;&gt;1&lt;/td&gt;&lt;td align=&quot;right&quot;&gt;1.000000&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;center&quot;&gt;ðŸ˜Ÿ&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;0.000000&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;1.000000&lt;/td&gt;&lt;td align=&quot;right&quot;&gt;1&lt;/td&gt;&lt;td align=&quot;right&quot;&gt;1.000000&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;center&quot;&gt;2âƒ£&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;0.000000&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;1.000000&lt;/td&gt;&lt;td align=&quot;right&quot;&gt;1&lt;/td&gt;&lt;td align=&quot;right&quot;&gt;1.000000&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;pre&gt;&lt;code&gt;71 rows Ã— 5 columns
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Karena jumlah data yang cukup kecil dan sepertinya setiap feature memiliki informasi yang cukup berpengaruh terhadap kelasnya, kami memutuskan untuk tidak menghapus satupun feature yang ada.&lt;/p&gt;&lt;h3 id=&quot;masking-feature-encode&quot;&gt;Masking Feature (Encode)&lt;/h3&gt;&lt;p&gt;Masking feature pada text. Feauture yang sudah di &lt;code&gt;.fit&lt;/code&gt; pada data sebelumnya, proses ini dilakukan karena pada feature ini terdapat peletakan tanda baca yang random / tidak beraturan, sedangkan pada proses berikutnya akan dilakukan penormalisasian tanda baca sehingga akan mengganggu proses tersebut. Maka dari itu perlu dilakukan masking agar proses normalisasi kalimat berjalan dengan lancar.&lt;/p&gt;&lt;pre&gt;&lt;code&gt;# Contoh
Website Google adalah http://google.com/
Trending #MosiTidakPercaya
@jokowi adalah presiden RI
Lucu ðŸ˜‚

# Encode
Website Google adalah MASKURLS1MASK
Trending MASKHASHTAGS1MASK
MASKTAGS1MASK adalah presiden RI
Lucu MASKEMOJIS1MASK
&lt;/code&gt;&lt;/pre&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;# Build Mask Code
FE.build_mask_code(0)
FE_test.build_mask_code(0)
# Apply to data
train_text = FE.encode(train_text)    # Train
test_text = FE_test.encode(test_text) # Test
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&quot;normalisasi-tanda-baca&quot;&gt;Normalisasi Tanda Baca&lt;/h3&gt;&lt;p&gt;Melakukan normalisasi tanda baca pada text. Karena banyak peletakan tanda baca yang salah pada text yang akan mengganggu &lt;code&gt;tokenizer&lt;/code&gt; dalam melakukan segmentasi pada kalimat.&lt;/p&gt;&lt;pre&gt;&lt;code&gt;# Contoh
Budi membayar2.000 ban yang dibelinya senilai rp.2.000.000

# Preprocessed
Budi membayar 2.000 ban yang dibelinya senilai rp. 2.000.000
&lt;/code&gt;&lt;/pre&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;# Add more string punctuation
string.punctuation += &amp;#x27;â€˜â€™â€¦â€œâ€â€“&amp;#x27;
&lt;/code&gt;&lt;/pre&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;def normalize(array):
    &amp;quot;&amp;quot;&amp;quot;
    Normalize text
    &amp;quot;&amp;quot;&amp;quot;
    punc, arr = string.punctuation, array.copy()
    for i in range(len(arr)):
        temp = list(arr[i])
        for j in range(1, len(temp) - 1):
            if (temp[j] in punc) and not\
            all([x in string.digits for x in [temp[j-1], temp[j+1]]]):
                temp[j] = &amp;#x27; &amp;#x27; + temp[j] + &amp;#x27; &amp;#x27;
            elif (temp[j] in string.ascii_lowercase) and (temp[j + 1] \
            in string.digits or temp[j + 1] in string.punctuation):
                temp[j] += &amp;#x27; &amp;#x27;
        arr[i] = &amp;#x27;&amp;#x27;.join(temp)
        arr[i] = &amp;#x27; &amp;#x27;.join(arr[i].split())
    return arr
&lt;/code&gt;&lt;/pre&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;# Apply to text
train_text = normalize(train_text) # Train
test_text = normalize(test_text)   # Test
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&quot;decode--menghapus-tanda-baca&quot;&gt;Decode &amp;amp; Menghapus Tanda Baca&lt;/h3&gt;&lt;p&gt;Men-Decode (mengembalikan feature) serta menghapus tanda baca yang ada pada text.&lt;/p&gt;&lt;pre&gt;&lt;code&gt;# Contoh
Website Google adalah MASKURLS1MASK

# Decode
Website Google adalah http://google.com/

# Remove Punctuation
Website Google adalah httpgooglecom
&lt;/code&gt;&lt;/pre&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;def remove_punc(arr, punc_):
    &amp;quot;&amp;quot;&amp;quot;
    Remove string punctuation
    &amp;quot;&amp;quot;&amp;quot;
    return asarray([x.translate(str.maketrans(&amp;#x27;&amp;#x27;, &amp;#x27;&amp;#x27;, punc_))
                    for x in arr])
&lt;/code&gt;&lt;/pre&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;# Apply to data
train_text = FE.decode(train_text)    # Train
train_text = remove_punc(train_text, string.punctuation)
test_text = FE_test.decode(test_text) # Test
test_text = remove_punc(test_text, string.punctuation)
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&quot;de-emojized&quot;&gt;De-Emojized&lt;/h3&gt;&lt;p&gt;Mengubah kode emoji yang ada pada text menjadi kata kata yang dapat dimengerti.&lt;/p&gt;&lt;pre&gt;&lt;code&gt;ðŸ™ -&amp;gt; folded hands
ðŸ˜ƒ -&amp;gt; grinning face with big eyes
&lt;/code&gt;&lt;/pre&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;def deemojized(arr):
    &amp;quot;&amp;quot;&amp;quot;
    De Emojized text
    &amp;quot;&amp;quot;&amp;quot;
    for i in range(len(arr)):
        arr[i] = emoji.demojize(arr[i])
        arr[i] = re.sub(&amp;#x27;:&amp;#x27;, &amp;#x27; &amp;#x27;, arr[i])
        arr[i] = re.sub(&amp;#x27;_&amp;#x27;, &amp;#x27; &amp;#x27;, arr[i])
        arr[i] = &amp;#x27; &amp;#x27;.join(arr[i].split())
    return arr
&lt;/code&gt;&lt;/pre&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;# Apply to data
train_text = deemojized(train_text) # Train
test_text = deemojized(test_text)   # Test
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&quot;mengecek-kata-yang-misspell-typo&quot;&gt;Mengecek Kata yang Misspell (typo)&lt;/h3&gt;&lt;p&gt;Mengecek kata-kata yang misspell atau typo serta kata - kata singkatan.&lt;/p&gt;&lt;ol&gt;&lt;li&gt;Membangun Vocabulary&lt;br/&gt;
Membangun Vocabulary dari data untuk di cek secara &lt;strong&gt;Manual&lt;/strong&gt;&lt;/li&gt;&lt;/ol&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;# Count Vectorizer
count = CountVectorizer()
count.fit(train_text)
&lt;/code&gt;&lt;/pre&gt;&lt;pre&gt;&lt;code&gt;CountVectorizer()
&lt;/code&gt;&lt;/pre&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;# Vocabulary to DataFrame
vocab = pd.DataFrame({&amp;#x27;Vocab&amp;#x27; : list(count.vocabulary_.keys()),
                     &amp;#x27;Word Index&amp;#x27; : list(count.vocabulary_.values())})
vocab = vocab.sort_values(by=[&amp;#x27;Word Index&amp;#x27;], ascending = False)
vocab[:30].style.hide_index()
&lt;/code&gt;&lt;/pre&gt;&lt;table&gt;&lt;thead&gt;&lt;tr&gt;&lt;th align=&quot;center&quot;&gt;Vocab&lt;/th&gt;&lt;th align=&quot;center&quot;&gt;Word Index&lt;/th&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td align=&quot;center&quot;&gt;ï¾ï¾»&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;20000&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;center&quot;&gt;åŠ æ²¹ jiayou&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;19999&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;center&quot;&gt;à¦¹à§Ÿ&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;19998&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;center&quot;&gt;à¦¸à¦²&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;19997&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;center&quot;&gt;à¦¸à¦¬&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;19996&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;center&quot;&gt;à¦¨à¦®&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;19995&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;center&quot;&gt;à¦¨à¦¬&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;19994&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;center&quot;&gt;à¦§à¦¨&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;19993&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;center&quot;&gt;à¦šà¦²à¦›&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;19992&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;center&quot;&gt;à¦•à¦–&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;19991&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;center&quot;&gt;à¦“à¦†à¦‡à¦¸&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;19990&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;center&quot;&gt;à¦†à¦°&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;19989&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;center&quot;&gt;à¦…à¦¨&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;19988&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;center&quot;&gt;ÙˆØ§Ù†Ø§&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;19987&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;center&quot;&gt;Ù„Ù„Ù‡&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;19986&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;center&quot;&gt;Ù„Ø³&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;19985&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;center&quot;&gt;Ù„Ø§&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;19984&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;center&quot;&gt;Ø±Ø§Ø¬Ø¹ÙˆÙ†&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;19983&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;center&quot;&gt;Ø§Ù†Ø§&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;19982&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;center&quot;&gt;Ø§Ù„ÙŠÙ‡&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;19981&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;center&quot;&gt;Ø§Ù„Ù„Ù‡&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;19980&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;center&quot;&gt;Ø§Ù„Ù„&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;19979&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;center&quot;&gt;Ø§Ù„Ø³&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;19978&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;center&quot;&gt;Ø§Øª&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;19977&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;center&quot;&gt;Å¥Î¯Î´Î±Ä¸&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;19976&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;center&quot;&gt;Ã¬nÃ¬&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;19975&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;center&quot;&gt;Ã¡tÃ¡u&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;19974&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;center&quot;&gt;Ã¡dÃ¡&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;19973&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;center&quot;&gt;zurina&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;19972&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;center&quot;&gt;zumi&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;19971&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;Dapat dilihat dari tabel diatas bahwa data terdiri dari beberapa bahasa karena terdapat beberapa kata yang bukan berasal dari bahasa Indonesia seperti &lt;code&gt;ï¾Ÿï¾ï¾»&lt;/code&gt;, &lt;code&gt;åŠ æ²¹jiayou&lt;/code&gt;, &lt;code&gt;à¦¹à§Ÿ&lt;/code&gt;, &lt;code&gt;à¦¸à¦²&lt;/code&gt;, dll.&lt;/p&gt;&lt;ol start=&quot;2&quot;&gt;&lt;li&gt;Mengeksport Vocabulary &lt;br/&gt;
Mengeksport vocabulary telah dibangun kedalam file &lt;code&gt;.txt&lt;/code&gt; untuk dapat dilakukan pengecekan secara manual terhadap kata-kata tersebut.&lt;/li&gt;&lt;/ol&gt;&lt;pre&gt;&lt;code&gt;f = open(&amp;quot;vocab.txt&amp;quot;, &amp;quot;w&amp;quot;)
f.write(&amp;quot; \n&amp;quot;.join(sorted(vocab.Vocab.values)))
f.close()
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Selanjutnya, mengimport file yang berisi kata - kata yang telah di cek secara manual lalu mengapikasikannya pada data text menggunakan kelas &lt;code&gt;SpellChecker&lt;/code&gt;, fitting didapat dilakukan pada file yang diinginkan atau dapat menggunakan file yang telah di benarkan oleh tim Catatan Cakrawala dengan mengisikan &lt;code&gt;cc-hand-fixed&lt;/code&gt; pada saat fitting class.&lt;/p&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;spellchecker = SpellChecker()
spellchecker.fit(&amp;#x27;cc-hand-fixed&amp;#x27;)
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;code&gt;Transform&lt;/code&gt; untuk apply pada data.&lt;/p&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;# Apply to Data
train_text = spellchecker.transform(train_text) # Train
test_text = spellchecker.transform(test_text)   # Test
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&quot;memhapus-kata-yang-kurang-penting--stopwords&quot;&gt;Memhapus Kata yang Kurang Penting &amp;amp; Stopwords&lt;/h3&gt;&lt;p&gt;Menghapus kata - kata yang kurang penting seperti kata yang hanya terdiri dari 1 karakter dan Stopwords. Untuk Stopwords kami menggabungkan stopwords bahasa Indonesia dari &lt;code&gt;Spacy&lt;/code&gt; dan &lt;code&gt;Sastrawi&lt;/code&gt; serta bahasa Inggris dari &lt;code&gt;Spacy&lt;/code&gt;.&lt;/p&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;from spacy.lang.id.stop_words import STOP_WORDS as ID
from spacy.lang.en.stop_words import STOP_WORDS as EN
from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory

stopwords = set(StopWordRemoverFactory().get_stop_words())
stopwords.update(EN)
stopwords.update(ID)
&lt;/code&gt;&lt;/pre&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;def RUnecesarry(array, stopwords):
    &amp;quot;&amp;quot;&amp;quot;
    Remove Unnecessary words
    &amp;quot;&amp;quot;&amp;quot;
    arr = array.copy()
    for i in range(len(arr)):
        temp = arr[i].split()
        temp = [x for x in temp if not (len(x) == 1 and \
                (x in string.ascii_lowercase or x in string.digits))]
        temp = [x for x in temp if x not in stopwords]
        arr[i] = &amp;#x27; &amp;#x27;.join(temp).lower()
    return arr
&lt;/code&gt;&lt;/pre&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;# Apply to data
train_text = RUnecesarry(train_text, stopwords) # Train
test_text = RUnecesarry(test_text, stopwords)   # Test
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Melihat hasil dari tahap preprocess data text&lt;/p&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;for i in [0, 803, 1002]:
    print(f&amp;quot;Actual : \n{train.text.values[i]}\n\nPreprocessed : \n{train_text[i]}&amp;quot;)
    print(&amp;#x27;&amp;#x27;.rjust(80, &amp;#x27;-&amp;#x27;))
&lt;/code&gt;&lt;/pre&gt;&lt;pre&gt;&lt;code&gt;Actual :
pemakaian masker menyebabkan penyakit legionnaires a caller to a radio talk show recently shared that his wife was hospitalized n told she had covid n only a couple of days left to live . a doctor friend suggested she be tested for legionnaires disease because she wore the same mask every day all day long . turns out it was legionnaires disease from the moisture n bacteria in her mask . she was given antibiotics n within two days was better . what if these â€˜spikesâ€™ in covid are really something else due to â€˜mask induced infectionsâ€™ .??ðŸ¤”ðŸ¤”ðŸ¤”

Preprocessed :
pemakaian masker menyebabkan penyakit legionnaires caller radio talk recently shared wife hospitalized told covid couple days left live doctor friend suggested tested legionnaires disease wore mask day day long turns legionnaires disease moisture bacteria mask given antibiotics days better spikes covid mask induced infections thinking face thinking face thinking face
--------------------------------------------------------------------------------
Actual :
kk dan nik anda bisa dipakai orang lain, waspadalah! bingung kan? speechless lah. . https://www.jpnn.com/kemenpan/news/kk-dan-nik-anda-bisa-dipakai-orang-lain-waspadalah?page=1

Preprocessed :
kk nik dipakai orang waspadalah bingung speechless httpswwwjpnncomkemenpannewskkdannikandabisadipakaioranglainwaspadalahpage1
--------------------------------------------------------------------------------
Actual :
ini gunung dieng salatiga lihat baik2..ini bukan gn.fuji.di jepangâ€¦bukan juga pegunungan alpen di eropaâ€¦tapi ini gn.dieng salatigaâ€¦indonesia..suhu -9Â°c.

Preprocessed :
gunung dieng salatiga lihat gn fuji jepang pegunungan alpen eropa gn dieng salatiga indonesia suhu 9Â°c
--------------------------------------------------------------------------------
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&quot;explorasi-data&quot;&gt;Explorasi Data&lt;/h2&gt;&lt;p&gt;Melakukan explorasi pada data untuk mendapatkan informasi - informasi lebih jauh dari data.&lt;/p&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;train.info()
&lt;/code&gt;&lt;/pre&gt;&lt;pre&gt;&lt;code&gt;&amp;lt;class &amp;#x27;pandas.core.frame.DataFrame&amp;#x27;&amp;gt;
RangeIndex: 4228 entries, 0 to 4227
Data columns (total 8 columns):
 #   Column            Non-Null Count  Dtype
---  ------            --------------  -----
 0   index             4228 non-null   int64
 1   ID                4228 non-null   int64
 2   label             4228 non-null   int64
 3   tanggal           4228 non-null   object
 4   judul             4228 non-null   object
 5   narasi            4228 non-null   object
 6   nama file gambar  4228 non-null   object
 7   text              4228 non-null   object
dtypes: int64(3), object(5)
memory usage: 264.4+ KB
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Merubah tipe pada kolom tanggal menjadi &lt;code&gt;datetime&lt;/code&gt;. Kolom tanggal tidak dapat langsung dirubah tipe datanya menjadi &lt;code&gt;datetime&lt;/code&gt; dikarenakan ada nya beberapa element yang mengandung kata &lt;code&gt;Okt&lt;/code&gt; dan &lt;code&gt;Agu&lt;/code&gt; sehingga harus diubah menjadi &lt;code&gt;Oct&lt;/code&gt; dan &lt;code&gt;Aug&lt;/code&gt;&lt;/p&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;def FixMonth(text):
    &amp;quot;&amp;quot;&amp;quot;
    Fungsi untuk merubah kata &amp;#x27;Okt&amp;#x27; menjadi &amp;#x27;Oct&amp;#x27; dan &amp;#x27;Agu&amp;#x27; menjadi &amp;#x27;Aug&amp;#x27;
    &amp;quot;&amp;quot;&amp;quot;
    if type(text) == str:
        if &amp;#x27;Okt&amp;#x27; in text: return text[:3]+&amp;#x27;Oct&amp;#x27;+text[6:]
        else: return text[:3]+&amp;#x27;Aug&amp;#x27;+text[6:]
    else:
        return text
&lt;/code&gt;&lt;/pre&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;# Apply to data
train[&amp;#x27;tanggal&amp;#x27;] = train[&amp;#x27;tanggal&amp;#x27;].apply(FixMonth)
train[&amp;#x27;tanggal&amp;#x27;] = pd.to_datetime(train[&amp;#x27;tanggal&amp;#x27;])
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&quot;mengecek-frekuensi-setiap-kelas&quot;&gt;Mengecek Frekuensi Setiap Kelas&lt;/h3&gt;&lt;p&gt;Menegecek frekuensi tiap kelasnya pada data.&lt;/p&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;fig_1 = go.Figure(go.Pie(labels=[&amp;#x27;Bukan Hoax&amp;#x27;, &amp;#x27;Hoax&amp;#x27;],
                       values=train.groupby(&amp;#x27;label&amp;#x27;).size().values,
                       textinfo=&amp;#x27;percent+label+value&amp;#x27;,
                       textfont_color=&amp;#x27;#ffffff&amp;#x27;,
                       marker_colors=[&amp;#x27;#0db7c5&amp;#x27;,&amp;#x27;#d03850&amp;#x27;])
               )
fig_1.update_layout(
    title={
        &amp;#x27;text&amp;#x27;: &amp;quot;Frekuensi Perkelasnya&amp;quot;,
        &amp;#x27;y&amp;#x27;:0.93,
        &amp;#x27;x&amp;#x27;:0.5,
        &amp;#x27;font_size&amp;#x27;:23},
    width=550, height=550
)

fig_1.show()
&lt;/code&gt;&lt;/pre&gt;&lt;div style=&quot;margin-bottom:32px&quot;&gt;&lt;canvas style=&quot;min-height:400px&quot; height=&quot;150&quot; width=&quot;300&quot; data-testid=&quot;canvas&quot; role=&quot;img&quot;&gt;&lt;/canvas&gt;&lt;/div&gt;&lt;p&gt;Dapat dilihat bahwa kelas 1 (&lt;code&gt;Hoax&lt;/code&gt;) jauh lebih banyak dari kelas 0 (&lt;code&gt;Bukan Hoax&lt;/code&gt;) atau terdapat &lt;code&gt;imbalance class&lt;/code&gt; pada data. Untuk penanganannya kami menggunakan &lt;code&gt;Stratify&lt;/code&gt; saat melakukan &lt;code&gt;Cross Validation&lt;/code&gt; sehingga proporsi pada subset &lt;code&gt;latih&lt;/code&gt; dan pada subset &lt;code&gt;validasi&lt;/code&gt; sama.&lt;/p&gt;&lt;h3 id=&quot;melihat-sebaran-kelas-per-waktunya&quot;&gt;Melihat Sebaran Kelas per Waktunya&lt;/h3&gt;&lt;p&gt;Melihat sebaran kelas perwaktunya.&lt;/p&gt;&lt;ol&gt;&lt;li&gt;Membuat kolom baru untuk menyimpan tahun &amp;amp; bulan&lt;/li&gt;&lt;li&gt;Mengelompokkan berdasarkan tahun, bulan, dan labelnya&lt;/li&gt;&lt;/ol&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;# Membuat Kolom baru untuk menyimpan nilai tahun dan bulan
train[&amp;#x27;year&amp;#x27;] = [i.year for i in train.tanggal]
train[&amp;#x27;month&amp;#x27;] = [i.month for i in train.tanggal]
# Grouping
data_waktu = (train.groupby([&amp;#x27;year&amp;#x27;, &amp;#x27;month&amp;#x27;, &amp;#x27;label&amp;#x27;]).size().reset_index()
              .pivot_table(columns=&amp;#x27;label&amp;#x27;, index=[&amp;#x27;year&amp;#x27;,&amp;#x27;month&amp;#x27;], values=0))
&lt;/code&gt;&lt;/pre&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;fig_2 = go.Figure(data=[
    go.Bar(name=&amp;#x27;Bukan Hoax&amp;#x27;,
           x=[str(i) for i in data_waktu.index],
           y=data_waktu[0].values,
           marker_color=&amp;#x27;#0db7c5&amp;#x27;),
    go.Bar(name=&amp;#x27;Hoax&amp;#x27;,
           x=[str(i) for i in data_waktu.index],
           y=data_waktu[1].values,
           marker_color=&amp;#x27;#d03850&amp;#x27;)
])

fig_2.update_layout(barmode=&amp;#x27;stack&amp;#x27;,
                  xaxis=dict(dtick=1,
                             showgrid=False,
                             title=&amp;#x27;(Tahun, Bulan)&amp;#x27;),
                  legend=dict(x=0.006,y=0.97,
                              bgcolor=&amp;#x27;rgba(255,255,255,0)&amp;#x27;,
                              bordercolor=&amp;#x27;rgba(0,0,0,0)&amp;#x27;,
                              font_color=&amp;#x27;#090919&amp;#x27;,
                              font_size=14),
                  title=dict(text=&amp;#x27;Sebaran Hoax per Waktu&amp;#x27;,
                             x=0.5,
                             y=0.9,
                             font_size=23),
                  width=1000,height=550
                 )

fig_2.show()
&lt;/code&gt;&lt;/pre&gt;&lt;div style=&quot;margin-bottom:32px&quot;&gt;&lt;canvas style=&quot;min-height:400px&quot; height=&quot;150&quot; width=&quot;300&quot; data-testid=&quot;canvas&quot; role=&quot;img&quot;&gt;&lt;/canvas&gt;&lt;/div&gt;&lt;p&gt;Ditinjau dari grafik diatas bahwa:&lt;/p&gt;&lt;ol&gt;&lt;li&gt;53% dari keseluruhan data berasal dari tahun 2019 - 2020&lt;/li&gt;&lt;li&gt;95% data pada tahun 2019 - 2020 adalah Hoax&lt;/li&gt;&lt;/ol&gt;&lt;h3 id=&quot;mengecek-kata---kata-yang-muncul-perkelasnya&quot;&gt;Mengecek Kata - Kata yang Muncul Perkelasnya&lt;/h3&gt;&lt;p&gt;Mengecek frekuensi kata - kata yang muncul berdasarkan kelasnya.&lt;/p&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;def find_words(texts, label, builder = CountVectorizer(min_df=3, max_df=0.9,
                                                       ngram_range=(1,2))):
    &amp;quot;&amp;quot;&amp;quot;
    Memeriksa kalimat berdasarkan kelasnya untuk membuat vocab
    &amp;quot;&amp;quot;&amp;quot;
    builder.fit(texts)
    n_class, res = len(set(label)), {}
    for i in tqdm(range(len(texts))):
        for vocab in texts[i].split():
            if vocab in builder.vocabulary_:
                if vocab not in res:
                    res[vocab] = [0] * n_class + [0]
                res[vocab][label[i]] += 1
                res[vocab][-1] += 1
    df = pd.DataFrame({&amp;#x27;kata&amp;#x27; : list(res.keys())})
    for i in range(n_class):
        df[f&amp;#x27;Kelas_{i}&amp;#x27;] = [res[x][i] for x in res]
    df[&amp;#x27;Frekuensi&amp;#x27;] = [res[x][-1] for x in res]
    return df.sort_values(by = [&amp;#x27;Frekuensi&amp;#x27;], ascending = False).reset_index(drop=True)
&lt;/code&gt;&lt;/pre&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;# Vocab DF per class
words = find_words(train_text, train.label.values)
words[&amp;#x27;Max_prop&amp;#x27;] = words[[&amp;#x27;Kelas_0&amp;#x27;, &amp;#x27;Kelas_1&amp;#x27;]].max(axis = 1) / words[&amp;#x27;Frekuensi&amp;#x27;]
words.head(10)
&lt;/code&gt;&lt;/pre&gt;&lt;table&gt;&lt;thead&gt;&lt;tr&gt;&lt;th align=&quot;center&quot;&gt;kata&lt;/th&gt;&lt;th align=&quot;center&quot;&gt;Kelas_0&lt;/th&gt;&lt;th align=&quot;center&quot;&gt;Kelas_1&lt;/th&gt;&lt;th align=&quot;center&quot;&gt;Frekuensi&lt;/th&gt;&lt;th align=&quot;center&quot;&gt;Max_prop&lt;/th&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td align=&quot;center&quot;&gt;indonesia&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;160&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;617&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;777&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;0.794080&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;center&quot;&gt;jokowi&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;93&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;522&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;615&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;0.848780&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;center&quot;&gt;foto&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;79&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;446&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;525&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;0.849524&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;center&quot;&gt;orang&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;66&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;458&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;524&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;0.874046&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;center&quot;&gt;video&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;62&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;378&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;440&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;0.859091&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;center&quot;&gt;corona&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;15&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;402&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;417&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;0.964029&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;center&quot;&gt;anak&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;62&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;342&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;404&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;0.846535&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;center&quot;&gt;virus&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;21&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;378&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;399&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;0.947368&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;center&quot;&gt;covid&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;24&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;342&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;366&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;0.934426&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;center&quot;&gt;china&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;33&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;324&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;357&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;0.907563&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;Berikut adalah Barplot untuk kata - kata pada kelas &lt;code&gt;1&lt;/code&gt; yang mempunyai frekuensi kumunculan lebih dari kelas &lt;code&gt;0&lt;/code&gt;.&lt;/p&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;_1 = words[(words[&amp;#x27;Kelas_1&amp;#x27;] &amp;gt; words[&amp;#x27;Kelas_0&amp;#x27;])]

plt.figure(figsize = (12,7))
sns.barplot(x = &amp;#x27;kata&amp;#x27;, y = &amp;#x27;Frekuensi&amp;#x27;, data = _1[:25])
plt.title(&amp;#x27;Frekuensi Kata yang Sering Muncul Pada Label Hoax&amp;#x27;, fontsize = 14)
plt.xticks(rotation = 90)
plt.show()
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;span class=&quot;gatsby-resp-image-wrapper&quot; style=&quot;position:relative;display:block;margin-left:auto;margin-right:auto;max-width:630px&quot;&gt;
      &lt;a class=&quot;gatsby-resp-image-link&quot; href=&quot;/static/96cb2de7a1c7de3184a2aa71d9760734/c54b3/output_96_0.png&quot; style=&quot;display:block&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;
    &lt;span class=&quot;gatsby-resp-image-background-image&quot; style=&quot;padding-bottom:67.72151898734178%;position:relative;bottom:0;left:0;background-image:url(&amp;#x27;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAOCAYAAAAvxDzwAAAACXBIWXMAAAsSAAALEgHS3X78AAADAklEQVQ4y51R32tTZxj+ekpVGPsnduHV7hSUMRjbxXa9ChPBi+qaNW1Cf220DLQaexS05GrYdKY1Zkv6I+akUddf8ZwkjRQrQrWlNLVqbJesWZv0hCRqbL5zvmd8aSLtusHYAw/v+37vy8Pzvh+ZnZ09nMlkTqmqWpvNZmsTicRJWZYbw+FwQygUMvKoKEojzyORyHc88pozFoudyuVytaqqnshkMifj8fhR8swttRTi62AAdE0DYzz7b+Czuq6XyPNsNusi0R6bqfAqzvvvmK7pXJftgMdKvZuVXslDmUUukM/nb5Jor9OcuS+j/MiYrrH/AcoFcrmcgzyzD5lTv/6C7Hyg+GZzpbILV4ZeInt/hn87B2NMKzt0kOUbQ+b00CAS7o7iS7kHr/Mb0LTi/luVuVd8p94j+Mrhbdx0u5CUuosLUgum77ZgSj6HyIITz9Xn+D2X2ueoIsz099T4r5ZXHjanBl1Y91qKi6PfIzjaBI9Uh97xb9EZ7IBBvo6BhRk83Ujij1y+JPYP2HFYeOMgK/0jppS7ItjOQn4T891pYP0BM7sQ6WJGxcYapoaYYeIOa5MfsKsPl9ndRZWtrL1lmT+32dtkkW0nCxpNv4YaS5RuaOIOk15LYXG0nQb9Jir5DdQ+1US7ps9To9JHWxUvbVMmaHNgmjZOPqFX5ASVAmk6I6VobDBNU8617a2ROaz2/TZAVvqH27nDpNeCxdF2BP0m+PwG2Kea0DV9HkbFhmb5NprlcbQEImiV52ENr+NeSMWje1tY9Wwh7VqD6nmC1Z/HhsmMte/4ku269an9h0vhm0ZRunVGHHCcFq3DdWKbr/Vyna/nUv1tu1jvcYv1I37R4Al2X/TNdzul5e5xZ1R81LckRnvnLNEb96/NWQe/JoSQagBkPzdK8SdE+Yywu0cIOfD3+TKqyFdHjtVMWi7WAI8FFF1C9OGP1YBN2IBDGMeYoADC4S8+/eCjT45/CKDqy07xwGffGA+9SKMaKqqxCQEUwtWznQc///joob8AhQl8YbJ5REIAAAAASUVORK5CYII=&amp;#x27;);background-size:cover;display:block&quot;&gt;&lt;/span&gt;
  &lt;img class=&quot;gatsby-resp-image-image&quot; alt=&quot;png&quot; title=&quot;png&quot; src=&quot;/static/96cb2de7a1c7de3184a2aa71d9760734/f058b/output_96_0.png&quot; srcSet=&quot;/static/96cb2de7a1c7de3184a2aa71d9760734/c26ae/output_96_0.png 158w,/static/96cb2de7a1c7de3184a2aa71d9760734/6bdcf/output_96_0.png 315w,/static/96cb2de7a1c7de3184a2aa71d9760734/f058b/output_96_0.png 630w,/static/96cb2de7a1c7de3184a2aa71d9760734/c54b3/output_96_0.png 727w&quot; sizes=&quot;(max-width: 630px) 100vw, 630px&quot; style=&quot;width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0&quot; loading=&quot;lazy&quot; decoding=&quot;async&quot;/&gt;
  &lt;/a&gt;
    &lt;/span&gt;&lt;/p&gt;&lt;p&gt;Plotting WordCloud&lt;/p&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;from wordcloud import WordCloud, ImageColorGenerator

__1 = {}
for i in range(len(_1)):
    __1[_1.kata.values[i]] = _1.Max_prop.values[i]

wordcloud = WordCloud(width = 4000, height = 3000, min_font_size = 5,
                      background_color = &amp;#x27;white&amp;#x27;, colormap = &amp;#x27;inferno&amp;#x27;).fit_words(__1)
plt.figure(figsize = (10, 8))
plt.imshow(wordcloud)
plt.axis(&amp;quot;off&amp;quot;)
plt.tight_layout(pad = 0)
plt.show()
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;span class=&quot;gatsby-resp-image-wrapper&quot; style=&quot;position:relative;display:block;margin-left:auto;margin-right:auto;max-width:630px&quot;&gt;
      &lt;a class=&quot;gatsby-resp-image-link&quot; href=&quot;/static/1876f08eb72ea6309e3d379ab9a70269/c6d67/output_98_0.png&quot; style=&quot;display:block&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;
    &lt;span class=&quot;gatsby-resp-image-background-image&quot; style=&quot;padding-bottom:75.31645569620254%;position:relative;bottom:0;left:0;background-image:url(&amp;#x27;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAPCAYAAADkmO9VAAAACXBIWXMAAAsSAAALEgHS3X78AAAEBklEQVQ4ywXBWXPaVgAGUP3KzvSxT50+tU91J9OmaZPMtI6TNPGkE+I4NN4AF1wwiy1ksECAJQESshBCZhGLwBAM9jheWO7N13OYsTOIZPbSNu/LmrOLrjW/PLe494J15M5YN6OuZSumFViOW6mNtNWuNq1Bp2NxrwMWHxAtet+3ru0Di1woFeLwXQwSbqYi1qzEThLvH/ighDMweQW7yxEEV4JQIgIGZQ3p7RjcS9vgIzIONjOI/hXDzuN9NKQC7vpFOJaKrplBrybzTMvon04GY/hfJRdqKEn3l320oWh0qEp0fSlIJ/UW/XJ7Tn1PQzQZytP1h356ZRn0aJOn3IZI83mNfvzgm+dEFVa9xTK3Q0cvhGX4VyIU6MPzyAtb0/FJL8L1vQfk5hK85wTRtSSGFz24ljy4qhngvRnsv2FxOerAOGZJM5tE3yxzTKOg6U+/coFfP6Z9WUJuL42954cIv4ggsy2ixJl48vU61KMCrp1TcG4e0VeH2HoUxMFWFo1KB3xAIulgARKnc4wWl/WQi0PkHUurJwqmTgvRdwl8XN7HqFGFGY7jaNUL9vkOHFHGjV1F9E0UYoDFqNeCIRSgJvJES5moimccc+ZL6BNNxWLSoROjjEYwgWZWg1EsoxpPYdFRcatmcNe1YEtlOJqJi04dXV2BrdkYVQyM9RzplUo4N7ocU/XxenVXgPinn5pbUVS2w7DZPGrBAk7dPGwuA8ufRiNn4IQ1YEh1aBkL6YiGZEBCNqog7skSMVZEp9TimGq0oFdCEjq5Cm3yRYwrLVyWZEwMBZ9yKcwdGROlhPS/ObRkDU3ZQDlfR11vQOJO0TdbGKsGua/XMb264Zi4X9I3X0QR+YenDdVGNiyjcBjH1dBGLS/hyPsfDFmF520cbVVBgRUwbA/BBwXkORlkPIAaFUncnUS70uWY1w99+srSFlZ/8dJULI/IloAH36whvV+A67Ef3tdxlFUD6y8DePnjJsriGWIbKTz7wY3fv3Xh5EDEyk9bRC/qGDYcjnn/5KPufuaB580xXVv2wuti8fbpLkJuHkO7j7U/gtjdOMDaSgC/fbeOsljHzl8xrP7sBb8noZTUsPrrDgEc0M8Djgl8CGsx7yGywdRcSYnE5wqS3b+jxMxqJBlIk7XHXnJ8kCbC4QlpaBZhfcdkYDaI91mAhFwcaao2CX5IzO5nbXyZX7PMePS5MR2fAbcCMM0BMw2YmpiNFUzHeZCrFObXJ5hfK7gbi/hynQVGeaDHYt5NYuZkQc953HbCIGNFYOh8ukGmPWmxcNKLuSPMFy3hvlcS7mq6MOuUhUm9ItxfnQqTdklwtDNhVK0Kn526cKZJgnVaFAbtqjDt8KnzWqnQa9Ze/g+nmNnkvonmxQAAAABJRU5ErkJggg==&amp;#x27;);background-size:cover;display:block&quot;&gt;&lt;/span&gt;
  &lt;img class=&quot;gatsby-resp-image-image&quot; alt=&quot;png&quot; title=&quot;png&quot; src=&quot;/static/1876f08eb72ea6309e3d379ab9a70269/f058b/output_98_0.png&quot; srcSet=&quot;/static/1876f08eb72ea6309e3d379ab9a70269/c26ae/output_98_0.png 158w,/static/1876f08eb72ea6309e3d379ab9a70269/6bdcf/output_98_0.png 315w,/static/1876f08eb72ea6309e3d379ab9a70269/f058b/output_98_0.png 630w,/static/1876f08eb72ea6309e3d379ab9a70269/c6d67/output_98_0.png 734w&quot; sizes=&quot;(max-width: 630px) 100vw, 630px&quot; style=&quot;width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0&quot; loading=&quot;lazy&quot; decoding=&quot;async&quot;/&gt;
  &lt;/a&gt;
    &lt;/span&gt;&lt;/p&gt;&lt;p&gt;Berikut adalah Barplot untuk kata - kata pada kelas &lt;code&gt;0&lt;/code&gt; yang mempunyai frekuensi kumunculan lebih dari kelas &lt;code&gt;1&lt;/code&gt;.&lt;/p&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;_0 = words[(words[&amp;#x27;Kelas_0&amp;#x27;] &amp;gt; words[&amp;#x27;Kelas_1&amp;#x27;])]

plt.figure(figsize = (12,7))
sns.barplot(x = &amp;#x27;kata&amp;#x27;, y = &amp;#x27;Frekuensi&amp;#x27;, data = _0[:25])
plt.title(&amp;#x27;Frekuensi Kata yang Sering Muncul Pada Label Bukan Hoax&amp;#x27;, fontsize = 14)
plt.xticks(rotation = 90)
plt.show()
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;span class=&quot;gatsby-resp-image-wrapper&quot; style=&quot;position:relative;display:block;margin-left:auto;margin-right:auto;max-width:630px&quot;&gt;
      &lt;a class=&quot;gatsby-resp-image-link&quot; href=&quot;/static/ecc25ab3ac5bb4601c3792db414a3d0d/c54b3/output_100_0.png&quot; style=&quot;display:block&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;
    &lt;span class=&quot;gatsby-resp-image-background-image&quot; style=&quot;padding-bottom:70.25316455696203%;position:relative;bottom:0;left:0;background-image:url(&amp;#x27;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAOCAYAAAAvxDzwAAAACXBIWXMAAAsSAAALEgHS3X78AAACdklEQVQ4y52SXUiTURiA36mVURcFXUR0o21aBJXRReBNJamBlYmlZRlUhE7KdhEaZjb7kZCw1CBXOMHNrakYlBBiyITCvzKiwJ/57ZvfsXC56dr8ZFvfeevsD4IM5gsP5z3n5TycnxfE6Rn4NjZx0D2/ULcoLt5xuVw1PM/XW63WCBzHNUxNTTXwPP+YzS0WSyPLOY6rt9vttaIo3nW73fc9Hs89QDfCpNb4wD87hywopRhtsD1hQHzYDLanrRU+8p3Vliilv6IFEf1hAIkE40+01f5ZBxP6KA2cka40ABt1YNPoKz2DgwHhSq8dDgCA1VZNW5XjhQ5F66egUJKYNfIuUQkZQktX5Q99Mzr7tBFhWBStFLB7GCwaw22nSYczXdU+p/AhJJFQikD/+snQI/8TQONb4J4ZbjlMehQMZd6RjhJp9F2DhIjLQildFkBjL/DPjWqnSY8zpgr82FWKvS+V2D/wCM1j7dhPBnD65xzOiYvoXFpCj8///yu76lpgskmvshtaPba2Mn64s5S86Swiuo5CUtt9gSh7bpIrvVpyo++1UGU2E7V5mGhGbKRn1EE+jy4QYcRFnAPzgmuICK6hCQIVR3LgcsbRTddzT2wvP30g8eq51MSCk/uSzl88tC33UpoiXXk8cWd+RlJCdmZyUl6efHP2Gfn6w4WKlCyVPDNNpSjYf02uTFEqinefSlbtzZKztgEct7NBBgCrvF9qAmvtr4rYEA8AcQCwjtVCxIeIRW+g7TayerhjgkJvcBJgoUkWzPtAWXNMllGeE5NeWRyzYeuWtfLU1Lg9OfmxJab3sq8U2QFgR8KuNeqz6piA54/wN/NnYSTwOgUGAAAAAElFTkSuQmCC&amp;#x27;);background-size:cover;display:block&quot;&gt;&lt;/span&gt;
  &lt;img class=&quot;gatsby-resp-image-image&quot; alt=&quot;png&quot; title=&quot;png&quot; src=&quot;/static/ecc25ab3ac5bb4601c3792db414a3d0d/f058b/output_100_0.png&quot; srcSet=&quot;/static/ecc25ab3ac5bb4601c3792db414a3d0d/c26ae/output_100_0.png 158w,/static/ecc25ab3ac5bb4601c3792db414a3d0d/6bdcf/output_100_0.png 315w,/static/ecc25ab3ac5bb4601c3792db414a3d0d/f058b/output_100_0.png 630w,/static/ecc25ab3ac5bb4601c3792db414a3d0d/c54b3/output_100_0.png 727w&quot; sizes=&quot;(max-width: 630px) 100vw, 630px&quot; style=&quot;width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0&quot; loading=&quot;lazy&quot; decoding=&quot;async&quot;/&gt;
  &lt;/a&gt;
    &lt;/span&gt;&lt;/p&gt;&lt;p&gt;Plotting WordCloud&lt;/p&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;__0 = {}
for i in range(len(_0)):
    __0[_0.kata.values[i]] = _0.Max_prop.values[i]

wordcloud = WordCloud(width = 4000, height = 3000, min_font_size = 5,
                      background_color = &amp;#x27;white&amp;#x27;, colormap = &amp;#x27;Blues&amp;#x27;).fit_words(__0)

plt.figure(figsize = (10, 8))
plt.imshow(wordcloud)
plt.axis(&amp;quot;off&amp;quot;)
plt.tight_layout(pad = 0)
plt.show()
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;span class=&quot;gatsby-resp-image-wrapper&quot; style=&quot;position:relative;display:block;margin-left:auto;margin-right:auto;max-width:630px&quot;&gt;
      &lt;a class=&quot;gatsby-resp-image-link&quot; href=&quot;/static/9433af17f0a44aa445d6f83f2f3d798c/c6d67/output_102_0.png&quot; style=&quot;display:block&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;
    &lt;span class=&quot;gatsby-resp-image-background-image&quot; style=&quot;padding-bottom:75.31645569620254%;position:relative;bottom:0;left:0;background-image:url(&amp;#x27;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAPCAYAAADkmO9VAAAACXBIWXMAAAsSAAALEgHS3X78AAADqklEQVQ4yyVSa28bVRDd/44EfCpfAAkqBGpFaCFtKTRpE6dJk21q5217HcfXa3vf792775d9bw5a82GkeRzNnJkzQlG17/auNMWk5cPSz4mb1uRYssnelUbOJjZZ+gUZmQn5LAdk5uZksAiJQUsSlS2h5ZpERUu8rJm6cWWsN/xXIcwb8sP+FP2Zhx/fT/FCXEHSKA7vDPzSm+FVX8XnqYsX4hI7Zwt8vTvE+1sD7BHIqs3WqjVHs+HY8MdPArGScQe+XoSbp4cz3hvb/MPQ4r8dz/nu+Yr/cbrgpxOH735R+Ou+yv+91Pi7a53Lbs6JnfKpmXCTVmuLVjCi8kB40GPJCHJYtOJJuYYaFBhqMW5VCsXPMbeTDrjN+1kDL21gRBVGWozrVQRJjzHWKLtdRdDDsidcyb7kxBVkL+cGrWHQCmpYQt/6NdysQVi0sJIaC6+AEpbw8xY2LXE59zFSI5hhwbq4bDY9Yerm0shMcWukfOYXIF6BZVhCMhNotISddqxqLP0Cl3qMRVhC6YbGNfSkhpk1mLo5K1uGR6AnDOaB9HFsw4kr3ic+VL/Ah6EFYiXYOV3gRqHojWzcmwlmToaBHOBOpdi7s7DyMogPXmdMnLpw46on/Hwwk56dyPjpwwM/Gln4fn+Kp4cEB0MLT95K0MMC7+9MXMw8nE0cfPtqhJfiCs9PZOx8kvHVyzu87ivsm90h7nXaE568laS/vii4WYSdkvjnSkPXeEB8vDxXtmzeXul4PVDx5kLD89PlltVYj9Hhfz9b4uPEYft3Jk4mTk+4XgRSmLdgHFzxsq2ydljACnI8OBmmdgo1KjE0EhAvh2RnkIMCWlyBVmuotIKTNcxMm87vCUW7kcK8QVyseVFv0Kw5Fn6BExKgbhkUv4AW/t/wRovhJDU6EQcKxfkyQpA3uDUSpoYFgqzuCVMrlWQngx4UnBYt2s0jbvQEexMX56sIx/MAf95YWAYFvixC6EGOjyTA8TzE/r2Hv0cOzleUnS9C1O2mJ7hxOYq7h3bi9eW9zi7GCruYu+yQhGygUHZEAvZJjtjcL9nR1GMTI2Zvhg7rLyM21BN2KofseOa3/WWEes0PhKRsl90Nk2qDIK1hdfcLC0hqCJcW0LwUupducyat0G1j0ho2reElDfKaQVIjLOwEecNEgZbrZ37WXAR5exbkrZjWTIyLVhzLjmj5qTiUHfGzpIuyEYnNmm9rab0RvawVs3oj5jUT82p9mpTtddmw7/4Dlc0yhAnD38YAAAAASUVORK5CYII=&amp;#x27;);background-size:cover;display:block&quot;&gt;&lt;/span&gt;
  &lt;img class=&quot;gatsby-resp-image-image&quot; alt=&quot;png&quot; title=&quot;png&quot; src=&quot;/static/9433af17f0a44aa445d6f83f2f3d798c/f058b/output_102_0.png&quot; srcSet=&quot;/static/9433af17f0a44aa445d6f83f2f3d798c/c26ae/output_102_0.png 158w,/static/9433af17f0a44aa445d6f83f2f3d798c/6bdcf/output_102_0.png 315w,/static/9433af17f0a44aa445d6f83f2f3d798c/f058b/output_102_0.png 630w,/static/9433af17f0a44aa445d6f83f2f3d798c/c6d67/output_102_0.png 734w&quot; sizes=&quot;(max-width: 630px) 100vw, 630px&quot; style=&quot;width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0&quot; loading=&quot;lazy&quot; decoding=&quot;async&quot;/&gt;
  &lt;/a&gt;
    &lt;/span&gt;&lt;/p&gt;&lt;h2 id=&quot;pemodelan&quot;&gt;Pemodelan&lt;/h2&gt;&lt;p&gt;Membuat model untuk memprediksi kelas &lt;code&gt;0&lt;/code&gt; (&lt;code&gt;tidak hoax&lt;/code&gt;) dan kelas &lt;code&gt;1&lt;/code&gt; (&lt;code&gt;hoax&lt;/code&gt;) pada data test.&lt;/p&gt;&lt;h3 id=&quot;accelerator-detection&quot;&gt;Accelerator Detection&lt;/h3&gt;&lt;p&gt;Menggunakan GPU atau TPU dari Kaggle sebagai Accelerator.&lt;/p&gt;&lt;p&gt;Note : Pastikan untuk menyalakan GPU / TPU sebagai Accelator, untuk effisiensi waktu pelatihan model.&lt;/p&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;try:
    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()
    print(&amp;#x27;Running on TPU &amp;#x27;, tpu.master())
except ValueError:
    tpu = None

if tpu:
    tf.config.experimental_connect_to_cluster(tpu)
    tf.tpu.experimental.initialize_tpu_system(tpu)
    strategy = tf.distribute.experimental.TPUStrategy(tpu)
    BATCH_SIZE = 16 * strategy.num_replicas_in_sync
else:
    strategy = tf.distribute.get_strategy()
    BATCH_SIZE = 32 * strategy.num_replicas_in_sync

AUTO = tf.data.experimental.AUTOTUNE
print(&amp;quot;REPLICAS: &amp;quot;, strategy.num_replicas_in_sync)
&lt;/code&gt;&lt;/pre&gt;&lt;pre&gt;&lt;code&gt;Running on TPU  grpc://10.0.0.2:8470
REPLICAS:  8
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&quot;pemodelan-data-gambar-efficientnet&quot;&gt;Pemodelan Data Gambar (EfficientNet)&lt;/h2&gt;&lt;p&gt;Model yang digunakan untuk data gambar yaitu pretrained Deep Learning model yang cukup popular yaitu &lt;code&gt;EfficientNet&lt;/code&gt;. &lt;code&gt;EfficientNet&lt;/code&gt; digunakan karena kemampuannya yang cukup mengesankan dalam mengklasifikasi data gambar pada &lt;code&gt;ImageNet&lt;/code&gt;.&lt;/p&gt;&lt;p&gt;&lt;span class=&quot;gatsby-resp-image-wrapper&quot; style=&quot;position:relative;display:block;margin-left:auto;margin-right:auto;max-width:630px&quot;&gt;
      &lt;a class=&quot;gatsby-resp-image-link&quot; href=&quot;/static/8e6e4c13294e03c69bc808b9d427122c/17d12/params.png&quot; style=&quot;display:block&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;
    &lt;span class=&quot;gatsby-resp-image-background-image&quot; style=&quot;padding-bottom:79.74683544303798%;position:relative;bottom:0;left:0;background-image:url(&amp;#x27;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAQCAYAAAAWGF8bAAAACXBIWXMAABYlAAAWJQFJUiTwAAACRElEQVQ4y4WU627bMAxG8/5vlQHZnxZph66zc7fTzk5cX+OLbMvyGagmQ4o1KwFCMiwdfiQlTcZxpKoqmqahUYp2GGhPJ1RRoHyfneOw3m5ZLJYcjxF936FUQ9uq89iglKKua8uYyCTPcy6mfY9kPqdcr6FpKLOcosjJsoSqPqH1wDCAETew2wWsVmt836coCiYC6foeWVU+PRGt19RdxwCM5yDHY8636Q++z5749fzKqYAsHQkDxd6Prbq+763SyVsUUVQVyOj7fyGCM2Ykz0aKHHwvwHW3JEmKGTXGaJSqWW9WVplY27ZMRGpvDOnjI3VZnmEG1UCayiLQukfrjmHQtobDYJDaX5sx5h3oeR6qrjG+b0GiKsvgeGhtbbuuJQgC5vM5m40oTN71j6N1AV3mFrjxPKowhOjAqYYkHlGNsd3bbrfs93tbcJmnIvkKdplfRgs8vMXo5I3YDVntUlzHIUlibtlnoA/Arb+nCgOSRUzTdVRlabslFoYhq9WKl5cXynN9L+puAl+D3xRRwu6nx2sQ2M3iUqthGOxx0Fp/2HjdkH+AjvPMMTqR5QN1fSIMD7YZ8vNWurdKYIF9L91syTNRoW0TxL9K8SZwHAfSpMZx9njext5XSfVytq6BX7kFSn3iOMF1l9zd3dkzJ00R2P+6+plJve1drqrS0qWjruuyWCzous6+HhJQ7qoEkDLIt7xOsl7Opfy7HicSUciSpkDsi6M1y+WS2Wxmg4jyh4cH7u/v7Y2ZTqc2Ewkoe68D/QGMI9CDW07AUAAAAABJRU5ErkJggg==&amp;#x27;);background-size:cover;display:block&quot;&gt;&lt;/span&gt;
  &lt;img class=&quot;gatsby-resp-image-image&quot; alt=&quot;params&quot; title=&quot;params&quot; src=&quot;/static/8e6e4c13294e03c69bc808b9d427122c/f058b/params.png&quot; srcSet=&quot;/static/8e6e4c13294e03c69bc808b9d427122c/c26ae/params.png 158w,/static/8e6e4c13294e03c69bc808b9d427122c/6bdcf/params.png 315w,/static/8e6e4c13294e03c69bc808b9d427122c/f058b/params.png 630w,/static/8e6e4c13294e03c69bc808b9d427122c/40601/params.png 945w,/static/8e6e4c13294e03c69bc808b9d427122c/78612/params.png 1260w,/static/8e6e4c13294e03c69bc808b9d427122c/17d12/params.png 1666w&quot; sizes=&quot;(max-width: 630px) 100vw, 630px&quot; style=&quot;width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0&quot; loading=&quot;lazy&quot; decoding=&quot;async&quot;/&gt;
  &lt;/a&gt;
    &lt;/span&gt;&lt;/p&gt;&lt;p&gt;Model yang akan digunakan pada notebook ini untuk data gambar adalah &lt;code&gt;EfficientNetB7&lt;/code&gt; dengan weight &lt;code&gt;noise-student&lt;/code&gt;.&lt;/p&gt;&lt;p&gt;Published Paper : &lt;a href=&quot;https://arxiv.org/abs/1905.11946&quot;&gt;EfficientNet&lt;/a&gt;&lt;/p&gt;&lt;h3 id=&quot;load-image-data---tf-dataset&quot;&gt;Load Image Data - TF Dataset&lt;/h3&gt;&lt;p&gt;Load data gambar ke bentuk Tensor lalu alokasikan ke TF Dataset untuk effisiensi memori. Data gambar yang digunakan adalah data hasil upsampling dengan menggunakan augmentasi pada kelas &lt;code&gt;0&lt;/code&gt; sebesar &lt;code&gt;200%&lt;/code&gt;.&lt;/p&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;# Train Dataset
train_prep = pd.read_csv(&amp;#x27;../input/data-bdc/Preprocess and Up Sample/Up-Sample-0-by-200%/Keterangan.csv&amp;#x27;)
# Valid Data
valid_prep = pd.read_csv(&amp;#x27;../input/data-bdc/Validitas/Keterangan.csv&amp;#x27;)
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Spesifikasikan direktori file gambar dengan pada GCS&lt;/p&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;# Spesifikasi PATH data train pada GCS
AUG_PATH = GCS_PATH + &amp;#x27;/Preprocess and Up Sample/&amp;#x27;

# Data Up Sample 200%
TRAIN_X_, TRAIN_y_ = shuffle([AUG_PATH + x for x in train_prep.DIR.values],
                             train_prep.label.values, random_state = SEED)

# Spesifikasi PATH data Valid pada GCS
VAL_X = [GCS_PATH + &amp;#x27;/&amp;#x27; + x for x in valid_prep.DIR.values]
VAL_y = valid_prep.label.values

# Spesifikasi PATH data test pada GCS
TEST_PATH = GCS_PATH + &amp;#x27;/Data BDC - Satria Data 2020/Data Uji/File Gambar Data Uji/&amp;#x27;
TEST_X = [TEST_PATH + x for x in test[&amp;#x27;nama file gambar&amp;#x27;].values]
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;strong&gt;Decoding image&lt;/strong&gt; : mengubah data gambar menjadi tensor.&lt;/p&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;def decode_image(filename, label=None, image_size=(512, 512)):
    &amp;quot;&amp;quot;&amp;quot;
    Decode Image from String Path Tensor
    &amp;quot;&amp;quot;&amp;quot;
    bits = tf.io.read_file(filename)
    image = tf.image.decode_jpeg(bits, channels=3)
    image = tf.cast(image, tf.float32) / 255.0

    if label is None: # if test
        shapes = tf.shape(image)
        h, w = shapes[-3], shapes[-2]
        dim = tf.minimum(h, w)
        image = tf.image.resize_with_crop_or_pad(image, dim, dim)
        image = tf.image.resize(image, image_size)
        return image
    else:
        image = tf.image.resize(image, image_size)
        return image, label
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Inisialisasi &lt;code&gt;TF Dataset&lt;/code&gt;.&lt;/p&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;# TF Train Dataset
train_dataset = (
    tf.data.Dataset
    .from_tensor_slices((TRAIN_X_, TRAIN_y_))
    .map(decode_image, num_parallel_calls=AUTO)
    .cache()
    .repeat()
    .shuffle(1024)
    .batch(BATCH_SIZE)
    .prefetch(AUTO)
)

# TF Valid Dataset
valid_dataset = (
    tf.data.Dataset
    .from_tensor_slices((VAL_X, VAL_y))
    .map(decode_image, num_parallel_calls=AUTO)
    .batch(BATCH_SIZE)
    .cache()
    .prefetch(AUTO)
)

# TF Test Dataset
test_dataset = (
    tf.data.Dataset
    .from_tensor_slices((TEST_X))
    .map(decode_image, num_parallel_calls=AUTO)
    .batch(BATCH_SIZE)
)
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&quot;membuat-model&quot;&gt;Membuat Model&lt;/h3&gt;&lt;p&gt;Membuat model yang akan digunakan. Berikut adalah arsitektur model yang akan digunakan.&lt;/p&gt;&lt;p&gt;&lt;span class=&quot;gatsby-resp-image-wrapper&quot; style=&quot;position:relative;display:block;margin-left:auto;margin-right:auto;max-width:630px&quot;&gt;
      &lt;a class=&quot;gatsby-resp-image-link&quot; href=&quot;/static/2a3851e6da1c28a039a2f955d591a62f/6af66/model_1.png&quot; style=&quot;display:block&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;
    &lt;span class=&quot;gatsby-resp-image-background-image&quot; style=&quot;padding-bottom:98.10126582278481%;position:relative;bottom:0;left:0;background-image:url(&amp;#x27;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAUCAYAAACNiR0NAAAACXBIWXMAAAsTAAALEwEAmpwYAAACJElEQVQ4y42UC5OqMAyF+f9/TscRdRQf+EBABRWLItjufJnJXnCve29nYpLanuakh3rOOXe73VwURe5wOLg0Td3pdBKvORaGoZvP5269XkuMLZdLMfYzrLXOU8DNZuN2u50AH49Hyff7vdtut+IBGwwG4lerlQAFQSBWFEUXsK5rV1WV+Ofz2YnxVPx4PFxZlu5+v0usRt40zR9Afhj8YYyRTXg18slkIlVfr1d3Pp9dnudil8tFDlcwhlT4er1clmVClx7hkyQR2nEcC8Veryf5YrEQ2tPp1Pm+L8AK+E2ZoH06J5NrzAH0jMvhAA6jr8zB4keFmnwagPw22vs9pEElUMboFXNqXAgS0eq0Wm5fW0QrZrOZVO2RsBHJMMECPLR0Eznr+I+5fr8vABTCYciJNRzoteXwLgk1Klc5sZb+0ltVBDGetd57P9AUCxUM0SMbNiFgLg7AT8PT69bGAgI9qEIHKsPh0I1GI2kL8Xg8Fqm15dKRTfumoAZFpcUloUPAiVU2bcBOhe+ALARUPzs8VTJPTkswXf9PQHqGVOgVlVIlFPk6qE5fm/YL8ysgQGyCFn0EhN4hDXoLfT4/1v0XIBS5GCSihqgB0McCg/5fAW1r8H9d17aqqo4FQSC+LEtrjLFFUXyvfx8/KuR2oUj/uAxiPjXkAmViXhqYfKLMc2GsteIZaZqaPM9NlmUmSRIThqHxfd9EUWTiOBbfNE1nn9oXqdb9GF7lj6MAAAAASUVORK5CYII=&amp;#x27;);background-size:cover;display:block&quot;&gt;&lt;/span&gt;
  &lt;img class=&quot;gatsby-resp-image-image&quot; alt=&quot;model&quot; title=&quot;model&quot; src=&quot;/static/2a3851e6da1c28a039a2f955d591a62f/f058b/model_1.png&quot; srcSet=&quot;/static/2a3851e6da1c28a039a2f955d591a62f/c26ae/model_1.png 158w,/static/2a3851e6da1c28a039a2f955d591a62f/6bdcf/model_1.png 315w,/static/2a3851e6da1c28a039a2f955d591a62f/f058b/model_1.png 630w,/static/2a3851e6da1c28a039a2f955d591a62f/6af66/model_1.png 640w&quot; sizes=&quot;(max-width: 630px) 100vw, 630px&quot; style=&quot;width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0&quot; loading=&quot;lazy&quot; decoding=&quot;async&quot;/&gt;
  &lt;/a&gt;
    &lt;/span&gt;&lt;/p&gt;&lt;p&gt;Load EfficientNet&lt;/p&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;# Tensorflow - Keras Layers
import tensorflow.keras.backend as K
import tensorflow.keras.layers as L
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import ModelCheckpoint

# EfficientNet
import efficientnet.tfkeras as efn
&lt;/code&gt;&lt;/pre&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;def Make_EfficientNet_model():
    &amp;quot;&amp;quot;&amp;quot;
    https://www.kaggle.com/xhlulu/flowers-tpu-concise-efficientnet-b7
    &amp;quot;&amp;quot;&amp;quot;
    model = tf.keras.Sequential([
        efn.EfficientNetB7(            # EfficientnetB7
            input_shape=(512, 512, 3),
            weights=&amp;#x27;noisy-student&amp;#x27;,
            include_top=False
        ),
        L.GlobalAveragePooling2D(),
        L.Dense(512, activation= &amp;#x27;relu&amp;#x27;),
        L.Dropout(0.2),
        L.Dense(1, activation=&amp;#x27;sigmoid&amp;#x27;)
    ])
    model.compile(optimizer=&amp;#x27;adam&amp;#x27;, loss = &amp;#x27;binary_crossentropy&amp;#x27;, metrics=[&amp;#x27;accuracy&amp;#x27;])
    return model
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Build &amp;amp; Scope model ke &lt;code&gt;TPU&lt;/code&gt; &amp;amp; Compile model.&lt;/p&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;with strategy.scope():
    model = Make_EfficientNet_model()
&lt;/code&gt;&lt;/pre&gt;&lt;pre&gt;&lt;code&gt;Downloading data from https://github.com/qubvel/efficientnet/releases/download/v0.0.1/efficientnet-b7_noisy-student_notop.h5
258072576/258068648 [==============================] - 4s 0us/step
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&quot;training-model&quot;&gt;Training Model&lt;/h3&gt;&lt;p&gt;Melakukan pelatihan terhadap model dari data train. Pelatihan dilakukan dengan ketentuan:&lt;/p&gt;&lt;pre&gt;&lt;code&gt;EPOCHS = 10
STEPS_PER_EPOCHS = len(TRAIN_X_) // BATCH_SIZE:128 (Jika menggunakan TPU)
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Pelatihan dilakukan dengan memperhatikan nilai dari &lt;code&gt;val_accuracy&lt;/code&gt;, &lt;code&gt;epoch&lt;/code&gt; yang memiliki nilai &lt;code&gt;val_accuracy&lt;/code&gt; terbaik akan disave &lt;code&gt;weights&lt;/code&gt; - nya dan akan di load pada saat melakukan evaluasi dan prediksi nantinya.&lt;/p&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;# Config
EPOCHS = 10
STEPS_PER_EPOCH = len(TRAIN_X_) // BATCH_SIZE
checkpoint = ModelCheckpoint(&amp;#x27;EfficientNetB7_best_model.h5&amp;#x27;, monitor=&amp;#x27;val_accuracy&amp;#x27;,
                             save_best_only=True, save_weights_only=True, mode=&amp;#x27;max&amp;#x27;)

# Fitting Model
print(f&amp;#x27;[INFO] Fitting Model&amp;#x27;)
history = model.fit(train_dataset, epochs = EPOCHS,
                    steps_per_epoch = STEPS_PER_EPOCH,
                    validation_data = valid_dataset,
                    callbacks = [checkpoint])
&lt;/code&gt;&lt;/pre&gt;&lt;pre&gt;&lt;code&gt;[INFO] Fitting Model
Epoch 1/10
38/38 [==============================] - 164s 4s/step - loss: 0.4728 - accuracy: 0.7858 - val_loss: 1.0803 - val_accuracy: 0.7417
Epoch 2/10
38/38 [==============================] - 40s 1s/step - loss: 0.3696 - accuracy: 0.8329 - val_loss: 0.8951 - val_accuracy: 0.8142
Epoch 3/10
38/38 [==============================] - 35s 914ms/step - loss: 0.3544 - accuracy: 0.8454 - val_loss: 0.9107 - val_accuracy: 0.8063
Epoch 4/10
38/38 [==============================] - 40s 1s/step - loss: 0.3058 - accuracy: 0.8647 - val_loss: 0.6083 - val_accuracy: 0.8236
Epoch 5/10
38/38 [==============================] - 35s 915ms/step - loss: 0.3186 - accuracy: 0.8625 - val_loss: 0.5071 - val_accuracy: 0.8047
Epoch 6/10
38/38 [==============================] - 35s 916ms/step - loss: 0.2928 - accuracy: 0.8736 - val_loss: 0.5885 - val_accuracy: 0.8189
Epoch 7/10
38/38 [==============================] - 35s 915ms/step - loss: 0.2837 - accuracy: 0.8734 - val_loss: 1.2540 - val_accuracy: 0.6409
Epoch 8/10
38/38 [==============================] - 35s 915ms/step - loss: 0.2478 - accuracy: 0.8925 - val_loss: 0.7599 - val_accuracy: 0.8142
Epoch 9/10
38/38 [==============================] - 35s 915ms/step - loss: 0.2325 - accuracy: 0.9025 - val_loss: 0.6619 - val_accuracy: 0.7496
Epoch 10/10
38/38 [==============================] - 35s 914ms/step - loss: 0.2350 - accuracy: 0.8966 - val_loss: 0.6473 - val_accuracy: 0.7811
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Meninjau &lt;code&gt;train history&lt;/code&gt; pada model&lt;/p&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;fig, (ax1, ax2) = plt.subplots(2, 1, figsize = (12, 10))
ax1.plot(range(1, EPOCHS + 1), history.history[&amp;#x27;loss&amp;#x27;], label = &amp;#x27;loss&amp;#x27;)
ax1.plot(range(1, EPOCHS + 1), history.history[&amp;#x27;val_loss&amp;#x27;], label = &amp;#x27;val_loss&amp;#x27;)
ax1.set_title(&amp;#x27;Loss at Training&amp;#x27;, fontsize = 14)
ax1.legend()
ax2.plot(range(1, EPOCHS + 1), history.history[&amp;#x27;accuracy&amp;#x27;], label = &amp;#x27;accuracy&amp;#x27;)
ax2.plot(range(1, EPOCHS + 1), history.history[&amp;#x27;val_accuracy&amp;#x27;], label = &amp;#x27;val_accuracy&amp;#x27;)
ax2.set_title(&amp;#x27;Accuracy at Training&amp;#x27;, fontsize = 14)
ax2.legend()
fig.show()
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;span class=&quot;gatsby-resp-image-wrapper&quot; style=&quot;position:relative;display:block;margin-left:auto;margin-right:auto;max-width:630px&quot;&gt;
      &lt;a class=&quot;gatsby-resp-image-link&quot; href=&quot;/static/86b46e6f767204d7be23816991d06fbb/6bbf7/output_123_0.png&quot; style=&quot;display:block&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;
    &lt;span class=&quot;gatsby-resp-image-background-image&quot; style=&quot;padding-bottom:82.91139240506328%;position:relative;bottom:0;left:0;background-image:url(&amp;#x27;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAARCAYAAADdRIy+AAAACXBIWXMAAAsSAAALEgHS3X78AAADFUlEQVQ4y4VUzVIkRRBuL170HTwoYISvYAQgL7AH2NC7B8MX4hnkxGE3NJbDDD3AsoNeZNAwQBgChmG6e7r6t7qyMivTqG5wYDW0Ir7IrKysqqzMLyt4/erVx3GcvFBZ9jJN002VplseaZpuZVm2eXd39/VwOPw2DMPver3e97e3t9/kabyZKtX6KKW2klRt3k6jl3EUvQjeDU+WwDSGnRPHIo65g58/gJnlcTi04oqZOEJxrvMjQsnUXGZRBMHxyc+fIZhUoBBBQ0LWiUPn9z4FM7f3CVRO0DgxxWKdmURYAEAF/X5v2SJm/naG2jHUzKZghsrH1YG5kw67Na/b+pmP349EeRAOBsuItjtQ2le3q2xrYVOKT8XftiYXJtvpzMJWi5hC/CUCpSCYPHh7dLQEYOdAba5QpA2fRISEgARKEig6iU1nfwJmJh+6jxPRpsG74+MVAOsaS1IZbFEaFA0oQCyAToxFMZbauUESaPG8WO2TkVywu7v7SVVVP6KFw1rrgdZ6YIwJK92EscoPVFmHhYZeXjd9Vdb7WdXsp0UVzvNqMJ1nhzNVhpEqw/u0OEpV9iYQkf9FEAQfPMgPvaT/8t3Z2fk0iqK3V1dXvydxfOqIRoQ4staOEOnXsiz/SJJkrLLsz2kUj2eJuii0Oc3rZqTK5nRe6NG80Kfzsvmt0vo4ODw4WLFGg7Ug2oA0yGJIRPucNla0deJLoZHFOmnh8+fh5PkgIgx2dn74wiLN/SFVWSDokowuiZqSxBQkpqSOnxU9kPmhwkwCVVdlEWyLYq0KfjkZLjUGMlNlIlb7LhEhaP07vjlhNMKOPPE7veWpFm4yEc9V27BA1fHw9U9vPkejlTh4JPY/O+QRjhad0uQLG9ROHAkiZsF+v7+MiKpNgg9HFo3xHNxJq1l8Z2Dzvo8/MA/29vZWAKDg936Yf4X3ISuujBbzJz+StbYMtre3P5pMJl8mSbI2Ho+/urm5WY+iaDWO49XLy8uNyWSy7vXZbLZ6cXGxcT+drkWz+9XpdLp2fn6+4e1+/fr6euPs7Gz9L6X62K2rH47sAAAAAElFTkSuQmCC&amp;#x27;);background-size:cover;display:block&quot;&gt;&lt;/span&gt;
  &lt;img class=&quot;gatsby-resp-image-image&quot; alt=&quot;png&quot; title=&quot;png&quot; src=&quot;/static/86b46e6f767204d7be23816991d06fbb/f058b/output_123_0.png&quot; srcSet=&quot;/static/86b46e6f767204d7be23816991d06fbb/c26ae/output_123_0.png 158w,/static/86b46e6f767204d7be23816991d06fbb/6bdcf/output_123_0.png 315w,/static/86b46e6f767204d7be23816991d06fbb/f058b/output_123_0.png 630w,/static/86b46e6f767204d7be23816991d06fbb/6bbf7/output_123_0.png 716w&quot; sizes=&quot;(max-width: 630px) 100vw, 630px&quot; style=&quot;width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0&quot; loading=&quot;lazy&quot; decoding=&quot;async&quot;/&gt;
  &lt;/a&gt;
    &lt;/span&gt;&lt;/p&gt;&lt;h3 id=&quot;load-weights-model&quot;&gt;Load weights model&lt;/h3&gt;&lt;p&gt;Load &lt;code&gt;weights&lt;/code&gt; terbaik yang telah disimpan.&lt;/p&gt;&lt;h4 id=&quot;note-&quot;&gt;Note :&lt;/h4&gt;&lt;ul&gt;&lt;li&gt;Untuk mendapatkan hasil yang sama dengan yang di submit pada website &lt;code&gt;BDC - Satria Data&lt;/code&gt; bisa menggunakan &lt;code&gt;weights&lt;/code&gt; yang telah kami simpan &lt;a href=&quot;https://www.kaggle.com/pencarikebahagiaan/modelku&quot;&gt;disini&lt;/a&gt;.&lt;/li&gt;&lt;li&gt;Untuk melihat proses training menggunakan data dengan kriteria Up-Sampling lainnya untuk perbandingan silahkan kunjugi notebook pada responsitory kami &lt;a href=&quot;https://github.com/Hyuto/BDC-Satria-Data/tree/master/Notebooks&quot;&gt;di sini&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;# model.load_weights(&amp;#x27;EfficientNetB7_best_model.h5&amp;#x27;)
model.load_weights(&amp;#x27;../input/modelku/Image Model/200%_best_model.h5&amp;#x27;) # Jika menggunakan Model kami
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&quot;memprediksi-dan-mengevaluasi-data&quot;&gt;Memprediksi dan Mengevaluasi Data&lt;/h3&gt;&lt;p&gt;Memprediksi data train dan data valid lalu dilakukan pengevaluasian untuk melihat kebaikan model.&lt;/p&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;# Valid
val_pred = model.predict(np.concatenate([x for x, y in valid_dataset], axis=0))
val_pred_classes = np.array(val_pred.flatten() &amp;gt;= .5, dtype = &amp;#x27;int&amp;#x27;)
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Mengevaluasi model dengan &lt;code&gt;Accuracy Score&lt;/code&gt; dan &lt;code&gt;F1 Score&lt;/code&gt;.&lt;/p&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;print(f&amp;#x27;Accuracy Valid Data : {accuracy_score(VAL_y, val_pred_classes)}&amp;#x27;)
print(f&amp;#x27;F1 Score Valid Data : {f1_score(VAL_y, val_pred_classes)}&amp;#x27;)
&lt;/code&gt;&lt;/pre&gt;&lt;pre&gt;&lt;code&gt;Accuracy Valid Data : 0.8299212598425196
F1 Score Valid Data : 0.9040852575488455
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Confussion Matrix&lt;/p&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;plt.figure(figsize = (7, 7))
sns.heatmap(confusion_matrix(VAL_y, val_pred_classes, normalize = &amp;#x27;true&amp;#x27;),
            annot=True, cmap=plt.cm.Blues)
plt.title(&amp;#x27;Normalized Confussion Matrix Valid Data&amp;#x27;)
plt.xlabel(&amp;#x27;Predicted&amp;#x27;)
plt.ylabel(&amp;#x27;Actual&amp;#x27;)
plt.show()
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;span class=&quot;gatsby-resp-image-wrapper&quot; style=&quot;position:relative;display:block;margin-left:auto;margin-right:auto;max-width:426px&quot;&gt;
      &lt;a class=&quot;gatsby-resp-image-link&quot; href=&quot;/static/69dc3cbfb884ef33415b42e135fcf954/531e1/output_131_0.png&quot; style=&quot;display:block&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;
    &lt;span class=&quot;gatsby-resp-image-background-image&quot; style=&quot;padding-bottom:104.43037974683544%;position:relative;bottom:0;left:0;background-image:url(&amp;#x27;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAVCAYAAABG1c6oAAAACXBIWXMAAAsSAAALEgHS3X78AAAEl0lEQVQ4y22TfWwTdRjHnw22u951EF5lokFDFDSOra1usqF/KDGBJQNcu46oQcwGspZhJyu76eyCJINoYkwwgS3tWJCIJvIq/ocEjCHEiBHJeluBIYy1ZWPFtb1r7+0xv197ywj+8clzd7n75Ps8z+/g5ngGrkaiRTEZn/x03xe2D1vbq5s8vjU7d3dW7fJ3VTV722pKVzdWFTy3rqa4rP6VopW11SbchoPP8FtOWJmVW+awZdtLmJeaASJxCYZj6VWRuPRgKJqaFMeSCZPhWGpycHQqYf/oRKKwrjdhdYUSnDOY4JyhSc4ZnOCcIeQ3H/MxDt8etsrfyla2Twvtw7E0RuLSNMMxCW/EJRSjaXT4TmHhhj4saTiCvKvfxKBC98AnzNoDe5m3vuxg3uwBGBpLEqktEpeM4VjaiMQlnTAcS9MqRlO6w3dSL6zr00sa+nXOGaLwrn6FCDlXSOCd/QGuPrjH+v45gH8e6iDem5pOSKTiWBLNezGaQofvJBbW9aG1oZ+mJJVzhlSS1PJ2UHii+cfPFr4TDNxEBEBEuPiH+CpJaAp/F0eRSG/cf1RIBE9tPYaL3j2KlvqckNnYJyz/+OfuZ33n/Mt2HAe4PanSGeZlOEP8WMKSXDIK7+qnwqK6XsGx70J3eeC8v6zjFAA5NvkZmgsxIvH0/wh7aas8keWkVDir9pCw9psrgZr95zs/uIUAl6/fnt5yHrIcHKLXEobHUuhoO42FG4NY4h5Anmw6h8q7BxBqDwuN314LrP/6QpdMZggA7MhExnZrXMY8Bq33JRwZl5GkdbT+gAXrDqJ102HkNh4yUflNhxHe+ErwnBa7t373d/vmQ79QoSWjoy2dNTCtUAxSU1kDJQVxStawsvEAwgvbkXO0ImvbaaJaHK0IT78n7L10t7v9TNi/6/gVgERSBkkx7LKKmMcgVVIMzKiIqYyOle4ehBVNyNk8yJa3UJjyFtVi8yCUNgrBa+OBz89e7SInBlIZDTIa2oiAMFNIanJa2Iyc3UtEFLbCo1rsXoQljcJPNx8Gei+KnWiew6Ss2E2BpBhGKqPR9h8VkoReKrXYqFgl17DYLVwelbrP/BX1f/+bmBOO3B2rMoVTkmKMxiZIxYxGhNq0kLS45PV2XLC6DYtX7cgJFzUIQw+07kvihP9XcRxIuyCraJvRpjGj/ccSzmyZChc2CHEZAxf+HAnQliXFIJAZ6rKKuqQYmomsopbM6Fqlu0eDFU0aZ/NqbIWHwpS3ZKlwgatjSsfA9TsP90hEKKtIhA5zyzNmmU+oPZKQrfBQSEoqnO/qMhD33X0gd94ZTwHIChJelFUMS4oRllUcTGd1MZ01whkNw+msMVjp3j8IK5rDnM0bZspbRKa8JcxWeK5zdm8U5jm3IaJ3YirblMwapGWSEAskxWCzOrL/pjLsbn/HvNujMfbWnXuW4JGj1udf28LDsk2ctTpQPHvpmjlFyzdw7MvtDG/3sDC3fhYizlZ1nJWfIYKk0sXQM0keEk6dPUf+ogIAWAwApeSPKlpaA/NbJbBU7wWmbBvw9p0Ac+vp+7qe++4/XYIdvJr+obYAAAAASUVORK5CYII=&amp;#x27;);background-size:cover;display:block&quot;&gt;&lt;/span&gt;
  &lt;img class=&quot;gatsby-resp-image-image&quot; alt=&quot;png&quot; title=&quot;png&quot; src=&quot;/static/69dc3cbfb884ef33415b42e135fcf954/531e1/output_131_0.png&quot; srcSet=&quot;/static/69dc3cbfb884ef33415b42e135fcf954/c26ae/output_131_0.png 158w,/static/69dc3cbfb884ef33415b42e135fcf954/6bdcf/output_131_0.png 315w,/static/69dc3cbfb884ef33415b42e135fcf954/531e1/output_131_0.png 426w&quot; sizes=&quot;(max-width: 426px) 100vw, 426px&quot; style=&quot;width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0&quot; loading=&quot;lazy&quot; decoding=&quot;async&quot;/&gt;
  &lt;/a&gt;
    &lt;/span&gt;&lt;/p&gt;&lt;h3 id=&quot;memprediksi-data-test&quot;&gt;Memprediksi Data Test&lt;/h3&gt;&lt;p&gt;Menggunakan model untuk memprediksi &lt;code&gt;test dataset&lt;/code&gt;.&lt;/p&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;EfficientNet_pred = model.predict(test_dataset)
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Clear session &amp;amp; free up memory.&lt;/p&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;K.clear_session()
tf.tpu.experimental.initialize_tpu_system(tpu)
&lt;/code&gt;&lt;/pre&gt;&lt;pre&gt;&lt;code&gt;&amp;lt;tensorflow.python.tpu.topology.Topology at 0x7ff9cb254190&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&quot;pemodelan-data-teks-bert&quot;&gt;Pemodelan Data Teks (BERT)&lt;/h2&gt;&lt;p&gt;BERT(Bidirectional Encoder Representations from Transformers) adalah pretrained model karya Jacob Devlin, Ming-Wei Chang, Kenton Lee dan Kristina Toutanova. Model ini berupa transformator dua arah yang telah dilatih sebelumnya.&lt;/p&gt;&lt;p&gt;Published Paper : &lt;a href=&quot;https://arxiv.org/abs/1810.04805&quot;&gt;BERT&lt;/a&gt;&lt;/p&gt;&lt;p&gt;Bert yang digunakan pada notebook ini adalah &lt;code&gt;bert-base-indonesian&lt;/code&gt; buatan &lt;code&gt;Cahya Wirawan&lt;/code&gt; yang berupa pre-trained BERT-base model pada 522MB data wikipedia indonesia dengan vocabulary sebesar 32.000. Keterangan lebih lanjut bisa dibaca &lt;a href=&quot;https://huggingface.co/cahya/bert-base-indonesian-522M&quot;&gt;di sini&lt;/a&gt;&lt;/p&gt;&lt;p&gt;Load Transformers terlebih dahulu&lt;/p&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;from transformers import *
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&quot;load-bert-tokenizer&quot;&gt;Load Bert Tokenizer&lt;/h3&gt;&lt;p&gt;Load Bert Tokenizer dari library &lt;code&gt;transformers&lt;/code&gt;&lt;/p&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;model_name=&amp;#x27;cahya/bert-base-indonesian-522M&amp;#x27;
tokenizer = BertTokenizer.from_pretrained(model_name)
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&quot;encode-teks&quot;&gt;Encode Teks&lt;/h3&gt;&lt;p&gt;Data teks akan di encode menjadi 3 tipe untuk input kedalam model.&lt;/p&gt;&lt;ul&gt;&lt;li&gt;input ids adalah ID / nomor yang merepresentasikan kata&lt;/li&gt;&lt;li&gt;attention mask adalah keterangan dari ID yang harus diperhatikan ditandakan oleh nomor 1.&lt;/li&gt;&lt;li&gt;token type id adalah ID yang berhubungan dengan multiple sequance.&lt;/li&gt;&lt;/ul&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;index = random.randint(0, len(train_text))
encode = tokenizer.encode_plus(train_text[index], return_attention_mask = True,
                               return_token_type_ids=True)
print(&amp;#x27;Actual : &amp;#x27;)
print(train_text[index])
print(&amp;#x27;&amp;#x27;.rjust(80, &amp;#x27;-&amp;#x27;))
print(&amp;#x27;Token IDS : &amp;#x27;)
print(encode[&amp;quot;input_ids&amp;quot;])
print(&amp;#x27;&amp;#x27;.rjust(80, &amp;#x27;-&amp;#x27;))
print(&amp;#x27;Attention Mask : &amp;#x27;)
print(encode[&amp;#x27;attention_mask&amp;#x27;])
print(&amp;#x27;&amp;#x27;.rjust(80, &amp;#x27;-&amp;#x27;))
print(&amp;#x27;Token Type IDS : &amp;#x27;)
print(encode[&amp;#x27;token_type_ids&amp;#x27;])
&lt;/code&gt;&lt;/pre&gt;&lt;pre&gt;&lt;code&gt;Actual :
jokowi ahok bagikan amplop berisi uang beras warga cilincing jokowi ikutan sembako amplop ahok cilincing
--------------------------------------------------------------------------------
Token IDS :
[3, 15071, 29708, 28939, 2688, 9155, 3709, 3714, 2111, 3167, 23190, 1010, 5398, 15071, 3821, 1487, 4461, 9745, 2688, 9155, 29708, 23190, 1010, 5398, 1]
--------------------------------------------------------------------------------
Attention Mask :
[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]
--------------------------------------------------------------------------------
Token Type IDS :
[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&quot;mengecek-sequence-terpanjang&quot;&gt;Mengecek Sequence Terpanjang&lt;/h3&gt;&lt;p&gt;Karena panjang dari data teks tidak sama. Maka perlu di lakukan padding untuk menyamakan panjang dari sequance.&lt;/p&gt;&lt;p&gt;Contoh:&lt;/p&gt;&lt;pre&gt;&lt;code&gt;jokowi ahok bagikan =&amp;gt; Encode IDS =&amp;gt; [3, 15071, 29708] =&amp;gt; Padding to 5 =&amp;gt; [3, 15071, 29708, 0, 0]
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Untuk melakukan padding perlu dicari sequence terpanjang pada data sebagai acuannya.&lt;/p&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;plt.figure(figsize=(7,7))
sns.distplot([len(tokenizer.encode(x)) for x in train_text], label = &amp;#x27;train&amp;#x27;)
plt.title(&amp;#x27;Distplot of Text Lenght&amp;#x27;, fontsize=14)
plt.legend()
plt.show()
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;span class=&quot;gatsby-resp-image-wrapper&quot; style=&quot;position:relative;display:block;margin-left:auto;margin-right:auto;max-width:444px&quot;&gt;
      &lt;a class=&quot;gatsby-resp-image-link&quot; href=&quot;/static/d4694f2bf4b44d4fbbb1e8ec937fba61/9b7bd/output_144_0.png&quot; style=&quot;display:block&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;
    &lt;span class=&quot;gatsby-resp-image-background-image&quot; style=&quot;padding-bottom:97.46835443037975%;position:relative;bottom:0;left:0;background-image:url(&amp;#x27;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAATCAYAAACQjC21AAAACXBIWXMAAAsSAAALEgHS3X78AAAC10lEQVQ4y52Uy08TQRzHfwvVKI9a41Gi5eFBEj2oidF40KN/g3cTTiYae/AgFwwXAmkMAR+xyEsUDBiVmxpaaItAeRWkLe1uH9a2UNjuzmx363bHTCmkYETKL9nMznd/+cz3N7+ZBb1eDzQWFxerVv3+uiAbqPGv+mpjsVi12Wy+0tDQcNtkMt1samq6bjKZbjU2Nt6gWktLy1We540ez0ptiPXXRCLhWrfbfSYHI4RAMpl0ZhSFiFhCCKclRZYlWZaldD4K3yWJfpJzGpbSUjDB43AkkuE41gMMw5wAAANCaI4QQmIpWeOlDCkmNG1rFAQhBAaDgaEuBVGcoaIruKG6IzxN0bJZ7aCRzQM50Ol0OgDQpQTRRUWbN6FaPYncimpW28eVtjNq+UkOyDBMJQDoRRHlgGOehDq6EC1MPkDJBUAag28HYBto9SbUYVeEZAscFAVkGKYCACoR2gJ+mo+qvQ6OKL+zhwOWlpYeAYCjCKFZKo64IqplnCVCOnPgsv8qmZ5DnHc4MhtRuyZYEkiIueTsIUo20HNIgbSp76fD6vMxP3H613eARTscGRrIOaTq4HRYfWULkC8/4ocruaSkpAwAyjFGroyqkXdTIfW1naPgXY3ZD7oXeAwAjssSdm3iDOl1cGqfM0joPsZS6eKB201R0pIrIcikx8Gp/XngcjS1qzH/Au9tykkAOKWkpZl4SiY9dpY61LrtrDY0HdYyW/fvf/c6u8uhuDwGqpKeiwkK6bZzpH8yRN58DxHLBEuWosKB/zpIFEMADEP3sEzgNx1T/rj27OuK2G3z4S6rD3fZfNhi9eJvSz8xF+fRrw0BJ1MIr6cQ3hQxErGE84+IJFlZW1tboQbLAaDCOm6vetLZV28enjA+/ThpvNfcWf+w1XLe/MFpvNs+Wn3tzoPLjzqGzg1aF4xt/Z/r7j9uvmSdWqie9wbPtra/uNDW8fJigOVO/wHNY/So0fNq5QAAAABJRU5ErkJggg==&amp;#x27;);background-size:cover;display:block&quot;&gt;&lt;/span&gt;
  &lt;img class=&quot;gatsby-resp-image-image&quot; alt=&quot;png&quot; title=&quot;png&quot; src=&quot;/static/d4694f2bf4b44d4fbbb1e8ec937fba61/9b7bd/output_144_0.png&quot; srcSet=&quot;/static/d4694f2bf4b44d4fbbb1e8ec937fba61/c26ae/output_144_0.png 158w,/static/d4694f2bf4b44d4fbbb1e8ec937fba61/6bdcf/output_144_0.png 315w,/static/d4694f2bf4b44d4fbbb1e8ec937fba61/9b7bd/output_144_0.png 444w&quot; sizes=&quot;(max-width: 444px) 100vw, 444px&quot; style=&quot;width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0&quot; loading=&quot;lazy&quot; decoding=&quot;async&quot;/&gt;
  &lt;/a&gt;
    &lt;/span&gt;&lt;/p&gt;&lt;p&gt;Dari grafik diatas didapatkan kesimpulan bahwa panjang sequence data tersebar pada rentang 0 - 220 yang memusat pada rentang 0 - 100. Maka dari itu akan digunakan &lt;code&gt;250&lt;/code&gt; sebagai acuan maximum dari panjang sequencenya&lt;/p&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;max_len = 250
&lt;/code&gt;&lt;/pre&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;def regular_encode(texts, tokenizer = tokenizer, maxlen=max_len):
    &amp;quot;&amp;quot;&amp;quot;
    Encoding data teks
    &amp;quot;&amp;quot;&amp;quot;
    enc_di = tokenizer.batch_encode_plus(
        texts,
        return_attention_masks=True,
        return_token_type_ids=True,
        pad_to_max_length=True,
        max_length=maxlen
    )

    return {x : np.asarray(enc_di[x]) for x in enc_di}
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&quot;membuat-model-1&quot;&gt;Membuat Model&lt;/h3&gt;&lt;p&gt;Membuat model yang digunakan untuk memprediksi data teks, berikut adalah arsitekturnya&lt;/p&gt;&lt;p&gt;&lt;span class=&quot;gatsby-resp-image-wrapper&quot; style=&quot;position:relative;display:block;margin-left:auto;margin-right:auto;max-width:630px&quot;&gt;
      &lt;a class=&quot;gatsby-resp-image-link&quot; href=&quot;/static/0a9abd1578520a97bf552ec8263923f6/0d40b/model_2.png&quot; style=&quot;display:block&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;
    &lt;span class=&quot;gatsby-resp-image-background-image&quot; style=&quot;padding-bottom:43.67088607594937%;position:relative;bottom:0;left:0;background-image:url(&amp;#x27;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAJCAIAAAC9o5sfAAAACXBIWXMAAAsTAAALEwEAmpwYAAAA6ElEQVQoz4XR64qFIBQF4N7/CStJMaMsL3nXimqgzcAcOGfO91P2Yqm7EkJIKSmlCKG6rjnn8iGEmKaprmuM8TAMSimYHMcRIdR1XdM0VXx4751z67paa40xcCKltNbqx7qu3vsYYwjB/6ruP5xz0Mk5n+cZY8wYa9uWEDKO43me96uXcCkl51xKSSl577XWOecQAhT+Fz6OwxijHsYYKeXyIIQwxrqu27btY/i6LuhMKZVSYoxKKehMKeWcv1wbaK2VUrACSiljbN/3+503YXg5/Dbs4rqu72EYCiHAg9u27ft+WZb7gx/8+QRq/deDOQAAAABJRU5ErkJggg==&amp;#x27;);background-size:cover;display:block&quot;&gt;&lt;/span&gt;
  &lt;img class=&quot;gatsby-resp-image-image&quot; alt=&quot;model2&quot; title=&quot;model2&quot; src=&quot;/static/0a9abd1578520a97bf552ec8263923f6/f058b/model_2.png&quot; srcSet=&quot;/static/0a9abd1578520a97bf552ec8263923f6/c26ae/model_2.png 158w,/static/0a9abd1578520a97bf552ec8263923f6/6bdcf/model_2.png 315w,/static/0a9abd1578520a97bf552ec8263923f6/f058b/model_2.png 630w,/static/0a9abd1578520a97bf552ec8263923f6/40601/model_2.png 945w,/static/0a9abd1578520a97bf552ec8263923f6/0d40b/model_2.png 1175w&quot; sizes=&quot;(max-width: 630px) 100vw, 630px&quot; style=&quot;width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0&quot; loading=&quot;lazy&quot; decoding=&quot;async&quot;/&gt;
  &lt;/a&gt;
    &lt;/span&gt;&lt;/p&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;def build_model(transformer, loss=&amp;#x27;binary_crossentropy&amp;#x27;, max_len=max_len):
    input_ids = tf.keras.Input(shape=(max_len,), dtype=tf.int32, name=&amp;quot;input_ids&amp;quot;)
    attention_mask = tf.keras.Input(shape=(max_len,), dtype=tf.int32, name=&amp;#x27;attention_mask&amp;#x27;)
    token_type_ids = tf.keras.Input(shape=(max_len,), dtype=tf.int32, name=&amp;#x27;token_type_ids&amp;#x27;)

    sequence_output, pooled_output = transformer([input_ids, attention_mask, token_type_ids])
    cls_token = sequence_output[:, 0, :]
    x = L.Dropout(0.3)(cls_token)
    out = L.Dense(1, activation=&amp;#x27;sigmoid&amp;#x27;)(x)

    model = tf.keras.Model(inputs=[input_ids, attention_mask, token_type_ids], outputs=out)
    model.compile(Adam(lr=3e-5), loss=loss, metrics=[&amp;#x27;accuracy&amp;#x27;])

    return model
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&quot;cross-validation&quot;&gt;Cross Validation&lt;/h3&gt;&lt;p&gt;Metode yang digunakan untuk CV adalah &lt;code&gt;StratifiedKflod&lt;/code&gt;. Metode ini digunakan untuk mengatasi kelas pada data yang imbalance. &lt;code&gt;StratifiedKfold&lt;/code&gt; ini tidak jauh berbeda dengan &lt;code&gt;Kfold&lt;/code&gt; hanya pada saat pembagian menjadi data train dan data valid, proporsi masing masing kelas tetap sama pada kedua data tersebut.&lt;/p&gt;&lt;p&gt;&lt;span class=&quot;gatsby-resp-image-wrapper&quot; style=&quot;position:relative;display:block;margin-left:auto;margin-right:auto;max-width:630px&quot;&gt;
      &lt;a class=&quot;gatsby-resp-image-link&quot; href=&quot;/static/4fe824f6dc276bb0d79b4250ef77d902/8b70b/StratifiedKfold.png&quot; style=&quot;display:block&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;
    &lt;span class=&quot;gatsby-resp-image-background-image&quot; style=&quot;padding-bottom:39.24050632911392%;position:relative;bottom:0;left:0;background-image:url(&amp;#x27;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAICAYAAAD5nd/tAAAACXBIWXMAAAsTAAALEwEAmpwYAAAB+0lEQVQoz0XOS0tUYQCA4flP/YmW0bpVbbMWSoJdsDZdRbBVkBUuMiGQICpMUdQa5+aQHmec41ycZjxzrt+5fucyo76BBW3fxcuT82RCWZfk9RgjiOAM+v0NDravcq5/phHAfBP06Jy2G7HYDND9GM2P+dAKaQuJI1PeNwMObUlOyIQ5NWapf0rPk3AOhU6RV9/uMdRWqRoaT77OEBtlflgwXvDQnIiaEzNREtROBL0w41beptIT5II45Xop5E1nyMBP4Czj+0nC5E7AwAO1u8rrpcvQecmWBVObexiWxYGTMJE3qfdtfocZY1s6u137/3D+eMTAj+E0Y0OTPKrYWG5IxZJMFmwMf0iju87y8iVoz7Dnwf1dh5ou6Ycpd4omu/1/whvlkLfN+K/wNGNNkzzY0XGET8HOGN/WsN2Ikq4zvVnAMTq0zCMWPl1BKo/pxHBzJ6Xcy8j5MuFaIeDjaoWBCOFsyNpJxN2SjemGFK2E2z9NTDcgb2aMFwN6YkTTOOb5+jv67RUGnsHslym69QVy8XDE9IFkY6uKiNKLYd5MmPtl43kBe+6QZxUL3w+pOikvqtaF/Mgf8XQ/48gCzztkcWWMWHlIrtVqoTYaHDYaqKpKmqbYjkNdUZBhiHBd6so+ruPgCHHRPSHwAx+1pmAbAyIpUWotNM3mD3/7N5AuVhFlAAAAAElFTkSuQmCC&amp;#x27;);background-size:cover;display:block&quot;&gt;&lt;/span&gt;
  &lt;img class=&quot;gatsby-resp-image-image&quot; alt=&quot;SKfold&quot; title=&quot;SKfold&quot; src=&quot;/static/4fe824f6dc276bb0d79b4250ef77d902/f058b/StratifiedKfold.png&quot; srcSet=&quot;/static/4fe824f6dc276bb0d79b4250ef77d902/c26ae/StratifiedKfold.png 158w,/static/4fe824f6dc276bb0d79b4250ef77d902/6bdcf/StratifiedKfold.png 315w,/static/4fe824f6dc276bb0d79b4250ef77d902/f058b/StratifiedKfold.png 630w,/static/4fe824f6dc276bb0d79b4250ef77d902/40601/StratifiedKfold.png 945w,/static/4fe824f6dc276bb0d79b4250ef77d902/78612/StratifiedKfold.png 1260w,/static/4fe824f6dc276bb0d79b4250ef77d902/8b70b/StratifiedKfold.png 1266w&quot; sizes=&quot;(max-width: 630px) 100vw, 630px&quot; style=&quot;width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0&quot; loading=&quot;lazy&quot; decoding=&quot;async&quot;/&gt;
  &lt;/a&gt;
    &lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;code&gt;n_split&lt;/code&gt; yang digunakan sebesar 10. Dataset Folding kami bisa di daoat dengan menggunakan:&lt;/p&gt;&lt;pre&gt;&lt;code&gt;cv = StratifiedKFold(n_splits=10)
for fold, (train_ind, val_ind) in enumerate(cv.split(train_text, train.label.values)):
    x_train, y_train = train_text[train_ind], train.label.values[train_ind]
    x_val, y_val = train_text[val_ind], train.label.values[val_ind]
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;load &lt;code&gt;folding.csv&lt;/code&gt; dari responsitory kami.&lt;/p&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;# Load Folding Dataset from responsitory
folding = pd.read_csv(&amp;#x27;https://raw.githubusercontent.com/Hyuto/BDC-Satria-Data/master/Folding.csv&amp;#x27;)
folding.head()
&lt;/code&gt;&lt;/pre&gt;&lt;table&gt;&lt;thead&gt;&lt;tr&gt;&lt;th align=&quot;center&quot;&gt;ID&lt;/th&gt;&lt;th align=&quot;center&quot;&gt;Fold 1&lt;/th&gt;&lt;th align=&quot;center&quot;&gt;Fold 2&lt;/th&gt;&lt;th align=&quot;center&quot;&gt;Fold 3&lt;/th&gt;&lt;th align=&quot;center&quot;&gt;Fold 4&lt;/th&gt;&lt;th align=&quot;center&quot;&gt;Fold 5&lt;/th&gt;&lt;th align=&quot;center&quot;&gt;Fold 6&lt;/th&gt;&lt;th align=&quot;center&quot;&gt;Fold 7&lt;/th&gt;&lt;th align=&quot;center&quot;&gt;Fold 8&lt;/th&gt;&lt;th align=&quot;center&quot;&gt;Fold 9&lt;/th&gt;&lt;th align=&quot;center&quot;&gt;Fold 10&lt;/th&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td align=&quot;center&quot;&gt;71&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;1&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;1&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;1&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;1&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;1&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;1&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;1&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;1&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;1&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;0&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;center&quot;&gt;461&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;1&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;0&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;1&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;1&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;1&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;1&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;1&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;1&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;1&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;1&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;center&quot;&gt;495&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;1&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;1&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;1&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;1&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;1&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;1&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;1&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;1&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;1&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;0&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;center&quot;&gt;550&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;1&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;1&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;1&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;1&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;0&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;1&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;1&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;1&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;1&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;1&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;center&quot;&gt;681&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;1&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;1&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;1&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;0&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;1&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;1&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;1&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;1&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;1&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;1&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;h3 id=&quot;train--evaluate-model&quot;&gt;Train &amp;amp; Evaluate Model&lt;/h3&gt;&lt;p&gt;Melakukan &lt;code&gt;training&lt;/code&gt; pada train data. Train dilakukan pada &lt;code&gt;fold 3&lt;/code&gt; &amp;amp; &lt;code&gt;fold 6&lt;/code&gt; karena kedua fold tersebut memberikan nilai akurasi yang cukup baik dari model pada &lt;code&gt;fold&lt;/code&gt; lainnya. Training memiliki konfihurasi sebagai berikut:&lt;/p&gt;&lt;pre&gt;&lt;code&gt;epochs = 15
steps_per_epoch = len(x_train)//BATCH_SIZE:128 (jika menggunakann TPU)
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Training dilakukan dengan memperhatikan nilai &lt;code&gt;val_accuracy&lt;/code&gt; pada setiap epochnya. Epoch dengan score &lt;code&gt;val_accuracy&lt;/code&gt; tertinggi akan di save &lt;code&gt;weight&lt;/code&gt; - nya untuk diload saat melakukan evaluasi dan pemrediksian terhadap data test.&lt;/p&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;# Init
scores_valid, scores_test, CMS = [], [], []
HISTORY = []

# Test Dataset
test_dataset = (tf.data.Dataset
    .from_tensor_slices((regular_encode(test_text)))
    .batch(BATCH_SIZE)
)

for i in range(1, 11):
    if i in [3, 6]: # Ambil Fold 3 dan 6
        K.clear_session()
        tf.tpu.experimental.initialize_tpu_system(tpu)

        fold = f&amp;#x27;Fold {i}&amp;#x27;
        print(f&amp;#x27;[INFO] {fold}&amp;#x27;)

        # Split Dataset per Fold
        x_train = train_text[folding[fold].values == 1]
        y_train = train.label.values[folding[fold].values == 1]
        x_val = train_text[folding[fold].values == 0]
        y_val = train.label.values[folding[fold].values == 0]

        # Encoding &amp;amp; to TF Dataset
        train_dataset = (tf.data.Dataset
            .from_tensor_slices((regular_encode(x_train), y_train))
            .batch(BATCH_SIZE)
            .cache()
            .repeat()
            .shuffle(1024)
            .prefetch(AUTO)
        )
        valid_dataset = (tf.data.Dataset
            .from_tensor_slices((regular_encode(x_val), y_val))
            .batch(BATCH_SIZE)
            .cache()
            .prefetch(AUTO)
        )

        # Training
        with strategy.scope(): # Build &amp;amp; Scoope Model
            BERT = TFBertModel.from_pretrained(model_name)
            model = build_model(BERT)

        checkpoint = ModelCheckpoint(f&amp;#x27;Fold_{i}_best_model.h5&amp;#x27;, monitor=&amp;#x27;val_accuracy&amp;#x27;,
                                     save_best_only=True, save_weights_only=True,
                                     mode=&amp;#x27;max&amp;#x27;)

        history = model.fit(train_dataset, epochs = 15,
                            steps_per_epoch = len(x_train)//BATCH_SIZE,
                            validation_data = valid_dataset,
                            callbacks = [checkpoint])

        # Load Weights
        # model.load_weights(f&amp;#x27;Fold_{i}_best_model.h5&amp;#x27;) # Trained Best Weights
        model.load_weights(f&amp;#x27;../input/modelku/{fold}_best_model.h5&amp;#x27;) # Catatan Cakrawala Weights

        # Predict
        pred_val  = model.predict(valid_dataset)  # Valid
        pred_test = model.predict(test_dataset)   # Test

        scores_valid.append(pred_val.flatten())
        scores_test.append(pred_test.flatten())
        CMS.append(confusion_matrix(y_val, np.array(pred_val.flatten() &amp;gt;= .5, dtype=&amp;#x27;int&amp;#x27;),
                                    normalize = &amp;#x27;true&amp;#x27;))
        HISTORY.append(history.history)

print(f&amp;#x27;[INFO] Done&amp;#x27;)
&lt;/code&gt;&lt;/pre&gt;&lt;pre&gt;&lt;code&gt;[INFO] Fold 3
Epoch 1/15
29/29 [==============================] - 51s 2s/step - loss: 0.6275 - accuracy: 0.7686 - val_loss: 0.4344 - val_accuracy: 0.8298
Epoch 2/15
29/29 [==============================] - 8s 277ms/step - loss: 0.4023 - accuracy: 0.8413 - val_loss: 0.4263 - val_accuracy: 0.8487
Epoch 3/15
29/29 [==============================] - 6s 199ms/step - loss: 0.3162 - accuracy: 0.8770 - val_loss: 0.4730 - val_accuracy: 0.8440
Epoch 4/15
29/29 [==============================] - 6s 200ms/step - loss: 0.2479 - accuracy: 0.9083 - val_loss: 0.5341 - val_accuracy: 0.7967
Epoch 5/15
29/29 [==============================] - 6s 202ms/step - loss: 0.1471 - accuracy: 0.9448 - val_loss: 0.5429 - val_accuracy: 0.8322
Epoch 6/15
29/29 [==============================] - 6s 201ms/step - loss: 0.0654 - accuracy: 0.9777 - val_loss: 0.7967 - val_accuracy: 0.8227
Epoch 7/15
29/29 [==============================] - 6s 203ms/step - loss: 0.0401 - accuracy: 0.9857 - val_loss: 0.8853 - val_accuracy: 0.8132
Epoch 8/15
29/29 [==============================] - 6s 201ms/step - loss: 0.0480 - accuracy: 0.9855 - val_loss: 0.8994 - val_accuracy: 0.8369
Epoch 9/15
29/29 [==============================] - 6s 200ms/step - loss: 0.0191 - accuracy: 0.9942 - val_loss: 0.9181 - val_accuracy: 0.8463
Epoch 10/15
29/29 [==============================] - 6s 204ms/step - loss: 0.0096 - accuracy: 0.9978 - val_loss: 1.0425 - val_accuracy: 0.8369
Epoch 11/15
29/29 [==============================] - 6s 198ms/step - loss: 0.0125 - accuracy: 0.9958 - val_loss: 1.0749 - val_accuracy: 0.8180
Epoch 12/15
29/29 [==============================] - 6s 200ms/step - loss: 0.0113 - accuracy: 0.9968 - val_loss: 1.2689 - val_accuracy: 0.8369
Epoch 13/15
29/29 [==============================] - 6s 201ms/step - loss: 0.0201 - accuracy: 0.9938 - val_loss: 1.1791 - val_accuracy: 0.8392
Epoch 14/15
29/29 [==============================] - 6s 198ms/step - loss: 0.0131 - accuracy: 0.9956 - val_loss: 1.2603 - val_accuracy: 0.8392
Epoch 15/15
29/29 [==============================] - 6s 202ms/step - loss: 0.0153 - accuracy: 0.9951 - val_loss: 1.2632 - val_accuracy: 0.8203
[INFO] Fold 6
Epoch 1/15
29/29 [==============================] - 26s 912ms/step - loss: 0.5667 - accuracy: 0.7853 - val_loss: 0.4016 - val_accuracy: 0.8463
Epoch 2/15
29/29 [==============================] - 6s 205ms/step - loss: 0.3648 - accuracy: 0.8605 - val_loss: 0.4023 - val_accuracy: 0.8369
Epoch 3/15
29/29 [==============================] - 27s 935ms/step - loss: 0.3321 - accuracy: 0.8733 - val_loss: 0.3992 - val_accuracy: 0.8463
Epoch 4/15
29/29 [==============================] - 6s 203ms/step - loss: 0.2438 - accuracy: 0.9075 - val_loss: 0.4846 - val_accuracy: 0.8203
Epoch 5/15
29/29 [==============================] - 6s 203ms/step - loss: 0.1393 - accuracy: 0.9533 - val_loss: 0.5838 - val_accuracy: 0.8227
Epoch 6/15
29/29 [==============================] - 6s 204ms/step - loss: 0.1016 - accuracy: 0.9617 - val_loss: 0.6281 - val_accuracy: 0.8109
Epoch 7/15
29/29 [==============================] - 6s 200ms/step - loss: 0.0481 - accuracy: 0.9834 - val_loss: 0.7427 - val_accuracy: 0.8180
Epoch 8/15
29/29 [==============================] - 6s 201ms/step - loss: 0.0360 - accuracy: 0.9875 - val_loss: 0.7745 - val_accuracy: 0.8227
Epoch 9/15
29/29 [==============================] - 6s 202ms/step - loss: 0.0177 - accuracy: 0.9957 - val_loss: 0.8515 - val_accuracy: 0.8014
Epoch 10/15
29/29 [==============================] - 6s 203ms/step - loss: 0.0098 - accuracy: 0.9959 - val_loss: 0.9747 - val_accuracy: 0.8156
Epoch 11/15
29/29 [==============================] - 6s 205ms/step - loss: 0.0054 - accuracy: 0.9981 - val_loss: 1.0571 - val_accuracy: 0.8274
Epoch 12/15
29/29 [==============================] - 6s 202ms/step - loss: 0.0064 - accuracy: 0.9986 - val_loss: 1.1008 - val_accuracy: 0.8440
Epoch 13/15
29/29 [==============================] - 6s 205ms/step - loss: 0.0073 - accuracy: 0.9975 - val_loss: 1.0765 - val_accuracy: 0.8274
Epoch 14/15
29/29 [==============================] - 6s 205ms/step - loss: 0.0099 - accuracy: 0.9970 - val_loss: 1.0959 - val_accuracy: 0.8392
Epoch 15/15
29/29 [==============================] - 6s 204ms/step - loss: 0.0075 - accuracy: 0.9976 - val_loss: 1.0887 - val_accuracy: 0.8251
[INFO] Done
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Meninjau &lt;code&gt;train history&lt;/code&gt; pada model setiap &lt;code&gt;fold&lt;/code&gt; - nya&lt;/p&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;fig, (ax1, ax2) = plt.subplots(2, 1, figsize = (12, 10))
for i, fold in enumerate([3, 6]):
    ax1.plot(range(1, 16), HISTORY[i][&amp;#x27;loss&amp;#x27;], label = f&amp;#x27;Fold {fold} loss&amp;#x27;)
    ax1.plot(range(1, 16), HISTORY[i][&amp;#x27;val_loss&amp;#x27;], label = f&amp;#x27;Fold {fold} val_loss&amp;#x27;)
    ax2.plot(range(1, 16), HISTORY[i][&amp;#x27;accuracy&amp;#x27;], label = f&amp;#x27;Fold {fold} accuracy&amp;#x27;)
    ax2.plot(range(1, 16), HISTORY[i][&amp;#x27;val_accuracy&amp;#x27;], label = f&amp;#x27;Fold {fold} val_accuracy&amp;#x27;)
ax1.set_title(&amp;#x27;Loss at Training&amp;#x27;, fontsize = 14)
ax2.set_title(&amp;#x27;Accuracy at Training&amp;#x27;, fontsize = 14)
ax1.legend()
ax2.legend()
fig.show()
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;span class=&quot;gatsby-resp-image-wrapper&quot; style=&quot;position:relative;display:block;margin-left:auto;margin-right:auto;max-width:630px&quot;&gt;
      &lt;a class=&quot;gatsby-resp-image-link&quot; href=&quot;/static/599ae3a41fde39efa22e75626a02c479/6bbf7/output_156_0.png&quot; style=&quot;display:block&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;
    &lt;span class=&quot;gatsby-resp-image-background-image&quot; style=&quot;padding-bottom:82.91139240506328%;position:relative;bottom:0;left:0;background-image:url(&amp;#x27;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAARCAYAAADdRIy+AAAACXBIWXMAAAsSAAALEgHS3X78AAADL0lEQVQ4y4VUS2/bRhBm0UMP/REFepDzHwLUqG30nIMdpPceiv43H3xq4UbWI1biOKdCQlxHlixRosTlksul+NzHzBRklcpJA3iAD7skd4Yz832zzuur62+XfvhsvV4/j6LoWEp5IoRoIKU8Xnnei+vrt7/0+71fLy7avy2X3s9JXh7Hm/Qk5t5JHK5qHMto/Zxz9szp9Tp7AfOqyWRCy+WSrLUEADsgEtLOACxBVZBNBZloTTYJycQBWcFIFblyuhftVpYIwTknzrkFa6H2I8TtaoGsAlQpQCoABAOIA4BsU/8LUGvAOgskMsbETrvd3jMW4o8JIBHWoHotNwgJRxuHaNMNQpYiGtN8e4jt+Tpg4lwPrlppnsk4S4jAAOmSaCPIRj5ZGRJUihCQEHeF1/vPsAvYuRy0dJlHsT+n9WJshD+3ZZrYpllEnwLR4pdhtgGF0+12n2ht4GO9hTGUmpJEsaG0KiitSspVSYVWBPCQnv+bMQad09PT77Is+70sy9dlUfS1Uv2yKAZlVfVXEb9cRLwfZbLjCf7K34QdPwn6bugN3MjvT9lyMAlW/SnzLmfcexNvkj8cInoUjuN8tcXXj549Ozv7PuD8zWKxeD+dTodCiJExZgjWjpQ2w1UY3k49dzb3vbub+WR27y/GfhIN5xEbuSIYzkI2mkfB0BXBe5lurpzLV709KZiqRT2dTkkI0fQj0xWxVFJS5VSBocoaesystcZpv2y3VFlGWivSShkwxsoysyxNLOJ/DNek1WzCIyzHzvn5+Z6xVtK/XhAUWZPVJ5r7TMlfsJ0OX/a6LatUnIs1eYs7KERQR2gmAsE2g4PWIqocyRRIukA0ClFVCDJCLBKENAasCjJGS+f8z86TqsykH67IVhVQnqANVwjCRxAMQXKEOECQIUJaO8v6mSBmiGVGzbsiR0xjMkWWOH+9e9eSWbZJldkNs1HNLVMLGbQmQGhqskoTaNPsYVsnPGiD0jpz1r73jb9mT5eue+jO5z8y398fT+4PQ873Z7P7A9fzDnjA9z/c3h4FUbQ/c92Dv29ujkQY/jAaDn8KGNu/G48P78YfjhhjT/8B42rNSby+gN4AAAAASUVORK5CYII=&amp;#x27;);background-size:cover;display:block&quot;&gt;&lt;/span&gt;
  &lt;img class=&quot;gatsby-resp-image-image&quot; alt=&quot;png&quot; title=&quot;png&quot; src=&quot;/static/599ae3a41fde39efa22e75626a02c479/f058b/output_156_0.png&quot; srcSet=&quot;/static/599ae3a41fde39efa22e75626a02c479/c26ae/output_156_0.png 158w,/static/599ae3a41fde39efa22e75626a02c479/6bdcf/output_156_0.png 315w,/static/599ae3a41fde39efa22e75626a02c479/f058b/output_156_0.png 630w,/static/599ae3a41fde39efa22e75626a02c479/6bbf7/output_156_0.png 716w&quot; sizes=&quot;(max-width: 630px) 100vw, 630px&quot; style=&quot;width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0&quot; loading=&quot;lazy&quot; decoding=&quot;async&quot;/&gt;
  &lt;/a&gt;
    &lt;/span&gt;&lt;/p&gt;&lt;p&gt;Confussion Matrix prediksian model terhadap data valid per - &lt;code&gt;foldnya&lt;/code&gt;&lt;/p&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;# Plotting Confussion Matrix
fig, ax = plt.subplots(1, 2, figsize = (14, 6))
i = 0
for j in range(10):
    if j in [2, 5]:
        sns.heatmap(CMS[i], annot=True, cmap = plt.cm.Blues, ax = ax[i])
        ax[i].set_title(f&amp;#x27;Normalized Confussion Matrix Valid Data Fold {j + 1}&amp;#x27;)
        ax[i].set_xlabel(&amp;#x27;Predicted&amp;#x27;)
        ax[i].set_ylabel(&amp;#x27;Actual&amp;#x27;)
        i += 1
fig.show()
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;span class=&quot;gatsby-resp-image-wrapper&quot; style=&quot;position:relative;display:block;margin-left:auto;margin-right:auto;max-width:630px&quot;&gt;
      &lt;a class=&quot;gatsby-resp-image-link&quot; href=&quot;/static/347db183fa9c90059caa30a5a3489834/97655/output_158_0.png&quot; style=&quot;display:block&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;
    &lt;span class=&quot;gatsby-resp-image-background-image&quot; style=&quot;padding-bottom:47.46835443037975%;position:relative;bottom:0;left:0;background-image:url(&amp;#x27;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAJCAYAAAAywQxIAAAACXBIWXMAAAsSAAALEgHS3X78AAACrUlEQVQoz02PW0jTARjFv8xpYpb2Uk8RiYROzUwzMLDU1KCHAt8y0tJkpjN1EdpD0UPhhYK5oXPpnGmGmF3AEkzTdG5zKkoTUue8LAunWabM7X878V8vPRw+OPw43znUOrW6v3nMsa2zONA+uYLCTisSnxiQrDQiuda0EnxZXZ6Qlu1bPzRLjRZHw7NRB/Tj31AzYEea2oyUWhPS6yyIvPNWT+GlUqp+MxyiMy86tQYb2saX2FsdU1xizWcmVWnA2afDc0eK35UlXMyWdNpcpDXa1VrjAppMdk91/xyXqjRw50XVjrAnH/ZpAjMbIkjZPx2sGVlYqxueh868wBd0WoUk1SiXXj+G1DqL7Uz1oOJcZo6fxrhIaoNdrTbYUT9iZx/3zQkXGsaFDM2YkKEZ55NrTdrY8ldSululOvTJ9tPZO+OEyfGbl+sMCM3X89G3XyCyqG3+SrOlTP++2xcAfZxbU/XOrGFwYYPVj9gRW9ouRBeLXCuf8qhHW9o9E05fpr8eYHms7TACAPCKqk7Q4av8nmgZdkfk2R50jSmGTB/8xECGh8rNihhYi3UZgTEFgq/0JijsupAkq9P2/NiW0vJ3Z7CLEVbXN10iyChqulgKu+EJOV0CSbRstm1gphTweBtue3jVposBwwtus9XBBsXL2X3xxayPNJ+5VNak2QLCaeOPSwz0bLr+vVbUdIFCc7A3Tg5JlGy1b2Lp3sai0dvQxQiN227ey5mtDgTFyRFwohAUnoesipZWAFHk5hCww0K56WKfA9CVVHa00NHspsDYwlZJlKxy0raaDmCXGLjDImvLzb0E0GiYtOuD4oqa/Y8X6OhYblPu/bZsAAfJO8XNea8oIvKhmEoKPKUgSWQ+TdjWRY9YjqNfWzv/cxT1GuQfU0QkLaRrFc1e/y8zkMkwXho3pQAAAABJRU5ErkJggg==&amp;#x27;);background-size:cover;display:block&quot;&gt;&lt;/span&gt;
  &lt;img class=&quot;gatsby-resp-image-image&quot; alt=&quot;png&quot; title=&quot;png&quot; src=&quot;/static/347db183fa9c90059caa30a5a3489834/f058b/output_158_0.png&quot; srcSet=&quot;/static/347db183fa9c90059caa30a5a3489834/c26ae/output_158_0.png 158w,/static/347db183fa9c90059caa30a5a3489834/6bdcf/output_158_0.png 315w,/static/347db183fa9c90059caa30a5a3489834/f058b/output_158_0.png 630w,/static/347db183fa9c90059caa30a5a3489834/97655/output_158_0.png 819w&quot; sizes=&quot;(max-width: 630px) 100vw, 630px&quot; style=&quot;width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0&quot; loading=&quot;lazy&quot; decoding=&quot;async&quot;/&gt;
  &lt;/a&gt;
    &lt;/span&gt;&lt;/p&gt;&lt;h2 id=&quot;ensemble&quot;&gt;Ensemble&lt;/h2&gt;&lt;blockquote&gt;&lt;p&gt;Ensemble is methods use multiple learning algorithms to obtain better predictive performance than could be obtained from any of the constituent learning algorithms alone.&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;Untuk mendapatkan hasil yang lebih baik kami melakukan &lt;code&gt;ensembling&lt;/code&gt; pada hasil prediksi model kami. Hasil model yang di &lt;code&gt;ensembling&lt;/code&gt; adalah hasil peramalan data test pada model &lt;code&gt;EfficientNetB7&lt;/code&gt; dan model &lt;code&gt;Bert&lt;/code&gt; pada fold &lt;code&gt;3&lt;/code&gt; dan &lt;code&gt;6&lt;/code&gt;. Metode ensemble yang digunakan adalah dengan mengambil rata - rata probabilitas dari hasil prediksian ketiga model tersebut.&lt;/p&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;ensemble = (EfficientNet_pred.flatten() + scores_test[0] + scores_test[1]) / 3
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&quot;mencari-threshold&quot;&gt;Mencari Threshold&lt;/h2&gt;&lt;blockquote&gt;&lt;p&gt;The decision for converting a predicted probability or scoring into a class label is governed by a parameter referred to as the â€œthresholdâ€ The default value for the threshold is 0.5 for normalized predicted probabilities or scores in the range between 0 or 1.&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;Kami mendapatkan beberapa indikasi bahwa untuk mendapatkan hasil yang lebih baik perlu dilakukan penggeseran terhadap tresholdnya:&lt;/p&gt;&lt;ol&gt;&lt;li&gt;Kelas pada label yang tidak berimbang&lt;br/&gt;
Kelas &lt;code&gt;0&lt;/code&gt; pada label memiliki frekuensi yang sangat kecil, berbeda jauh jika dibandingkan dengan pada kelas &lt;code&gt;1&lt;/code&gt;. Kami menduga hal ini berpengaruh terhadap berat masing - masing kelas pada saat dilakukan pemodelan &lt;code&gt;tidak sama&lt;/code&gt;.&lt;/li&gt;&lt;li&gt;Nilai loss dan akurasi yang berbanding lurus pada saat training&lt;br/&gt;
Pada saat training kami mendapatkan ketika &lt;code&gt;val_loss&lt;/code&gt; (validation loss) menaik &lt;code&gt;val_accuracy&lt;/code&gt; juga tetap menaik untuk beberapa saat. Hal ini cukup aneh karena seharusnya ketika nilai loss menaik maka nilai akurasi menurun. Hal ini membawa kami pada kesimpulan bahwa &lt;code&gt;threshold&lt;/code&gt;-nya perlu untuk dirubah.&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;Mengecek distribusi peluang &lt;code&gt;ensemble&lt;/code&gt;&lt;/p&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;plt.figure(figsize = (8,6))
sns.distplot(ensemble)
plt.title(&amp;#x27;Distribusi Sebaran Peluang Prediksi&amp;#x27;, fontsize = 14)
plt.show()
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;span class=&quot;gatsby-resp-image-wrapper&quot; style=&quot;position:relative;display:block;margin-left:auto;margin-right:auto;max-width:484px&quot;&gt;
      &lt;a class=&quot;gatsby-resp-image-link&quot; href=&quot;/static/2023d3371a898bd7709f417b7372fcb3/ff42b/output_163_0.png&quot; style=&quot;display:block&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;
    &lt;span class=&quot;gatsby-resp-image-background-image&quot; style=&quot;padding-bottom:77.84810126582278%;position:relative;bottom:0;left:0;background-image:url(&amp;#x27;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAQCAYAAAAWGF8bAAAACXBIWXMAAAsSAAALEgHS3X78AAACK0lEQVQ4y62Tz2sTQRTHB8zGHxCldtNsfhjaWtFLoR7qIbWoFy968m8wED3kFOJJCISYXBJ/QCKW6EEUPUVB9KheLB4UJeDBtJZNZm1KdrM7u9ndZHcyu7JNo+KPQ0kGHt8vM48P7w3vgUgkAorFIlUqldzlcpnK5/PuYDA47fP5jjnq9/tnHR8KhcKOMgwzFwgEZrxe74lwOHzkxctXrscP7lFvnz9yF27foUClUvHIsvyu2+3WNU2r6bq+JssyK0lSfRjtdhsOvSAInKOiKDYQQqyT31HVGuqodUWR34BoNBrCGPP2CEc3+7agmrZF+hzIZrO0aZrQebAsC1uWRXYTtm0TrYdxS+nZpI/XQTwe95um+X0HSHZTmWVZ26oZmOwAN0Amk/GOFRiLxQLjAPIdYwBMp9MjV6j2MIGiPgAmk0nfqECla5IqhwbAXC5HjxWYSCSYUYFIN8mnhmRbDjCVSv31h8NER3/3f8J+AjWDfGTFX0DDMLh/DTb5zzAP7wkZDLakGfgzlJyWv4FC4SaNMW6NsnqsoNlVTnZWD4JrySQtIfRBUtQtHin1TUGGHI+go1W2BSGPoIA6sNmWt4PdEuGXBg83miJc32zDr5wAn6yu1d/Xmi1BlFYBfXhiT2nl/r7jC6eYi1eu719OPXN7ZhemAkuXPIC6QXnmFun50+cnLr/uu5j5pUkQOjkJ6LsuMHP2IDh0lAGLt6jpC1cPLJ85N7Xy8OneH2fdSSvhaB+UAAAAAElFTkSuQmCC&amp;#x27;);background-size:cover;display:block&quot;&gt;&lt;/span&gt;
  &lt;img class=&quot;gatsby-resp-image-image&quot; alt=&quot;png&quot; title=&quot;png&quot; src=&quot;/static/2023d3371a898bd7709f417b7372fcb3/ff42b/output_163_0.png&quot; srcSet=&quot;/static/2023d3371a898bd7709f417b7372fcb3/c26ae/output_163_0.png 158w,/static/2023d3371a898bd7709f417b7372fcb3/6bdcf/output_163_0.png 315w,/static/2023d3371a898bd7709f417b7372fcb3/ff42b/output_163_0.png 484w&quot; sizes=&quot;(max-width: 484px) 100vw, 484px&quot; style=&quot;width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0&quot; loading=&quot;lazy&quot; decoding=&quot;async&quot;/&gt;
  &lt;/a&gt;
    &lt;/span&gt;&lt;/p&gt;&lt;p&gt;Dapat dilihat dari plot diatas bahwa sebaran peluang mulai memusat pada selang &lt;code&gt;0.65&lt;/code&gt; - &lt;code&gt;1&lt;/code&gt;.&lt;/p&gt;&lt;p&gt;Maka dari itu kami putuskan untuk mengambil &lt;code&gt;0.65&lt;/code&gt; sebagai &lt;code&gt;threshold&lt;/code&gt;-nya.&lt;/p&gt;&lt;h2 id=&quot;membuat-submission&quot;&gt;Membuat Submission&lt;/h2&gt;&lt;p&gt;Membuat Submission File.&lt;/p&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;test[&amp;#x27;prediksi&amp;#x27;] = np.array(ensemble &amp;gt; .65, dtype = &amp;#x27;int&amp;#x27;)
submission_temp = pd.read_csv(&amp;#x27;../input/data-bdc/Submission Template.csv&amp;#x27;)[[&amp;#x27;ID&amp;#x27;]]
hasil = submission_temp.merge(test[[&amp;#x27;ID&amp;#x27;, &amp;#x27;prediksi&amp;#x27;]], on = &amp;#x27;ID&amp;#x27;)
hasil.head()
&lt;/code&gt;&lt;/pre&gt;&lt;table&gt;&lt;thead&gt;&lt;tr&gt;&lt;th align=&quot;center&quot;&gt;ID&lt;/th&gt;&lt;th align=&quot;center&quot;&gt;prediksi&lt;/th&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td align=&quot;center&quot;&gt;56&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;1&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;center&quot;&gt;1129&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;1&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;center&quot;&gt;8468&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;1&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;center&quot;&gt;9527&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;1&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;center&quot;&gt;11152&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;1&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;hasil.to_csv(&amp;#x27;Catatan Cakrawala BDC - Ensemble.csv&amp;#x27;, index = False)
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&quot;kesimpulan--saran&quot;&gt;Kesimpulan &amp;amp; Saran&lt;/h2&gt;&lt;h3 id=&quot;kesimpulan&quot;&gt;Kesimpulan&lt;/h3&gt;&lt;ol&gt;&lt;li&gt;Data memiliki frekuensi kelas yang &lt;code&gt;tidak berimbang&lt;/code&gt; sehingga perlu di perhatikan saat pembuatan model&lt;/li&gt;&lt;li&gt;Dengan menggukan model yang kami buat didapatkan hasil yang cukup baik dengan f1 score 94 % namun memerlukan komputasi yang cukup berat dan biaya yang mahal untuk ukuran data yang tidak terlalu besar.&lt;/li&gt;&lt;/ol&gt;&lt;h3 id=&quot;saran&quot;&gt;Saran&lt;/h3&gt;&lt;ol&gt;&lt;li&gt;Jika memungkinkan diperlukan &lt;code&gt;penambahan data&lt;/code&gt; pada label bukan hoax agar frekuensi setiap kelas pada data tidak berbanding cukup jauh&lt;/li&gt;&lt;li&gt;Menggunakan model &lt;code&gt;multimodal&lt;/code&gt; karena hasil pada ensembel model gambar dan model teks menunjukkan hasil yang lebih baik.&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;Â© Catatan Cakrawala 2020&lt;/p&gt;</content:encoded></item><item><title><![CDATA[Sentiment Detector [ID]]]></title><description><![CDATA[Sentiment detector built with focused corona topic dataset using SVM]]></description><link>https://Hyuto.github.io/showcase/sa-corona</link><guid isPermaLink="false">https://Hyuto.github.io/showcase/sa-corona</guid><pubDate>Sat, 12 Sep 2020 05:00:00 GMT</pubDate><content:encoded>
                        &lt;div&gt;
                          &lt;h2&gt;Sentiment Detector [ID]&lt;/h2&gt;
                          &lt;p&gt;Sentiment detector built with focused corona topic dataset using SVM&lt;/p&gt;
                        &lt;/div&gt;
                      </content:encoded></item><item><title><![CDATA[MNIST Digit Classifier Using Keras, Tensorflow, and TPU]]></title><description><![CDATA[This notebook is basically my notebook run on  kaggle  so if you want to try
and run the code with same environment as mine go to linkâ€¦]]></description><link>https://Hyuto.github.io/blog/cnn-keras-cv-0-996-tpu/</link><guid isPermaLink="false">https://Hyuto.github.io/blog/cnn-keras-cv-0-996-tpu/</guid><pubDate>Tue, 25 Aug 2020 15:56:36 GMT</pubDate><content:encoded>&lt;p&gt;This notebook is basically my notebook run on &lt;a href=&quot;https://www.kaggle.com/&quot;&gt;kaggle&lt;/a&gt; so if you want to try
and run the code with same environment as mine go to link bellow.&lt;/p&gt;&lt;p&gt;Kaggle Notebook : &lt;a href=&quot;https://www.kaggle.com/wahyusetianto/cnn-keras-cv-0-996-tpu&quot;&gt;CNN Keras CV - 0.996 [TPU]&lt;/a&gt;&lt;/p&gt;&lt;p&gt;P.S. Don&amp;#x27;t forget to &lt;em&gt;upvote&lt;/em&gt; if you like it ðŸ˜Š.&lt;/p&gt;&lt;h2 id=&quot;overview&quot;&gt;Overview&lt;/h2&gt;&lt;p&gt;This kernel is on purpose to build model for MNIST digits dataset. In this kernel we&amp;#x27;re gonna do some preprocessing then make augmentation datagen so our model didn&amp;#x27;t train on the same image data, then at each of our model(here we use 15 folds so there will be 15 models) to make prediction for test dataset and by the end we&amp;#x27;re gonna do ensembles for the prediction.&lt;/p&gt;&lt;h2 id=&quot;web-app&quot;&gt;Web App&lt;/h2&gt;&lt;p&gt;You can visit my web app for the live prediction by the best model trained on this kernel run on Tensorflow.js &lt;a href=&quot;https://hyuto.github.io/showcase/digit-recognizer/&quot;&gt;Digit Recognizer&lt;/a&gt;.&lt;/p&gt;&lt;h2 id=&quot;train-on-tpu&quot;&gt;Train on TPU!!&lt;/h2&gt;&lt;p&gt;why? Because it&amp;#x27;s &lt;strong&gt;faster&lt;/strong&gt;. While people usually train on GPU for image related things, in this notebook we try to do things on TPU and see how it affect the Accuracy.&lt;/p&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;import random
import numpy as np
import pandas as pd
from timeit import default_timer
from sklearn.model_selection import KFold

# Plot
import matplotlib.pyplot as plt
import seaborn as sns

# Tensorflow and Keras
import tensorflow as tf
from tensorflow import keras
import tensorflow.keras.backend as K
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.models import Sequential, load_model
from tensorflow.keras.layers import Dense, Dropout, Flatten, Conv2D
from tensorflow.keras.layers import MaxPool2D, BatchNormalization
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.callbacks import LearningRateScheduler, ModelCheckpoint

print(f&amp;#x27;Using Tensorflow Version : {tf.__version__}&amp;#x27;)
print(f&amp;#x27;Using Keras Version      : {keras.__version__}&amp;#x27;)
&lt;/code&gt;&lt;/pre&gt;&lt;pre&gt;&lt;code&gt;Using Tensorflow Version : 2.2.0
Using Keras Version      : 2.3.0-tf
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&quot;load-the-data&quot;&gt;Load the Data&lt;/h2&gt;&lt;p&gt;Load the MNIST data&lt;/p&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;train = pd.read_csv(&amp;#x27;../input/digit-recognizer/train.csv&amp;#x27;)
test = pd.read_csv(&amp;#x27;../input/digit-recognizer/test.csv&amp;#x27;)
train.head()
&lt;/code&gt;&lt;/pre&gt;&lt;pre&gt;&lt;code&gt;5 rows Ã— 785 columns
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&quot;preprocess&quot;&gt;Preprocess&lt;/h2&gt;&lt;h3 id=&quot;specifying-x-and-y&quot;&gt;Specifying X and y&lt;/h3&gt;&lt;p&gt;for y we need to encode to one hot vector. In keras we have function &lt;code&gt;to_categorical&lt;/code&gt; for this.&lt;/p&gt;&lt;p&gt;Example :&lt;/p&gt;&lt;pre&gt;&lt;code&gt;y = [1, 0, 4]
to_categorical(y)

# Output:
[
 [0,1,0,0,0], # 1
 [1,0,0,0,0], # 0
 [0,0,0,0,1], # 4
]
&lt;/code&gt;&lt;/pre&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;# Specifying X and y
X = train.drop([&amp;#x27;label&amp;#x27;], axis = 1)
y = to_categorical(train[&amp;#x27;label&amp;#x27;].values) # To Categorical y
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&quot;normalizing-images&quot;&gt;Normalizing Images&lt;/h3&gt;&lt;p&gt;Data normalization is an important step to ensures that each input parameter (pixel, in this case) has a similar data distribution. This makes convergence faster while training the network.&lt;/p&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;# Normalize
X = X / 255.0
X_test = test / 255.0
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&quot;reshape&quot;&gt;Reshape&lt;/h3&gt;&lt;p&gt;Train and test images (28px x 28px) has been stock into pandas. Dataframe as 1D vectors of 784 values. We reshape all data to 28x28x1 3D matrices.&lt;/p&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;# Reshape Array
X = X.values.reshape(-1, 28, 28, 1)
X_test = X_test.values.reshape(-1, 28, 28, 1)
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&quot;lets-take-a-look-at-our-data&quot;&gt;Let&amp;#x27;s take a look at our data&lt;/h3&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;fig, axes = plt.subplots(ncols=10, nrows=5, figsize = (13, 7))
init = 0
for i in range(5):
    j = 0
    for k in range(2):
        ind = random.choices(train.label[train.label == init].index, k = 5)
        init += 1
        while j &amp;lt; len(ind):
            axes[i, k*5 + j].imshow(X[ind[j]][:,:,0], cmap=plt.cm.binary)
            axes[i, k*5 + j].axis(&amp;#x27;off&amp;#x27;)
            j += 1
        j = 0
fig.tight_layout()
plt.show()
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;span class=&quot;gatsby-resp-image-wrapper&quot; style=&quot;position:relative;display:block;margin-left:auto;margin-right:auto;max-width:630px&quot;&gt;
      &lt;a class=&quot;gatsby-resp-image-link&quot; href=&quot;/static/0995ecc1e7de346086519e39d71607ea/cc488/output_12_0.png&quot; style=&quot;display:block&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;
    &lt;span class=&quot;gatsby-resp-image-background-image&quot; style=&quot;padding-bottom:52.53164556962025%;position:relative;bottom:0;left:0;background-image:url(&amp;#x27;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAALCAYAAAB/Ca1DAAAACXBIWXMAAAsSAAALEgHS3X78AAADBElEQVQozxWS3VPiVgDF869Xtzq7XTu7+1B37IMz4kpgpUZnG7aZKhgRhQQDCIR8EGgwueTrkhByEyaB28n775w558wh2u32axiGZLPZHGRZVqEoqj+fz8nRaCQlSfK93W4rcRwXJ5OJGEVRxfO81+VyWXIcp79er/+CEA4AAH8jhB4URekTz8/PQJblnzzPW/1+/+7k5MS0bfsnx3EQQvhvo9HwN5tNVZZlZ7Va3em6DrbbLaPrupll2Z2iKAuEUFNRlKHruoAQBKEEADjgeb4MAPggSRIZx/GBqqoVjPH7l5eXq81mcyAIwnfLsj64rktijA91XSdN0/wtDMOSqqqfkiT5M9cSxWIxGo/HD6enp8iyrNbV1VVo2/YDTdOp4zhNkiS3q9WKvbi42Hie15pOp+s4jhvD4TDMeZ7nc32/0+noPM+viSRJClEU7bmue75arQ4AAGfb7XYPQlhcLBbvDMMoxXH8i67rFxjjg5xHCO0HQXCWN/F9v4Ax/mhZ1h+maZ4RNE3jKIo6giDgbrf7enx8jCVJ6pTLZRwEQY9hGAwh5HNusVi8DodD7HmeMBqNsGmag9lshgEAqiiKHsdxmJhOp+e73W7fNM1v+Tbdbrdg2/a+ruulLMt+dV23nKbp3mQyKZqmeZg32e1278IwLKRpehgEwTfP845Go9GxZVnnBM/zvizLNZIkA9/3H2mahgihWqFQQMvlkm21WglC6I5l2bWmaQ2WZZeaptXH4zFUVfUxf8FgMOh0Op1JvV5fEpqmXWuadsRx3A2E8HOv16PSND2iKIre7XafFEWpRlH0UVXVH0mSfJZl+TqKot8dx6Ewxl8Mw7gxDOPrfD4vvL29XROz2UzDGOem0/V6Td/f36tJklD1ev0tDMMf1WrVsCyLEkXxvyiK6DwJhPCmUqmoDMPQiqJos9ks/yPPsuwkT2j6vl+VJAkEQXBbq9WMNE2rDMO4nucxl5eXMMsymuM4GyF022q1TNd1/6Eoynh6eroVRRE4jvPY6/UGsiwb/wOjpOMVzbCcsQAAAABJRU5ErkJggg==&amp;#x27;);background-size:cover;display:block&quot;&gt;&lt;/span&gt;
  &lt;img class=&quot;gatsby-resp-image-image&quot; alt=&quot;png&quot; title=&quot;png&quot; src=&quot;/static/0995ecc1e7de346086519e39d71607ea/f058b/output_12_0.png&quot; srcSet=&quot;/static/0995ecc1e7de346086519e39d71607ea/c26ae/output_12_0.png 158w,/static/0995ecc1e7de346086519e39d71607ea/6bdcf/output_12_0.png 315w,/static/0995ecc1e7de346086519e39d71607ea/f058b/output_12_0.png 630w,/static/0995ecc1e7de346086519e39d71607ea/cc488/output_12_0.png 928w&quot; sizes=&quot;(max-width: 630px) 100vw, 630px&quot; style=&quot;width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0&quot; loading=&quot;lazy&quot; decoding=&quot;async&quot;/&gt;
  &lt;/a&gt;
    &lt;/span&gt;&lt;/p&gt;&lt;p&gt;as you can see there some nice &amp;amp; bad hand written digits number at our dataset.&lt;/p&gt;&lt;h2 id=&quot;data-augmentation&quot;&gt;Data Augmentation&lt;/h2&gt;&lt;p&gt;we currently have about 42000 image data, let&amp;#x27;s multiply that value by doing some soft augmentation.&lt;/p&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;datagen = ImageDataGenerator(rotation_range=10,
                             zoom_range = 0.10,
                             width_shift_range=0.1,
                             height_shift_range=0.1)
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;let&amp;#x27;s take a look on our Augmentation datagen&lt;/p&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;def AUG_test(X, y):
    fig, axes = plt.subplots(1, 5, figsize = (10,5))
    axes[0].imshow(X[:,:,0], cmap=plt.cm.binary)
    axes[0].set_title(&amp;#x27;Actual&amp;#x27;)
    axes[0].axis(&amp;#x27;off&amp;#x27;)
    for i in range(1, 5):
        aug, _ = datagen.flow(X.reshape(1,28,28,1), y.reshape(1,10)).next()
        axes[i].imshow(aug.reshape(28,28),cmap=plt.cm.binary)
        axes[i].set_title(&amp;#x27;Augmented&amp;#x27;)
        axes[i].axis(&amp;#x27;off&amp;#x27;)
    fig.tight_layout()
    return plt.show()
&lt;/code&gt;&lt;/pre&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;AUG_test(X[0], y[0])
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;span class=&quot;gatsby-resp-image-wrapper&quot; style=&quot;position:relative;display:block;margin-left:auto;margin-right:auto;max-width:630px&quot;&gt;
      &lt;a class=&quot;gatsby-resp-image-link&quot; href=&quot;/static/710183c6067410e8862ad6a60e749cd2/3d4b6/output_18_0.png&quot; style=&quot;display:block&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;
    &lt;span class=&quot;gatsby-resp-image-background-image&quot; style=&quot;padding-bottom:22.78481012658228%;position:relative;bottom:0;left:0;background-image:url(&amp;#x27;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAFCAYAAABFA8wzAAAACXBIWXMAAAsSAAALEgHS3X78AAABPklEQVQY0zWPwUsCURDG908KokuXDh27RYfskuXfE54CxYuXlA6BmBIqhRt4adNyfT1SWFAQBDVCAtd1ffOLF3aZ+Wa+b+abcbTWifF4fAYcAntKqYTruqnFYnEK7M7n86Nms5kKgiAZhuEBsO95XlIpddHv94+BneFweNJqtS611ucOQLvdRin1DTRtHccx3W7XwkcR+bHA933W67UG3m1dr9f/Nfc2hGEo1WoVJwgC0uk0xpgJ0LBkNpuNPc+z8AH40lpLoVBgu+ylXC5TLBZjIDLG3AGSy+WiTqeDk8lkmM1mVrwAnhuNBnZge+kTsMzn80wm1o/P6XTq12q1P15EbKoMBgNKpdJfzxmNRr513mw2j0Cm1+t1V6vV6/aa6+Vy6VYqlQ7wYYy5BW4sFpFXEbFvXEVR9CYiL9bwF7WcWuUXOXHlAAAAAElFTkSuQmCC&amp;#x27;);background-size:cover;display:block&quot;&gt;&lt;/span&gt;
  &lt;img class=&quot;gatsby-resp-image-image&quot; alt=&quot;png&quot; title=&quot;png&quot; src=&quot;/static/710183c6067410e8862ad6a60e749cd2/f058b/output_18_0.png&quot; srcSet=&quot;/static/710183c6067410e8862ad6a60e749cd2/c26ae/output_18_0.png 158w,/static/710183c6067410e8862ad6a60e749cd2/6bdcf/output_18_0.png 315w,/static/710183c6067410e8862ad6a60e749cd2/f058b/output_18_0.png 630w,/static/710183c6067410e8862ad6a60e749cd2/3d4b6/output_18_0.png 712w&quot; sizes=&quot;(max-width: 630px) 100vw, 630px&quot; style=&quot;width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0&quot; loading=&quot;lazy&quot; decoding=&quot;async&quot;/&gt;
  &lt;/a&gt;
    &lt;/span&gt;&lt;/p&gt;&lt;h2 id=&quot;build-cnn-model&quot;&gt;Build CNN Model&lt;/h2&gt;&lt;p&gt;The CNN&amp;#x27;s in this kernel follow LeNet5&amp;#x27;s design on &lt;a href=&quot;https://www.kaggle.com/cdeotte&quot;&gt;Chris Deotte&lt;/a&gt; kernel &lt;a href=&quot;https://www.kaggle.com/cdeotte/25-million-images-0-99757-mnist&quot;&gt;here&lt;/a&gt;&lt;/p&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;def build_model():
    model = Sequential()
    model.add(Conv2D(32, kernel_size = 3, activation=&amp;#x27;relu&amp;#x27;,
                     input_shape = (28, 28, 1)))
    model.add(BatchNormalization())
    model.add(Conv2D(32, kernel_size = 3, activation=&amp;#x27;relu&amp;#x27;))
    model.add(BatchNormalization())
    model.add(Conv2D(32, kernel_size = 5, strides=2, padding=&amp;#x27;same&amp;#x27;,
                     activation=&amp;#x27;relu&amp;#x27;))
    model.add(BatchNormalization())
    model.add(Dropout(0.4))

    model.add(Conv2D(64, kernel_size = 3, activation=&amp;#x27;relu&amp;#x27;))
    model.add(BatchNormalization())
    model.add(Conv2D(64, kernel_size = 3, activation=&amp;#x27;relu&amp;#x27;))
    model.add(BatchNormalization())
    model.add(Conv2D(64, kernel_size = 5, strides=2, padding=&amp;#x27;same&amp;#x27;,
                     activation=&amp;#x27;relu&amp;#x27;))
    model.add(BatchNormalization())
    model.add(Dropout(0.4))

    model.add(Conv2D(128, kernel_size = 4, activation=&amp;#x27;relu&amp;#x27;))
    model.add(BatchNormalization())
    model.add(Flatten())
    model.add(Dropout(0.4))
    model.add(Dense(10, activation=&amp;#x27;softmax&amp;#x27;))

    model.compile(optimizer=&amp;quot;adam&amp;quot;, loss=&amp;quot;categorical_crossentropy&amp;quot;,
                  metrics=[&amp;quot;accuracy&amp;quot;])

    return model
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&quot;detect-and-instantiate-tpu-distribution-strategy&quot;&gt;Detect and Instantiate TPU Distribution Strategy&lt;/h2&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;# detect and init the TPU
tpu = tf.distribute.cluster_resolver.TPUClusterResolver()
tf.config.experimental_connect_to_cluster(tpu)
tf.tpu.experimental.initialize_tpu_system(tpu)

# instantiate a distribution strategy
tpu_strategy = tf.distribute.experimental.TPUStrategy(tpu)
&lt;/code&gt;&lt;/pre&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;# Config
EPOCHS = 45
BATCH_SIZE = 16 * tpu_strategy.num_replicas_in_sync
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&quot;training&quot;&gt;Training&lt;/h2&gt;&lt;p&gt;Here we use Kfold CV to split our data by 15 and build model at each fold. At the callbacks we use &lt;span class=&quot;math math-inline&quot;&gt;f(x) = 0.001 \times 0.95^x&lt;/span&gt; for our LR Scheduler and do Checkpoint at &lt;strong&gt;best &lt;em&gt;val_acc&lt;/em&gt;&lt;/strong&gt; score.&lt;/p&gt;&lt;p&gt;Note that Tensorflow distribution strategy haven&amp;#x27;t supported &lt;code&gt;ImageDataGenerator&lt;/code&gt; by this &lt;a href=&quot;https://github.com/tensorflow/tensorflow/issues/34346&quot;&gt;issue&lt;/a&gt; so instead use that at &lt;code&gt;fit_generator&lt;/code&gt; we just have to extract Augmentation data by looping through and done training by &lt;code&gt;fit&lt;/code&gt;.&lt;/p&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;# Init
scores, History = [], []
pred = np.zeros(shape = (len(test), 10))

# CV
cv = KFold(n_splits=15, shuffle = True, random_state = 42)
for fold, (train_index, val_index) in enumerate(cv.split(X)):
    start = default_timer()
    # Clear Session
    K.clear_session()
    tf.tpu.experimental.initialize_tpu_system(tpu)

    # Splitting
    X_train , y_train = X[train_index], y[train_index]
    X_val, y_val = X[val_index], y[val_index]

    # Augmentation
    Train_x, Train_y = None, None
    batch = 0
    for x_batch, y_batch in datagen.flow(X_train, y_train,
                                         batch_size=BATCH_SIZE):
        if batch == 0:
            Train_x, Train_y = x_batch, y_batch
        elif batch &amp;gt;= X.shape[0] // BATCH_SIZE:
            break
        else:
            Train_x = np.concatenate((Train_x, x_batch))
            Train_y = np.concatenate((Train_y, y_batch))
        batch += 1

    # Model
    with tpu_strategy.scope():
        model = build_model()

    # Callbacks
    annealer = LearningRateScheduler(lambda x: 1e-3 * 0.95 ** x) # LR
    sv = ModelCheckpoint(f&amp;#x27;Model Fold {fold}.h5&amp;#x27;, monitor=&amp;#x27;val_accuracy&amp;#x27;,
                         save_best_only=True, mode=&amp;#x27;max&amp;#x27;)

    # Training
    history = model.fit(Train_x, Train_y, batch_size = BATCH_SIZE,
                        epochs = EPOCHS, verbose = 0, callbacks=[annealer, sv],
                        steps_per_epoch = X_train.shape[0]//BATCH_SIZE,
                        validation_data = (X_val, y_val))
    History.append(history.history)

    # Load best model
    model = load_model(f&amp;#x27;Model Fold {fold}.h5&amp;#x27;)

    # Evaluate
    score = model.evaluate(X_val, y_val, verbose = 0)[1]
    scores.append(score)

    # Making Prediction
    pred += model.predict(X_test)

    time = round(default_timer() - start, 4)
    print(f&amp;#x27;[INFO] Fold {fold + 1} val_accuracy : {round(score, 4)} - Time : {time} s&amp;#x27;)

print()
print(f&amp;#x27;[INFO] Mean CV scores : {round(sum(scores)/len(scores), 4)}&amp;#x27;)
&lt;/code&gt;&lt;/pre&gt;&lt;pre&gt;&lt;code&gt;[INFO] Fold 1 val_accuracy : 0.9961 - Time : 168.7797 s
[INFO] Fold 2 val_accuracy : 0.9957 - Time : 173.7161 s
[INFO] Fold 3 val_accuracy : 0.9986 - Time : 178.845 s
[INFO] Fold 4 val_accuracy : 0.9957 - Time : 174.5829 s
[INFO] Fold 5 val_accuracy : 0.9946 - Time : 173.8493 s
[INFO] Fold 6 val_accuracy : 0.9979 - Time : 175.585 s
[INFO] Fold 7 val_accuracy : 0.9964 - Time : 176.5861 s
[INFO] Fold 8 val_accuracy : 0.9968 - Time : 177.6894 s
[INFO] Fold 9 val_accuracy : 0.995 - Time : 179.8059 s
[INFO] Fold 10 val_accuracy : 0.9943 - Time : 176.4369 s
[INFO] Fold 11 val_accuracy : 0.9939 - Time : 182.6052 s
[INFO] Fold 12 val_accuracy : 0.9961 - Time : 177.0005 s
[INFO] Fold 13 val_accuracy : 0.995 - Time : 180.026 s
[INFO] Fold 14 val_accuracy : 0.9954 - Time : 179.1445 s
[INFO] Fold 15 val_accuracy : 0.9975 - Time : 175.2097 s

[INFO] Mean CV scores : 0.9959
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;And so we&amp;#x27;ve got really great CV score there. Let&amp;#x27;s check the Training History.&lt;/p&gt;&lt;h2 id=&quot;history&quot;&gt;History&lt;/h2&gt;&lt;div class=&quot;tabbed&quot;&gt;
  &lt;span class=&quot;gatsby-resp-image-wrapper&quot; style=&quot;position:relative;display:block;margin-left:auto;margin-right:auto;max-width:630px&quot;&gt;
      &lt;a class=&quot;gatsby-resp-image-link&quot; href=&quot;/static/7422a0fae641ba551fdb715a86950450/302a4/val_loss.png&quot; style=&quot;display:block&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;
    &lt;span class=&quot;gatsby-resp-image-background-image&quot; style=&quot;padding-bottom:66.45569620253164%;position:relative;bottom:0;left:0;background-image:url(&amp;#x27;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAANCAYAAACpUE5eAAAACXBIWXMAAAsSAAALEgHS3X78AAAB6klEQVQ4y6WT6XLbMAyE+/7Pl2mnmTpObEuWdVKUeJP6OmTiK/1ZaDArQOAKJBc/tm0jW0qJGOMThhCoqoqmaajruriUkrym1MSIDwHnPVeeH1cyrTXeW7TVxPhF+oXZM3n2EofPXCZR68K5rgkh3gnzR2sdwa/088DV4hZv796GglvaCO6ed9rStS0hhscOI84HZnGhOb5gQsQ6y+rXUpjdmETIxxEiRodCTAS1Kuq6Kt3fCLeUMNYyjzXt20/25z3VNDAukk4PaK24TAOHYUKsK51U9Iumyy4Fp9PpmTCmiDWWufmg3/3i+L6jb2uOh4pDfeHSCY6XE+00Fx+1RAjJOE5oZVm1IT5uOZ9h7nDpBGL3G3Vq8E2F37/Svu2o397xQ4MbWmxfE9oTrs1YMR//YLUipvSw5W3D5i3Pgu5jz9icmcaJrukZ63e61xfmpkUeT4yHM2JYmMeJZRRoKQnelKaeZGONwbgVoUbMuuD1gl0EVgqMNqyjQA0dS98hhwEjJ8wqUKIlOkP4TqiNIeVnSyxWYkKOIyZa/OaxUWOdLj91waDtijES7RTO2387zMIO/lO41ltccEU6zjuMM0Xsyip89CUXiuBTWet9uN/y4+j54G9TkW6YbhPyHb2/1z+N3jX4X8s8fwHNYfM7HIlj8gAAAABJRU5ErkJggg==&amp;#x27;);background-size:cover;display:block&quot;&gt;&lt;/span&gt;
  &lt;img class=&quot;gatsby-resp-image-image&quot; alt=&quot;validation loss&quot; title=&quot;validation loss&quot; src=&quot;/static/7422a0fae641ba551fdb715a86950450/f058b/val_loss.png&quot; srcSet=&quot;/static/7422a0fae641ba551fdb715a86950450/c26ae/val_loss.png 158w,/static/7422a0fae641ba551fdb715a86950450/6bdcf/val_loss.png 315w,/static/7422a0fae641ba551fdb715a86950450/f058b/val_loss.png 630w,/static/7422a0fae641ba551fdb715a86950450/40601/val_loss.png 945w,/static/7422a0fae641ba551fdb715a86950450/302a4/val_loss.png 1080w&quot; sizes=&quot;(max-width: 630px) 100vw, 630px&quot; style=&quot;width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0&quot; loading=&quot;lazy&quot; decoding=&quot;async&quot;/&gt;
  &lt;/a&gt;
    &lt;/span&gt;
  &lt;span class=&quot;gatsby-resp-image-wrapper&quot; style=&quot;position:relative;display:block;margin-left:auto;margin-right:auto;max-width:630px&quot;&gt;
      &lt;a class=&quot;gatsby-resp-image-link&quot; href=&quot;/static/db083aad0563651af90980c4e8ae20ae/302a4/val_accuracy.png&quot; style=&quot;display:block&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;
    &lt;span class=&quot;gatsby-resp-image-background-image&quot; style=&quot;padding-bottom:66.45569620253164%;position:relative;bottom:0;left:0;background-image:url(&amp;#x27;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAANCAYAAACpUE5eAAAACXBIWXMAAAsSAAALEgHS3X78AAABvklEQVQ4y6WTfW/bIBCH+/2/1/6vlqzNqrROHWNTNzZ2/AIY8DPZmb1kq6pNOwkh7uDh7vhxN44jk4UQ8N6vYzKlFEmSkOc5QghkJpFSrnuC/3Vm4dwtsL7v1+DgB6yzs18Pmt52OO/Qg6E1DdYNc8z6YU3EOXcL1EYTxoANlqavZ6AJlt5pjLOoqqY4lVRlRVXX1FWJqgvObYWzw1rVCrTGoE1LISV921PKE+9RTHmUiOeEUki65IU2OdIcj9THhDKNOb1G6LPCh6nk8QpoLUWakWy3iO1X5GaD2n0jftzwdP+FU/ZEkew57B+QMqJMXxCHHad4h+kafAhXGY4jRhvqvCJ9vCc/PCOi7+yTPbGMyfN3RPKCUA2yUhSlQrWat6Ihywt64wh/lGwH+ipHRA9kqiBTJXlV4kbQNnCuW856YKrMmECYE4H6rGm7Du9/exRjLZV6pcgjpm5YH+CihLk3zgbGn326uC6z7luyNL19lElDxhicNzRdO9+8+D8bk02tepPyVjYTXWs9X9oYz7+YtoY0yz7QodZzH7Ud5uAi1s/mOZGuJRWCYQFef71pY/gL0Efgm6+3LP7XJs4PIm3zZreLK4cAAAAASUVORK5CYII=&amp;#x27;);background-size:cover;display:block&quot;&gt;&lt;/span&gt;
  &lt;img class=&quot;gatsby-resp-image-image&quot; alt=&quot;validation accuracy&quot; title=&quot;validation accuracy&quot; src=&quot;/static/db083aad0563651af90980c4e8ae20ae/f058b/val_accuracy.png&quot; srcSet=&quot;/static/db083aad0563651af90980c4e8ae20ae/c26ae/val_accuracy.png 158w,/static/db083aad0563651af90980c4e8ae20ae/6bdcf/val_accuracy.png 315w,/static/db083aad0563651af90980c4e8ae20ae/f058b/val_accuracy.png 630w,/static/db083aad0563651af90980c4e8ae20ae/40601/val_accuracy.png 945w,/static/db083aad0563651af90980c4e8ae20ae/302a4/val_accuracy.png 1080w&quot; sizes=&quot;(max-width: 630px) 100vw, 630px&quot; style=&quot;width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0&quot; loading=&quot;lazy&quot; decoding=&quot;async&quot;/&gt;
  &lt;/a&gt;
    &lt;/span&gt;&lt;/div&gt;&lt;h2 id=&quot;ensembleing-predictions&quot;&gt;Ensembleing Predictions&lt;/h2&gt;&lt;p&gt;&lt;code&gt;np.argmax&lt;/code&gt; through the prediction to get the number of class&lt;/p&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;pred = np.array([np.argmax(x) for x in pred])

# Countplot Prediction
plt.figure(figsize = (7,7))
sns.countplot(pred)
plt.title(&amp;#x27;Countplot of Predictions&amp;#x27;)
plt.show()
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;span class=&quot;gatsby-resp-image-wrapper&quot; style=&quot;position:relative;display:block;margin-left:auto;margin-right:auto;max-width:451px&quot;&gt;
      &lt;a class=&quot;gatsby-resp-image-link&quot; href=&quot;/static/331531bb65cc6908f464a6e2e9da132e/38070/output_30_0.png&quot; style=&quot;display:block&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;
    &lt;span class=&quot;gatsby-resp-image-background-image&quot; style=&quot;padding-bottom:94.9367088607595%;position:relative;bottom:0;left:0;background-image:url(&amp;#x27;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAATCAYAAACQjC21AAAACXBIWXMAAAsSAAALEgHS3X78AAAD10lEQVQ4y62Ta0yTZxiG3wnq4lTAP8KSRV1iMv4s2yKJLNkhTA5CoQUMjE4UdbGwEEB0HLYsYGlhC4eICqKlFCi0wEI4CAj0a3WTmhFhBzqzgG1tRmCljQu0/Vpo+3338nVucT8Wd+BJnvfnlevO/bzEYrEQAOSHOUOI6eH8i78A4Uvc/mzZq9FQLxUUFLyek5NzuLCw8LX8/Pw3RCLRIZFIFNXf379veXl578LCQoTRaAw3m80RBoNhDwcL4oB2u73Vs+HF6sNv3GumGS+9vu6ladrndDgYh8PBuly0n6Zpv8vlYpxOJ+um3T6Px+N1u93crnvW12G32yfJ0ODANkLIdufqr0oAsMqFfnt3Lv7lMNzj9bhvE9uaO5gzNC8uK2xuwNqW7V3q/JCdtc+zi4vz7Nr9Gdb1eJWlnRssyzKs3+9n4QVrMT1iFe3t7OqqjfX5nL5l2gOjza4lGY2TwedtIMekqnZBwzhW2k76jK1ZeLcrFS03Psb3MbEYvzwMdeO9gIrX4cHijVno+yh8UiHGV3cqob+b5+fduovkIY2ORGWXc5G38S92dvJqx2ANAIWIV78Pmbwcc0d5GKkbgOrSNAavNUIjV2Cp+TvoVZOQfFEPSnMBWk2mnz+hh2CY0hHRiDUoukxJ0qTqtuTaMc7QyxnGqTN/ByYkYaR+AH1Ns7hSlIuuigqsyB9gSjUBcU0ttFQpbms/8Kf8AXynoGErISRYIFZ2JP9pmIU41V+BvVdncb2sCL3SalhlhqeAJdBRQn/K+BT4HDAy4cTzPwEktaqrnfcMYEtJIXokUlhlP/49cFf4voj0obVAZN7Tkf8r8EB04g5CyA7BRWVnct2t/2+oBrZEnZORNMkmGR4+XcGdTVCquOuZpfwjYEzx1a0IlNK9OaXkjq4EHbk0RdIkKsWmRE5v1ARzd8iv7FA+KWXD2JrFxKoyGJm8jJlLSGJG6gaY3iszTEtJAdMjkTBWmYGZ6p5gxDW1DEWVMDpK6A0AhyiKZHfOBb1ddZOkV/cEWrYpcmCSCwM/pbWtHIajSRitH0Rf07e4XlqIXqkUttYH0KsnUfV5HXTaUtzRCcGfvAfBTe3X5Pw0tpydB+F9KmtIrB54bGk5Pm9ozjC9155ubr52wTgdm2Dur+kxKxv05svFH5k6KipNj5rum7WKYdNnYql5bLTYNDaaYUwaoqy8L0e7yZN5bv+hmJ0HjwjDGk69ueuc4NXQ/SmvhIlyBTtPHXg57ExyTsixxLOh/LeiQ7Li4kPyE86EFZ3I2x0TGx+alxezOzMjMuxgZvaeyOOnX/gNDjTpSlOVscEAAAAASUVORK5CYII=&amp;#x27;);background-size:cover;display:block&quot;&gt;&lt;/span&gt;
  &lt;img class=&quot;gatsby-resp-image-image&quot; alt=&quot;png&quot; title=&quot;png&quot; src=&quot;/static/331531bb65cc6908f464a6e2e9da132e/38070/output_30_0.png&quot; srcSet=&quot;/static/331531bb65cc6908f464a6e2e9da132e/c26ae/output_30_0.png 158w,/static/331531bb65cc6908f464a6e2e9da132e/6bdcf/output_30_0.png 315w,/static/331531bb65cc6908f464a6e2e9da132e/38070/output_30_0.png 451w&quot; sizes=&quot;(max-width: 451px) 100vw, 451px&quot; style=&quot;width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0&quot; loading=&quot;lazy&quot; decoding=&quot;async&quot;/&gt;
  &lt;/a&gt;
    &lt;/span&gt;&lt;/p&gt;&lt;h2 id=&quot;lets-check-some-of-our-prediction&quot;&gt;Let&amp;#x27;s check some of our prediction&lt;/h2&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;fig, axes = plt.subplots(ncols=10, nrows=5, figsize = (13, 7))
init = 0
for i in range(5):
    j = 0
    for k in range(2):
        ind = random.choices(test.index, k = 5)
        init += 1
        while j &amp;lt; len(ind):
            axes[i, k*5 + j].imshow(X_test[ind[j]][:,:,0], cmap=plt.cm.binary)
            axes[i, k*5 + j].set_title(f&amp;#x27;Prediction : {pred[ind][j]}&amp;#x27;, fontsize = 11)
            axes[i, k*5 + j].axis(&amp;#x27;off&amp;#x27;)
            j += 1
        j = 0
fig.tight_layout()
plt.show()
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;span class=&quot;gatsby-resp-image-wrapper&quot; style=&quot;position:relative;display:block;margin-left:auto;margin-right:auto;max-width:630px&quot;&gt;
      &lt;a class=&quot;gatsby-resp-image-link&quot; href=&quot;/static/93510023de4a8aa0b9df3c023f51c209/6295b/output_32_0.png&quot; style=&quot;display:block&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;
    &lt;span class=&quot;gatsby-resp-image-background-image&quot; style=&quot;padding-bottom:53.16455696202532%;position:relative;bottom:0;left:0;background-image:url(&amp;#x27;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAALCAYAAAB/Ca1DAAAACXBIWXMAAAsSAAALEgHS3X78AAADS0lEQVQozxXQa2/bVACA4fwy2KiGJsbaISaE6MQqxIBxGWnG2CQkVIkx2iplg03a1jGE1rJVCNJQGtpG6mLnNDSJ6zRu6pwcH9ttbMeXOL4kduK0/mDUH/BKj94YwzAJlmUv5fP5b3mef4ckye9ardZ5lmV/UBTlAsMwdziOGy0Wi3ckSRqjKGpa1/VzEMIZy7LOAQC+z+fzF3men+I4bjy2u7vLHh4ePqcoymi320/X19d7R0dHSUEQwn6//3Oj0QiiKLqLEAp833+wvb09HA6HSdM0Q9/3kyRJ9jDGT3Vd1x3H+TMmSdJlURTP7+zsXBUE4Wy9Xv/Cdd1TLMtO1mq1EYRQ3DCM0yzLxm3bHuE4LgEhfJXn+QTG+BWapr90HOd1jPFnhmG8GctkMnvtdnthdXVV9X3/CYTQ0XV9liTJwPO8e8vLy76u63MEQXi2bf+UyWT6ruvOEAQxlGV5dmtry2k2m0+y2WyrXC7/EatWqx9gjMd4nv9c1/U3RFGMF4vF1xqNxldBEJypVCrXIYQjsixfxxifoSjqRq/XO8UwzNee550+EVuWdZam6WuCIIzFarVaNQzDhVarpRiGMV+tVi1N02YAAH3Hce6urKz0bNtOptPpbqfTucdxnBcEwXQulxuYpjmzublpMQzzGCEkQwiXTh7eNE3zsqIot13Xfc+yrGlN0y7QND3H8/zbGxsbSUmS3qJpOuk4zkVBEOb6/f4oRVE/mqY5CgCYhRC+KwjC7U6nMxEDAPyl6/o0wzAvOY6bKhQKBcMwEgCAncFgcHN/f7/s+/4NAEA5CIJbEELKdd1JhBCtKEqiVCr95zjO1OLi4st0Op2MEQTxSFXVW6IovlAUZbJSqaR6vd6VQqGwqmna1Xq9vhIEwccAgL/DMPwUY/yPqqofIoQyURRdIUkyJctyfGlp6QVJkt/EcrncrKIo1/L5/CNN0z4hSfLXg4ODSzzPL6iqOpFOp581m833CYJ45nneBEJosdvtjgMAfk+lUuMIod8ajcZHe3t78xjjeCyVSjW73e5yqVQaCILwvFwuR2EY3hdFMTo+Pp7PZrORZVkPGYaJTNP8RZKkyLbt+2traxHG+AFFUVG73T7p+vV6/d//Ac59rqkeFQn6AAAAAElFTkSuQmCC&amp;#x27;);background-size:cover;display:block&quot;&gt;&lt;/span&gt;
  &lt;img class=&quot;gatsby-resp-image-image&quot; alt=&quot;png&quot; title=&quot;png&quot; src=&quot;/static/93510023de4a8aa0b9df3c023f51c209/f058b/output_32_0.png&quot; srcSet=&quot;/static/93510023de4a8aa0b9df3c023f51c209/c26ae/output_32_0.png 158w,/static/93510023de4a8aa0b9df3c023f51c209/6bdcf/output_32_0.png 315w,/static/93510023de4a8aa0b9df3c023f51c209/f058b/output_32_0.png 630w,/static/93510023de4a8aa0b9df3c023f51c209/6295b/output_32_0.png 919w&quot; sizes=&quot;(max-width: 630px) 100vw, 630px&quot; style=&quot;width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0&quot; loading=&quot;lazy&quot; decoding=&quot;async&quot;/&gt;
  &lt;/a&gt;
    &lt;/span&gt;&lt;/p&gt;&lt;p&gt;looks like our models really doing good for predicting test data&lt;/p&gt;&lt;h2 id=&quot;conclusions&quot;&gt;Conclusions&lt;/h2&gt;&lt;p&gt;Based on our CV scores it&amp;#x27;s lower than &lt;a href=&quot;https://www.kaggle.com/cdeotte&quot;&gt;Chris Deotte&lt;/a&gt; kernel &lt;a href=&quot;https://www.kaggle.com/cdeotte/25-million-images-0-99757-mnist&quot;&gt;here&lt;/a&gt; with 0.99757 on validation accuracy on the same model architecture. So here&amp;#x27;s the point:&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;GPU do better job in this task [Expected] while TPU give you lower accuracy but faster since we&amp;#x27;d train 15 CNNs model on 45 epochs in less than one hour.&lt;/p&gt;&lt;/blockquote&gt;&lt;h2 id=&quot;references&quot;&gt;References&lt;/h2&gt;&lt;ol&gt;&lt;li&gt;25 Million Images! [0.99757] MNIST &lt;a href=&quot;https://www.kaggle.com/cdeotte/25-million-images-0-99757-mnist&quot;&gt;Link&lt;/a&gt;&lt;/li&gt;&lt;li&gt;Introduction to CNN Keras - 0.997 (top 6%) &lt;a href=&quot;https://www.kaggle.com/yassineghouzam/introduction-to-cnn-keras-0-997-top-6&quot;&gt;Link&lt;/a&gt;&lt;/li&gt;&lt;/ol&gt;</content:encoded></item><item><title><![CDATA[Machine Translation English to Japanese with Seq2Seq & Tensorflow]]></title><description><![CDATA[Hello guys, lately i've been studying about machine translation and give it a try. Most of code in this notebook is based on tensorflowâ€¦]]></description><link>https://Hyuto.github.io/blog/machine-translation-en-jp-seq2seq-tf/</link><guid isPermaLink="false">https://Hyuto.github.io/blog/machine-translation-en-jp-seq2seq-tf/</guid><pubDate>Wed, 19 Aug 2020 01:51:25 GMT</pubDate><content:encoded>&lt;p&gt;Hello guys, lately i&amp;#x27;ve been studying about machine translation and give it a try.&lt;/p&gt;&lt;p&gt;Most of code in this notebook is based on tensorflow tutorial on their website
&lt;a href=&quot;https://www.tensorflow.org/addons/tutorials/networks_seq2seq_nmt&quot;&gt;TensorFlow Addons Networks : Sequence-to-Sequence NMT with Attention Mechanism&lt;/a&gt;.&lt;/p&gt;&lt;p&gt;This notebook is basically my notebook run on &lt;a href=&quot;https://www.kaggle.com/&quot;&gt;kaggle&lt;/a&gt; so if you want to
try and run the code with same environment as mine go to link bellow.&lt;/p&gt;&lt;p&gt;Kaggle Notebook : &lt;a href=&quot;https://www.kaggle.com/wahyusetianto/machine-translation-en-jp-seq2seq-tf&quot;&gt;Machine Translation EN-JP Seq2Seq Tensorflow&lt;/a&gt;&lt;/p&gt;&lt;p&gt;P.S. Don&amp;#x27;t forget to &lt;em&gt;upvote&lt;/em&gt; if you like it ðŸ˜Š.&lt;/p&gt;&lt;h2 id=&quot;overview&quot;&gt;Overview&lt;/h2&gt;&lt;p&gt;So in this notebook we&amp;#x27;re going to build English to Japanese machine translation, Japanese text
contains lots of unique words because they have 3 type of it:&lt;/p&gt;&lt;ol&gt;&lt;li&gt;Kanji&lt;/li&gt;&lt;li&gt;Katakana&lt;/li&gt;&lt;li&gt;Hiragana&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;that&amp;#x27;s the interesting part of it and so it&amp;#x27;ll be little complicated to process. So let&amp;#x27;s get started.&lt;/p&gt;&lt;h2 id=&quot;install-some-tools&quot;&gt;Install some tools&lt;/h2&gt;&lt;ol&gt;&lt;li&gt;Sacreblue for calculate BLEU score&lt;/li&gt;&lt;li&gt;Googletrans =&amp;gt; Google Translate for testing some sentences later&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;Note : You can use NLTK for calculating BLEU score &lt;a href=&quot;https://www.nltk.org/_modules/nltk/translate/bleu_score.html&quot;&gt;documentation&lt;/a&gt;&lt;/p&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;!pip -q install sacrebleu
!pip -q install googletrans
!pip -q install tensorflow-addons --upgrade
&lt;/code&gt;&lt;/pre&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;import random, re, string, itertools, timeit, sacrebleu
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from tqdm.notebook import tqdm
from IPython.display import display, clear_output
from sklearn.model_selection import train_test_split

# Tensorflow &amp;amp; Keras
import tensorflow as tf
import tensorflow_addons as tfa
import tensorflow.keras.backend as K
from tensorflow.keras.layers import Input, Dense, LSTM, LSTMCell
from tensorflow.keras.layers import Embedding, Bidirectional
from tensorflow.keras.models import Model
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.losses import SparseCategoricalCrossentropy
from tensorflow.keras.optimizers import Adam

# Japanese Word Tokenizer
from janome.tokenizer import Tokenizer as janome_tokenizer

plt.style.use(&amp;#x27;seaborn-pastel&amp;#x27;)
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&quot;dataset&quot;&gt;Dataset&lt;/h2&gt;&lt;p&gt;Here we use 55463 en-jp corpus from &lt;a href=&quot;http://www.manythings.org/bilingual/&quot;&gt;ManyThings.org Bilingual Sentence Pairs&lt;/a&gt;&lt;/p&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;# Download Data &amp;amp; Unzip Data
!wget http://www.manythings.org/anki/jpn-eng.zip
!unzip jpn-eng.zip
&lt;/code&gt;&lt;/pre&gt;&lt;pre&gt;&lt;code&gt;--2020-08-19 01:52:31--  http://www.manythings.org/anki/jpn-eng.zip
Resolving www.manythings.org (www.manythings.org)... 104.24.109.196, 104.24.108.196, 172.67.173.198, ...
Connecting to www.manythings.org (www.manythings.org)|104.24.109.196|:80... connected.
HTTP request sent, awaiting response... 200 OK
Length: 2303148 (2.2M) [application/zip]
Saving to: â€˜jpn-eng.zipâ€™

jpn-eng.zip         100%[===================&amp;gt;]   2.20M  9.70MB/s    in 0.2s

2020-08-19 01:52:32 (9.70 MB/s) - â€˜jpn-eng.zipâ€™ saved [2303148/2303148]

Archive:  jpn-eng.zip
  inflating: jpn.txt
  inflating: _about.txt
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Load data to memory.&lt;/p&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;data = []

f1 = open(&amp;#x27;./jpn.txt&amp;#x27;, &amp;#x27;r&amp;#x27;)
data += [x.rstrip().lower().split(&amp;#x27;\t&amp;#x27;)[:2] for x in tqdm(f1.readlines())]
f1.close()

print(f&amp;#x27;Loaded {len(data)} Sentences&amp;#x27;)
&lt;/code&gt;&lt;/pre&gt;&lt;pre&gt;&lt;code&gt;Loaded 53594 Sentences
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&quot;text-preprocessing&quot;&gt;Text Preprocessing&lt;/h2&gt;&lt;h3 id=&quot;handling-misspell-words--clearing-punctuation&quot;&gt;Handling misspell words &amp;amp; Clearing Punctuation&lt;/h3&gt;&lt;p&gt;we&amp;#x27;re gonna change the misspell words in english sentences and clearing punctuation from text.&lt;/p&gt;&lt;p&gt;&amp;quot;aren&amp;#x27;t my english bad?&amp;quot; -&amp;gt; &amp;quot;are not my english bad&amp;quot;&lt;/p&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;mispell_dict = {
    &amp;quot;aren&amp;#x27;t&amp;quot; : &amp;quot;are not&amp;quot;,
    &amp;quot;can&amp;#x27;t&amp;quot; : &amp;quot;cannot&amp;quot;,
    &amp;quot;couldn&amp;#x27;t&amp;quot; : &amp;quot;could not&amp;quot;,
    &amp;quot;didn&amp;#x27;t&amp;quot; : &amp;quot;did not&amp;quot;,
    &amp;quot;doesn&amp;#x27;t&amp;quot; : &amp;quot;does not&amp;quot;,
    &amp;quot;don&amp;#x27;t&amp;quot; : &amp;quot;do not&amp;quot;,
    &amp;quot;hadn&amp;#x27;t&amp;quot; : &amp;quot;had not&amp;quot;,
    &amp;quot;hasn&amp;#x27;t&amp;quot; : &amp;quot;has not&amp;quot;,
    &amp;quot;haven&amp;#x27;t&amp;quot; : &amp;quot;have not&amp;quot;,
    &amp;quot;he&amp;#x27;d&amp;quot; : &amp;quot;he would&amp;quot;,
    &amp;quot;he&amp;#x27;ll&amp;quot; : &amp;quot;he will&amp;quot;,
    &amp;quot;he&amp;#x27;s&amp;quot; : &amp;quot;he is&amp;quot;,
    &amp;quot;i&amp;#x27;d&amp;quot; : &amp;quot;i would&amp;quot;,
    &amp;quot;i&amp;#x27;d&amp;quot; : &amp;quot;i had&amp;quot;,
    &amp;quot;i&amp;#x27;ll&amp;quot; : &amp;quot;i will&amp;quot;,
    &amp;quot;i&amp;#x27;m&amp;quot; : &amp;quot;i am&amp;quot;,
    &amp;quot;isn&amp;#x27;t&amp;quot; : &amp;quot;is not&amp;quot;,
    &amp;quot;it&amp;#x27;s&amp;quot; : &amp;quot;it is&amp;quot;,
    &amp;quot;it&amp;#x27;ll&amp;quot;:&amp;quot;it will&amp;quot;,
    &amp;quot;i&amp;#x27;ve&amp;quot; : &amp;quot;i have&amp;quot;,
    &amp;quot;let&amp;#x27;s&amp;quot; : &amp;quot;let us&amp;quot;,
    &amp;quot;mightn&amp;#x27;t&amp;quot; : &amp;quot;might not&amp;quot;,
    &amp;quot;mustn&amp;#x27;t&amp;quot; : &amp;quot;must not&amp;quot;,
    &amp;quot;shan&amp;#x27;t&amp;quot; : &amp;quot;shall not&amp;quot;,
    &amp;quot;she&amp;#x27;d&amp;quot; : &amp;quot;she would&amp;quot;,
    &amp;quot;she&amp;#x27;ll&amp;quot; : &amp;quot;she will&amp;quot;,
    &amp;quot;she&amp;#x27;s&amp;quot; : &amp;quot;she is&amp;quot;,
    &amp;quot;shouldn&amp;#x27;t&amp;quot; : &amp;quot;should not&amp;quot;,
    &amp;quot;that&amp;#x27;s&amp;quot; : &amp;quot;that is&amp;quot;,
    &amp;quot;there&amp;#x27;s&amp;quot; : &amp;quot;there is&amp;quot;,
    &amp;quot;they&amp;#x27;d&amp;quot; : &amp;quot;they would&amp;quot;,
    &amp;quot;they&amp;#x27;ll&amp;quot; : &amp;quot;they will&amp;quot;,
    &amp;quot;they&amp;#x27;re&amp;quot; : &amp;quot;they are&amp;quot;,
    &amp;quot;they&amp;#x27;ve&amp;quot; : &amp;quot;they have&amp;quot;,
    &amp;quot;we&amp;#x27;d&amp;quot; : &amp;quot;we would&amp;quot;,
    &amp;quot;we&amp;#x27;re&amp;quot; : &amp;quot;we are&amp;quot;,
    &amp;quot;weren&amp;#x27;t&amp;quot; : &amp;quot;were not&amp;quot;,
    &amp;quot;we&amp;#x27;ve&amp;quot; : &amp;quot;we have&amp;quot;,
    &amp;quot;what&amp;#x27;ll&amp;quot; : &amp;quot;what will&amp;quot;,
    &amp;quot;what&amp;#x27;re&amp;quot; : &amp;quot;what are&amp;quot;,
    &amp;quot;what&amp;#x27;s&amp;quot; : &amp;quot;what is&amp;quot;,
    &amp;quot;what&amp;#x27;ve&amp;quot; : &amp;quot;what have&amp;quot;,
    &amp;quot;where&amp;#x27;s&amp;quot; : &amp;quot;where is&amp;quot;,
    &amp;quot;who&amp;#x27;d&amp;quot; : &amp;quot;who would&amp;quot;,
    &amp;quot;who&amp;#x27;ll&amp;quot; : &amp;quot;who will&amp;quot;,
    &amp;quot;who&amp;#x27;re&amp;quot; : &amp;quot;who are&amp;quot;,
    &amp;quot;who&amp;#x27;s&amp;quot; : &amp;quot;who is&amp;quot;,
    &amp;quot;who&amp;#x27;ve&amp;quot; : &amp;quot;who have&amp;quot;,
    &amp;quot;won&amp;#x27;t&amp;quot; : &amp;quot;will not&amp;quot;,
    &amp;quot;wouldn&amp;#x27;t&amp;quot; : &amp;quot;would not&amp;quot;,
    &amp;quot;you&amp;#x27;d&amp;quot; : &amp;quot;you would&amp;quot;,
    &amp;quot;you&amp;#x27;ll&amp;quot; : &amp;quot;you will&amp;quot;,
    &amp;quot;you&amp;#x27;re&amp;quot; : &amp;quot;you are&amp;quot;,
    &amp;quot;you&amp;#x27;ve&amp;quot; : &amp;quot;you have&amp;quot;,
    &amp;quot;&amp;#x27;re&amp;quot;: &amp;quot; are&amp;quot;,
    &amp;quot;wasn&amp;#x27;t&amp;quot;: &amp;quot;was not&amp;quot;,
    &amp;quot;we&amp;#x27;ll&amp;quot;:&amp;quot; will&amp;quot;,
    &amp;quot;didn&amp;#x27;t&amp;quot;: &amp;quot;did not&amp;quot;,
    &amp;quot;tryin&amp;#x27;&amp;quot;:&amp;quot;trying&amp;quot;
}

mispell_re = re.compile(&amp;#x27;(%s)&amp;#x27; % &amp;#x27;|&amp;#x27;.join(mispell_dict.keys()))

def preprocess(text) -&amp;gt; str:
    def replace(match):
        return mispell_dict[match.group(0)]

    text = mispell_re.sub(replace, text)
    return text
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Japanese words have their own punctuation like ã€thisã€‘&lt;/p&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;# Adding Japanese Punctuation
string.punctuation += &amp;#x27;ã€ã€‚ã€ã€‘ã€Œã€ã€Žã€â€¦ãƒ»ã€½ï¼ˆï¼‰ã€œï¼Ÿï¼ï½¡ï¼šï½¤ï¼›ï½¥&amp;#x27;

CP = lambda x : x.translate(str.maketrans(&amp;#x27;&amp;#x27;, &amp;#x27;&amp;#x27;, string.punctuation))
&lt;/code&gt;&lt;/pre&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;data = [x for x in data if len(x) == 2]

eng_data = [CP(preprocess(x[0])) for x in data]
jpn_data = [CP(x[1]) for x in data]
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&quot;segmenting-japanese-sentences&quot;&gt;Segmenting Japanese Sentences&lt;/h3&gt;&lt;p&gt;Unlike english sentence we can tokenize it by splitting words with space just like this,&lt;/p&gt;&lt;pre&gt;&lt;code&gt;&amp;#x27;This is english or i think so&amp;#x27;.split()

Output:
[&amp;#x27;This&amp;#x27;, &amp;#x27;is&amp;#x27;, &amp;#x27;english&amp;#x27;, &amp;#x27;or&amp;#x27;, &amp;#x27;i&amp;#x27;, &amp;#x27;think&amp;#x27;, &amp;#x27;so&amp;#x27;]
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;but in Japanese we can&amp;#x27;t do it that way. Here we gonna use Janome Tokenizer to segmenting Japanese sentence and adding space to it so Keras Tokenizer can handle it.&lt;/p&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;# Initialize Janome Tokenizer
token_jp = janome_tokenizer()
&lt;/code&gt;&lt;/pre&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;sample_text = &amp;#x27;ã“ã“ã§ç§ã¯è‹±èªžã§è©±ã—ã¦ã„ã‚‹&amp;#x27;
&amp;#x27; &amp;#x27;.join([word for word in token_jp.tokenize(sample_text, wakati=True) \
          if word != &amp;#x27; &amp;#x27;])
&lt;/code&gt;&lt;/pre&gt;&lt;pre&gt;&lt;code&gt;&amp;#x27;ã“ã“ ã§ ç§ ã¯ è‹±èªž ã§ è©±ã— ã¦ ã„ã‚‹&amp;#x27;
&lt;/code&gt;&lt;/pre&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;# Apply to Japanese Sentences
jpn_data = [&amp;#x27; &amp;#x27;.join([word for word in token_jp.tokenize(x, wakati=True) \
                      if word != &amp;#x27; &amp;#x27;]) for x in tqdm(jpn_data)]
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;For evaluating our model let&amp;#x27;s split our data.&lt;/p&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;eng_train, eng_test, jpn_train, jpn_test = \
train_test_split(eng_data, jpn_data, test_size = 0.04, random_state = 42)

print(f&amp;quot;Splitting to {len(eng_train)} Train data and \
{len(eng_test)} Test data&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;&lt;pre&gt;&lt;code&gt;Splitting to 51450 Train data and 2144 Test data
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&quot;add-bos-and-eos&quot;&gt;Add BOS and EOS&lt;/h3&gt;&lt;p&gt;We put BOS &amp;quot;Begin of Sequence&amp;quot; and EOS â€œEnd of Sequence&amp;quot; to help our decoder recognize begin and end of a sequence.&lt;/p&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;eng_train = [&amp;#x27;bos &amp;#x27;+ x + &amp;#x27; eos&amp;#x27; for x in eng_train + [&amp;#x27;unk unk unk&amp;#x27;]]
jpn_train = [&amp;#x27;bos &amp;#x27;+ x + &amp;#x27; eos&amp;#x27; for x in jpn_train + [&amp;#x27;unk unk unk&amp;#x27;]]

eng_val = [&amp;#x27;bos &amp;#x27;+ x + &amp;#x27; eos&amp;#x27; for x in eng_test]
jpn_val = [&amp;#x27;bos &amp;#x27;+ x + &amp;#x27; eos&amp;#x27; for x in jpn_test]
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&quot;word-tokenizing&quot;&gt;Word Tokenizing&lt;/h2&gt;&lt;p&gt;Here we use Tokenizer API from Keras to make vocabulary and tokenizing our data&lt;/p&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;# English Tokenizer
en_tokenizer = Tokenizer(filters=&amp;#x27;&amp;#x27;)
en_tokenizer.fit_on_texts(eng_train)

# Japannese Tokenizer
jp_tokenizer = Tokenizer(filters=&amp;#x27;&amp;#x27;)
jp_tokenizer.fit_on_texts(jpn_train)
&lt;/code&gt;&lt;/pre&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;print(f&amp;#x27;English vocab size   :&amp;#x27;, len(en_tokenizer.word_index) - 3)
print(f&amp;#x27;Japanese vocab size  :&amp;#x27;, len(jp_tokenizer.word_index) - 3)
&lt;/code&gt;&lt;/pre&gt;&lt;pre&gt;&lt;code&gt;English vocab size   : 9646
Japanese vocab size  : 14403
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&quot;word-cloud&quot;&gt;Word Cloud&lt;/h2&gt;&lt;p&gt;What comes when doing NLP? It&amp;#x27;s Word Cloud. Let&amp;#x27;s do it for our vocab.&lt;/p&gt;&lt;p&gt;Font : &lt;a href=&quot;https://www.google.com/get/noto/&quot;&gt;Google Noto Fonts&lt;/a&gt; -&amp;gt; Noto Sans CJK JP&lt;/p&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;!wget https://noto-website-2.storage.googleapis.com/pkgs/NotoSansCJKjp-hinted.zip
!wget https://raw.githubusercontent.com/Hyuto/NMT-TF-Seq2seq-EN-JP/master/Japan.jpg
!wget https://raw.githubusercontent.com/Hyuto/NMT-TF-Seq2seq-EN-JP/master/English.png
!mkdir font
!unzip NotoSansCJKjp-hinted.zip -d ./font
&lt;/code&gt;&lt;/pre&gt;&lt;pre&gt;&lt;code&gt;--2020-08-19 01:53:52--  https://noto-website-2.storage.googleapis.com/pkgs/NotoSansCJKjp-hinted.zip
Resolving noto-website-2.storage.googleapis.com (noto-website-2.storage.googleapis.com)... 172.217.204.128, 2607:f8b0:400c:c15::80
Connecting to noto-website-2.storage.googleapis.com (noto-website-2.storage.googleapis.com)|172.217.204.128|:443... connected.
HTTP request sent, awaiting response... 200 OK
Length: 121096772 (115M) [application/zip]
Saving to: â€˜NotoSansCJKjp-hinted.zipâ€™

NotoSansCJKjp-hinte 100%[===================&amp;gt;] 115.49M  53.3MB/s    in 2.2s

2020-08-19 01:53:54 (53.3 MB/s) - â€˜NotoSansCJKjp-hinted.zipâ€™ saved [121096772/121096772]

--2020-08-19 01:53:55--  https://raw.githubusercontent.com/Hyuto/NMT-TF-Seq2seq-EN-JP/master/Japan.jpg
Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.200.133
Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.200.133|:443... connected.
HTTP request sent, awaiting response... 200 OK
Length: 3976 (3.9K) [image/jpeg]
Saving to: â€˜Japan.jpgâ€™

Japan.jpg           100%[===================&amp;gt;]   3.88K  --.-KB/s    in 0s

2020-08-19 01:53:55 (46.7 MB/s) - â€˜Japan.jpgâ€™ saved [3976/3976]

--2020-08-19 01:53:56--  https://raw.githubusercontent.com/Hyuto/NMT-TF-Seq2seq-EN-JP/master/English.png
Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.200.133
Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.200.133|:443... connected.
HTTP request sent, awaiting response... 200 OK
Length: 303684 (297K) [image/png]
Saving to: â€˜English.pngâ€™

English.png         100%[===================&amp;gt;] 296.57K  --.-KB/s    in 0.1s

2020-08-19 01:53:56 (2.18 MB/s) - â€˜English.pngâ€™ saved [303684/303684]

Archive:  NotoSansCJKjp-hinted.zip
  inflating: ./font/LICENSE_OFL.txt
  inflating: ./font/NotoSansCJKjp-Black.otf
  inflating: ./font/NotoSansCJKjp-Bold.otf
  inflating: ./font/NotoSansCJKjp-DemiLight.otf
  inflating: ./font/NotoSansCJKjp-Light.otf
  inflating: ./font/NotoSansCJKjp-Medium.otf
  inflating: ./font/NotoSansCJKjp-Regular.otf
  inflating: ./font/NotoSansCJKjp-Thin.otf
  inflating: ./font/NotoSansMonoCJKjp-Bold.otf
  inflating: ./font/NotoSansMonoCJKjp-Regular.otf
  inflating: ./font/README
&lt;/code&gt;&lt;/pre&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;from wordcloud import WordCloud, ImageColorGenerator
from PIL import Image

def get_words(arr):
    keys = list(arr.keys())
    count = list(arr.values())
    return &amp;#x27; &amp;#x27;.join([x for _,x in sorted(zip(count, keys), reverse = True)][2:])

def transform(arr):
    for i in range(len(arr)):
        for j in range(len(arr[i])):
            if not any(arr[i][j]):
                arr[i][j] = np.array([225, 225, 225, 225])
    return arr

font_path = &amp;#x27;./font/NotoSansCJKjp-Light.otf&amp;#x27;


mask = &amp;#x27;./English.png&amp;#x27;
mask = np.array(Image.open(mask))
mask = transform(mask)
image_colors = ImageColorGenerator(mask)
words = get_words(en_tokenizer.word_counts).title()
wc = WordCloud(background_color=&amp;quot;white&amp;quot;, max_words=2000, random_state=42,
               width=mask.shape[1], height=mask.shape[0])
wc = wc.generate(words)
fig1, ax1 = plt.subplots(figsize=(20,15))
ax1.imshow(wc.recolor(color_func=image_colors), interpolation=&amp;#x27;bilinear&amp;#x27;)
ax1.axis(&amp;quot;off&amp;quot;)

mask = &amp;#x27;./Japan.jpg&amp;#x27;
mask = np.array(Image.open(mask))
image_colors = ImageColorGenerator(mask)
words = get_words(jp_tokenizer.word_counts).title()
wc = WordCloud(collocations=False, background_color=&amp;quot;white&amp;quot;, mode=&amp;quot;RGBA&amp;quot;,
               max_words=6000, font_path=font_path, contour_width=1,
               scale=5, max_font_size = 50, relative_scaling=0.5,
               random_state=42, width=mask.shape[1], height=mask.shape[0])
wc = wc.generate(words)
fig2, ax2 = plt.subplots(figsize=(20,15))
ax2.imshow(wc.recolor(color_func=image_colors), interpolation=&amp;#x27;bilinear&amp;#x27;)
ax2.axis(&amp;quot;off&amp;quot;)

fig1.savefig(&amp;#x27;WC_English.png&amp;#x27;)
fig2.savefig(&amp;#x27;WC_Japanese.png&amp;#x27;)
plt.close(fig1)
plt.close(fig2)

!rm -rf ./font
&lt;/code&gt;&lt;/pre&gt;&lt;div class=&quot;tabbed&quot;&gt;
  &lt;span class=&quot;gatsby-resp-image-wrapper&quot; style=&quot;position:relative;display:block;margin-left:auto;margin-right:auto;max-width:630px&quot;&gt;
      &lt;a class=&quot;gatsby-resp-image-link&quot; href=&quot;/static/925d8b9db09014ceb7735de6722223fc/07a9c/WC_English.png&quot; style=&quot;display:block&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;
    &lt;span class=&quot;gatsby-resp-image-background-image&quot; style=&quot;padding-bottom:75.31645569620254%;position:relative;bottom:0;left:0;background-image:url(&amp;#x27;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAPCAYAAADkmO9VAAAACXBIWXMAAAsSAAALEgHS3X78AAACg0lEQVQ4y3WUe1PaUBDF/f5fojPtdOpM31qtDxQReQfQJAQSEhIICAQIT5WQhF8HrA62sH/cvbsze2bvPbtnj7+2XC75z7akdtlL/d5msFj4zD2PMAzx/YCQJU+TGaNml8Dz8D0P7/GR+fQB31sQLBYEfvBav/J7q+MlMRpPqVlNalYL+75Lr9Ul8/WcXCSNKWu0iiWMeA5XNbAydyjnSSYd5xVsDbjZYdsZYDXuKWsWomYhRFLkLrPkErfoQonCp2MK+7+pZ0TUsySp71fULYcw3NFhtzek1uhyq1hkZZPEWZqb4wSxwziVwyiZ9weopzd0FAMzXyZ/LWK2x+vv2QroDkbUzBaKZiOW60iKxfX+KZfvftLNSpg3ecq/rhjIGj3FQDmK0Tdbb5+8Ceg4LpJUparbFAsVBNEgmy2TjwrYxQpWPE/16IpZUWYmV3goyPTkCsFuUmbcNx3seoe61SZ9kSN+lECNCXRiWTo5kV6pSldSsW417EKZe6mynoqtgEN3REfRGZhNnHYfJZpDu0jzpFvoHw+QTxKMW11aJZ22VqcqlKnlxN1j8+AOcSSVodFgvGI8r9BMFQlVHf3DD5InWe4EjXHbwRCr2GqdiqjhBxuksAH4NJkyUHT6moXbaNOIC6T3zximb+l8OyEVKRCLyYjXRUoFDUPSMXSbxS6WZ30XRzMZmk06RpPI5yhnX2IYgkI9mka+SHOXlJHzJWqqQdOyMavmesO2Ak6cAbZSxVAMZMXk/EYikdcQFYuWM+Dh6ZHpbMZoPGYynTCdTXHd4RZSnqO1X+2vt1gwn88JA58g8J8LlkvCIFjf1w0s3wrKG3F4FZblboX5V402md1k+Q9clGHGV8oF9gAAAABJRU5ErkJggg==&amp;#x27;);background-size:cover;display:block&quot;&gt;&lt;/span&gt;
  &lt;img class=&quot;gatsby-resp-image-image&quot; alt=&quot;English WC&quot; title=&quot;English WC&quot; src=&quot;/static/925d8b9db09014ceb7735de6722223fc/f058b/WC_English.png&quot; srcSet=&quot;/static/925d8b9db09014ceb7735de6722223fc/c26ae/WC_English.png 158w,/static/925d8b9db09014ceb7735de6722223fc/6bdcf/WC_English.png 315w,/static/925d8b9db09014ceb7735de6722223fc/f058b/WC_English.png 630w,/static/925d8b9db09014ceb7735de6722223fc/40601/WC_English.png 945w,/static/925d8b9db09014ceb7735de6722223fc/78612/WC_English.png 1260w,/static/925d8b9db09014ceb7735de6722223fc/07a9c/WC_English.png 1440w&quot; sizes=&quot;(max-width: 630px) 100vw, 630px&quot; style=&quot;width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0&quot; loading=&quot;lazy&quot; decoding=&quot;async&quot;/&gt;
  &lt;/a&gt;
    &lt;/span&gt;
  &lt;span class=&quot;gatsby-resp-image-wrapper&quot; style=&quot;position:relative;display:block;margin-left:auto;margin-right:auto;max-width:630px&quot;&gt;
      &lt;a class=&quot;gatsby-resp-image-link&quot; href=&quot;/static/97526e703f2d47eceebc07ee03968a2e/07a9c/WC_Japanese.png&quot; style=&quot;display:block&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;
    &lt;span class=&quot;gatsby-resp-image-background-image&quot; style=&quot;padding-bottom:75.31645569620254%;position:relative;bottom:0;left:0;background-image:url(&amp;#x27;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAPCAYAAADkmO9VAAAACXBIWXMAAAsSAAALEgHS3X78AAAA70lEQVQ4y62UDQuCMBiE/f+/LaikKCvsO/qiaE6duu1iA0Pi3VzQCzIQ9+yOuxmBGK31Z22f7nvfRKFAEk4cEPlgXahL8bfqfoVKkRtd9p3AdoPMS4jzHeX2ZNcmy0m1/ZalRMM4sukKbDjDazBFNllCHK+QpQi33H6gRIVicwSLE2TxwoL5fA02mqO6Pcmg/MC6QZHuweIFeLIBn6UWzMYJ6sfrd4UmDHG5W5Cxa5QZYJEeoKqabIQT+FHZSAswMJ6ska92NhiqTsEpa6lsOPWTQRals6+9ln1do8ofrtBz9XTI1etQSWDfTyLCn+cNu5Sfjc744QQAAAAASUVORK5CYII=&amp;#x27;);background-size:cover;display:block&quot;&gt;&lt;/span&gt;
  &lt;img class=&quot;gatsby-resp-image-image&quot; alt=&quot;Japanese WC&quot; title=&quot;Japanese WC&quot; src=&quot;/static/97526e703f2d47eceebc07ee03968a2e/f058b/WC_Japanese.png&quot; srcSet=&quot;/static/97526e703f2d47eceebc07ee03968a2e/c26ae/WC_Japanese.png 158w,/static/97526e703f2d47eceebc07ee03968a2e/6bdcf/WC_Japanese.png 315w,/static/97526e703f2d47eceebc07ee03968a2e/f058b/WC_Japanese.png 630w,/static/97526e703f2d47eceebc07ee03968a2e/40601/WC_Japanese.png 945w,/static/97526e703f2d47eceebc07ee03968a2e/78612/WC_Japanese.png 1260w,/static/97526e703f2d47eceebc07ee03968a2e/07a9c/WC_Japanese.png 1440w&quot; sizes=&quot;(max-width: 630px) 100vw, 630px&quot; style=&quot;width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0&quot; loading=&quot;lazy&quot; decoding=&quot;async&quot;/&gt;
  &lt;/a&gt;
    &lt;/span&gt;&lt;/div&gt;&lt;p&gt;now let&amp;#x27;s transform our train sentences to sequences.&lt;/p&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;def Sequences(texts, tokenizer):
    res = []
    for text in texts:
        seq = []
        for w in text.split():
            try:
                seq.append(tokenizer.word_index[w])
            except:
                seq.append(tokenizer.word_index[&amp;#x27;unk&amp;#x27;])
        res.append(seq)
    return res
&lt;/code&gt;&lt;/pre&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;# Transform Sentences to Sequences
data_en = en_tokenizer.texts_to_sequences(eng_train)
data_jp = jp_tokenizer.texts_to_sequences(jpn_train)

val_en = Sequences(eng_val, en_tokenizer)
val_jp = Sequences(jpn_val, jp_tokenizer)
&lt;/code&gt;&lt;/pre&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;plt.figure(figsize = (8,8))
sns.distplot([len(x) for x in data_en], label=&amp;#x27;English&amp;#x27;)
sns.distplot([len(x) for x in data_jp], label=&amp;#x27;Japanese&amp;#x27;)
plt.title(&amp;#x27;Distribution of Sentences Length&amp;#x27;)
plt.legend()
plt.show()
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;span class=&quot;gatsby-resp-image-wrapper&quot; style=&quot;position:relative;display:block;margin-left:auto;margin-right:auto;max-width:490px&quot;&gt;
      &lt;a class=&quot;gatsby-resp-image-link&quot; href=&quot;/static/8ce63a24a5b8ffbe5487aa1074fa8a20/41d3c/output_32_0.png&quot; style=&quot;display:block&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;
    &lt;span class=&quot;gatsby-resp-image-background-image&quot; style=&quot;padding-bottom:98.10126582278481%;position:relative;bottom:0;left:0;background-image:url(&amp;#x27;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAUCAYAAACNiR0NAAAACXBIWXMAAAsSAAALEgHS3X78AAACwklEQVQ4y6WUW08TQRTH91UTL/XViES8pRD7RELDEwnhkY/gix/AJx8MGm+BGCWR1jbGxAh4ibEQiAoJIBcxxkBSwYhEpCz0Srud3XZ3u213ZnbnmG5boSGGFs/m7Ewyk9/+/2d2DudwODiv13PY5XKf8HoeH3e7+mwDg4PHOjs765ubmx1Op7Opo6OjoaWlpam1tdXe3t5+zul0Nra1tZ33+XxHHz3os3lcHtvAQL/N6/Ue4QCAEwThpWEYlFKKKKWSYRgSxiSt67qSz+dVjLFcnu8aZdMwJCWHU/EkQps8ryOE5iygLMujAADEBCAG1BR5E4OkpCEWjQJC6DsXCAQ4NZN5C8BAp0DylJkAzGTMyv3CBAYmAFBLECH+ssLhAjBPgKZyjBUWC+/ibG8wK4sbrIcxy5eu698sIEKiZVnOM4q0/YHFdfZ3rABGIhFOVTO+wjejCtCwvAOsNiqAsViMU0rAtSTQVaEMZFVD/2n5N2J0KWYVu8JWzcB0unAoACsJRr+GypwDKuR5vmQZwL+N6VxY+3+FqXTaUriU0OloKMpyJtl9grUDkwhZNVyVsnQkFGYyze0AoUZgMBjkFFW1LP9UEB2J8mxdUQ6uMJtROVVVhgoXblEU6LvtDeYXJQAwSz/3/tA9liVJHKUmwLwYouOJDfZJiANh1ddxD1BVlCFsmDAvBsmEsMEm4mGGaNq6stV0CMYYLQH9XGwzyOnZ3LBKMUwmeTYt8jApbMGKtg0AtfUyjPFysduIqTdbmggf4r+UycS6NpUIaFPxrcwPJaJJuqpliZ7BFGuYEo0YRCO0MjElKjUMls1lv3Acxx3qutF15vn0WMOT2ff1V3u7G+8Oui+89s/UXe7pbrxyv9f+bGHh1DXvHfvtpz32j2uf63pfuC7ect9rGl+eOt0//urs9Yc3L40tTpyZmZ89+Qdd/6OoUfpEpAAAAABJRU5ErkJggg==&amp;#x27;);background-size:cover;display:block&quot;&gt;&lt;/span&gt;
  &lt;img class=&quot;gatsby-resp-image-image&quot; alt=&quot;png&quot; title=&quot;png&quot; src=&quot;/static/8ce63a24a5b8ffbe5487aa1074fa8a20/41d3c/output_32_0.png&quot; srcSet=&quot;/static/8ce63a24a5b8ffbe5487aa1074fa8a20/c26ae/output_32_0.png 158w,/static/8ce63a24a5b8ffbe5487aa1074fa8a20/6bdcf/output_32_0.png 315w,/static/8ce63a24a5b8ffbe5487aa1074fa8a20/41d3c/output_32_0.png 490w&quot; sizes=&quot;(max-width: 490px) 100vw, 490px&quot; style=&quot;width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0&quot; loading=&quot;lazy&quot; decoding=&quot;async&quot;/&gt;
  &lt;/a&gt;
    &lt;/span&gt;&lt;/p&gt;&lt;p&gt;based on the distplot English sentences contains about 20 - 40 words while Japanese have more wider range.&lt;/p&gt;&lt;p&gt;Let&amp;#x27;s check their max length&lt;/p&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;max_en = max([len(x) for x in data_en] + [len(x) for x in val_en])
max_jp = max([len(x) for x in data_jp] + [len(x) for x in val_jp])

print(f&amp;#x27;Maximum length of English sequences is  {max_en}&amp;#x27;)
print(f&amp;#x27;Maximum length of Japanese sequences is {max_jp}&amp;#x27;)
&lt;/code&gt;&lt;/pre&gt;&lt;pre&gt;&lt;code&gt;Maximum length of English sequences is  49
Maximum length of Japanese sequences is 54
&lt;/code&gt;&lt;/pre&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;# Padding Sequences
data_en = pad_sequences(data_en, padding=&amp;#x27;post&amp;#x27;, maxlen = max_en)
data_jp = pad_sequences(data_jp, padding=&amp;#x27;post&amp;#x27;, maxlen = max_jp)

val_en = pad_sequences(val_en, padding=&amp;#x27;post&amp;#x27;, maxlen = max_en)
val_jp = pad_sequences(val_jp, padding=&amp;#x27;post&amp;#x27;, maxlen = max_jp)
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&quot;build--train-model&quot;&gt;Build &amp;amp; Train Model&lt;/h2&gt;&lt;p&gt;Now it&amp;#x27;s the time brace yourself.&lt;/p&gt;&lt;p&gt;We&amp;#x27;ll build model based on Seq2seq approaches with Attention optimization.&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;Seq2Seq is a method of encoder-decoder based machine translation that maps an input of sequence to an output of sequence with a tag and attention value. The idea is to use 2 RNN that will work together with a special token and trying to predict the next state sequence from the previous sequence.&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;&lt;span class=&quot;gatsby-resp-image-wrapper&quot; style=&quot;position:relative;display:block;margin-left:auto;margin-right:auto;max-width:630px&quot;&gt;
      &lt;a class=&quot;gatsby-resp-image-link&quot; href=&quot;/static/a5a04f4dc28fd14e1a9c5e883fda9bf5/5a190/seq2seq.png&quot; style=&quot;display:block&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;
    &lt;span class=&quot;gatsby-resp-image-background-image&quot; style=&quot;padding-bottom:50.632911392405056%;position:relative;bottom:0;left:0;background-image:url(&amp;#x27;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAKCAIAAAA7N+mxAAAACXBIWXMAAAsTAAALEwEAmpwYAAABeElEQVQoz22R2W7bMBBF/f/fE+SlD0WA5K1tHFuKbS22ZWqjJNJcRdLm0ipKUBvtATHALBd3wFmEe5xzWZZRRkMIPvi56P38fJJkjPGrdeZiQwiL8A/GGGvtbeViOCdgFDBPl+xcNtWW4PL/4nsmc85AWz6t3x6T7bf18gHWzwTHzrnF/dhNvEk73P1cv5awXm3X2SmP0vf0mM/OU59JUrRpj9uyO3S4rvqi7k8dagDMjdFEqAPE7YD3LWoGfGj6lohJ7P0k7s4wLn5lRZI28b5KdyDagTgDyaZcEYorRHZIRvkxxeOmqN7rYY+4s/bTWY2kKlejKE/Fsu9SAKIe7jg51lXsvdXmAvtByBEOSEjZDUgq9Xdto3qBXzWPweEF7F+yzXfU/tAs4ujNXrX3nlIy73jL54dZ61qIrAuYsKu1Uiou9OXqhdTz8b337gP7xSQ2xgAAKKWMETXK8xkLwTljlJJRCsqotVZrzRj7iFRKKYRQSv051W8r8je8hZRFFgAAAABJRU5ErkJggg==&amp;#x27;);background-size:cover;display:block&quot;&gt;&lt;/span&gt;
  &lt;img class=&quot;gatsby-resp-image-image&quot; alt=&quot;Seq2Seq&quot; title=&quot;Seq2Seq&quot; src=&quot;/static/a5a04f4dc28fd14e1a9c5e883fda9bf5/f058b/seq2seq.png&quot; srcSet=&quot;/static/a5a04f4dc28fd14e1a9c5e883fda9bf5/c26ae/seq2seq.png 158w,/static/a5a04f4dc28fd14e1a9c5e883fda9bf5/6bdcf/seq2seq.png 315w,/static/a5a04f4dc28fd14e1a9c5e883fda9bf5/f058b/seq2seq.png 630w,/static/a5a04f4dc28fd14e1a9c5e883fda9bf5/5a190/seq2seq.png 800w&quot; sizes=&quot;(max-width: 630px) 100vw, 630px&quot; style=&quot;width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0&quot; loading=&quot;lazy&quot; decoding=&quot;async&quot;/&gt;
  &lt;/a&gt;
    &lt;/span&gt;&lt;/p&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;# Config
epochs = 7
BATCH_SIZE = 64
BUFFER_SIZE = len(data_jp)
steps_per_epoch = BUFFER_SIZE//BATCH_SIZE
val_steps_per_epoch = len(val_jp) // BATCH_SIZE
embedding_dims = 256
rnn_units = 1024
dense_units = 1024
Dtype = tf.float32
&lt;/code&gt;&lt;/pre&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;def max_len(tensor):
    &amp;quot;&amp;quot;&amp;quot;
    Get max len in Sequences
    &amp;quot;&amp;quot;&amp;quot;
    return max( len(t) for t in tensor)
&lt;/code&gt;&lt;/pre&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;# Max Len
Tx = max_len(data_en)
Ty = max_len(data_jp)

# Vocab
input_vocab_size = len(en_tokenizer.word_index) + 1   # English
output_vocab_size = len(jp_tokenizer.word_index) + 1  # Japanese

# Changing to TF data
dataset = (tf.data.Dataset.from_tensor_slices((data_en, data_jp))
           .shuffle(BUFFER_SIZE)
           .batch(BATCH_SIZE, drop_remainder=True)
          )

val_dataset = (tf.data.Dataset.from_tensor_slices((val_en, val_jp))
               .batch(BATCH_SIZE)
              )
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Let&amp;#x27;s define our based Seq2Seq Model&lt;/p&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;# ENCODER
class EncoderNetwork(tf.keras.Model):
    def __init__(self,input_vocab_size,embedding_dims, rnn_units ):
        super().__init__()
        self.encoder_embedding = Embedding(input_dim=input_vocab_size,
                                           output_dim=embedding_dims)
        self.encoder_rnnlayer = LSTM(rnn_units,return_sequences=True,
                                     return_state=True )

# DECODER
class DecoderNetwork(tf.keras.Model):
    def __init__(self,output_vocab_size, embedding_dims, rnn_units):
        super().__init__()
        self.decoder_embedding = Embedding(input_dim=output_vocab_size,
                                           output_dim=embedding_dims)
        self.dense_layer = Dense(output_vocab_size)
        self.decoder_rnncell = LSTMCell(rnn_units)
        # Sampler
        self.sampler = tfa.seq2seq.sampler.TrainingSampler()
        # Create attention mechanism with memory = None
        self.attention_mechanism = \
            self.build_attention_mechanism(dense_units,None,BATCH_SIZE*[Tx])
        self.rnn_cell = self.build_rnn_cell(BATCH_SIZE)
        self.decoder = tfa.seq2seq.BasicDecoder(self.rnn_cell,
                                                sampler= self.sampler,
                                                output_layer = self.dense_layer
                                               )

    def build_attention_mechanism(self, units, memory, MSL):
        &amp;quot;&amp;quot;&amp;quot;
        MSL : Memory Sequence Length
        &amp;quot;&amp;quot;&amp;quot;
        #return tfa.seq2seq.LuongAttention(units, memory = memory,
        #                                  memory_sequence_length = MSL)
        return tfa.seq2seq.BahdanauAttention(units, memory = memory,
                                             memory_sequence_length = MSL)

    # wrap decodernn cell
    def build_rnn_cell(self, batch_size):
        return tfa.seq2seq.AttentionWrapper(self.decoder_rnncell,
                                            self.attention_mechanism,
                                            attention_layer_size=dense_units)

    def build_decoder_initial_state(self, batch_size, encoder_state, Dtype):
        decoder_initial_state = self.rnn_cell.get_initial_state(batch_size = batch_size,
                                                                dtype = Dtype)
        decoder_initial_state = decoder_initial_state.clone(cell_state = encoder_state)
        return decoder_initial_state
&lt;/code&gt;&lt;/pre&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;# Build Model
encoderNetwork = EncoderNetwork(input_vocab_size, embedding_dims, rnn_units)
decoderNetwork = DecoderNetwork(output_vocab_size, embedding_dims, rnn_units)

# Optimizer
optimizer = tf.keras.optimizers.Adam()
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Make custom training loop&lt;/p&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;def loss_function(y_pred, y):
    #shape of y [batch_size, ty]
    #shape of y_pred [batch_size, Ty, output_vocab_size]
    sparsecategoricalcrossentropy = SparseCategoricalCrossentropy(from_logits=True,
                                                                  reduction=&amp;#x27;none&amp;#x27;)
    loss = sparsecategoricalcrossentropy(y_true=y, y_pred=y_pred)
    mask = tf.logical_not(tf.math.equal(y,0))   #output 0 for y=0 else output 1
    mask = tf.cast(mask, dtype=loss.dtype)
    loss = mask * loss
    loss = tf.reduce_mean(loss)
    return loss

@tf.function
def train_step(input_batch, output_batch, encoder_initial_cell_state):
    # initialize loss = 0
    loss = 0
    with tf.GradientTape() as tape:
        encoder_emb_inp = encoderNetwork.encoder_embedding(input_batch)
        a, a_tx, c_tx = encoderNetwork.encoder_rnnlayer(encoder_emb_inp,
                                                        initial_state = encoder_initial_cell_state)

        # [last step activations,last memory_state] of
        # encoder passed as input to decoder Network

        # Prepare correct Decoder input &amp;amp; output sequence data
        decoder_input = output_batch[:,:-1] # ignore eos
        # compare logits with timestepped +1 version of decoder_input
        decoder_output = output_batch[:,1:] #ignore bos

        # Decoder Embeddings
        decoder_emb_inp = decoderNetwork.decoder_embedding(decoder_input)

        # Setting up decoder memory from encoder output
        # and Zero State for AttentionWrapperState
        decoderNetwork.attention_mechanism.setup_memory(a)
        decoder_initial_state = decoderNetwork.build_decoder_initial_state(BATCH_SIZE,
                                                                           encoder_state=[a_tx, c_tx],
                                                                           Dtype=tf.float32)

        # BasicDecoderOutput
        outputs, _, _ = decoderNetwork.decoder(decoder_emb_inp,initial_state=decoder_initial_state,
                                               sequence_length=BATCH_SIZE*[Ty-1])

        logits = outputs.rnn_output

        # Calculate loss
        loss = loss_function(logits, decoder_output)

    # Returns the list of all layer variables / weights.
    variables = encoderNetwork.trainable_variables + decoderNetwork.trainable_variables
    # differentiate loss wrt variables
    gradients = tape.gradient(loss, variables)

    # grads_and_vars â€“ List of(gradient, variable) pairs.
    grads_and_vars = zip(gradients,variables)
    optimizer.apply_gradients(grads_and_vars)
    return loss

@tf.function
def evaluate(input_batch, output_batch, encoder_initial_cell_state):
    loss = 0
    encoder_emb_inp = encoderNetwork.encoder_embedding(input_batch)
    a, a_tx, c_tx = encoderNetwork.encoder_rnnlayer(encoder_emb_inp,
                                                    initial_state =encoder_initial_cell_state)
    decoder_input = output_batch[:,:-1]
    decoder_output = output_batch[:,1:]
    decoder_emb_inp = decoderNetwork.decoder_embedding(decoder_input)
    decoderNetwork.attention_mechanism.setup_memory(a)
    decoder_initial_state = decoderNetwork.build_decoder_initial_state(BATCH_SIZE,
                                                                       encoder_state=[a_tx, c_tx],
                                                                       Dtype=tf.float32)
    outputs, _, _ = decoderNetwork.decoder(decoder_emb_inp,initial_state=decoder_initial_state,
                                           sequence_length=BATCH_SIZE*[Ty-1])
    logits = outputs.rnn_output
    loss = loss_function(logits, decoder_output)
    return loss
&lt;/code&gt;&lt;/pre&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;# RNN LSTM hidden and memory state initializer
def initialize_initial_state():
    return [tf.zeros((BATCH_SIZE, rnn_units)), tf.zeros((BATCH_SIZE, rnn_units))]
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&quot;calculating-bleu-score&quot;&gt;Calculating BLEU Score&lt;/h2&gt;&lt;blockquote&gt;&lt;p&gt;BLEU (bilingual evaluation understudy) is an algorithm for evaluating the quality of text which has been machine-translated from one natural language to another. Quality is considered to be the correspondence between a machine&amp;#x27;s output and that of a human: &amp;quot;the closer a machine translation is to a professional human translation, the better it is&amp;quot;. - Wikipedia&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;BLEU is a metric for evaluating a generated sentence to a reference sentence.
A perfect match results in a score of 1.0, whereas a perfect mismatch results in a score of 0.0.&lt;/p&gt;&lt;p&gt;So now we&amp;#x27;re going to define our translation function &amp;amp; calculate the BLEU score for test data at the end of every epoch while training. Note that test data is sentences that our tokenizer didn&amp;#x27;t train with, so there must be some words that our tokenizer didn&amp;#x27;t know.
I&amp;#x27;m currently working to fix this issue. Based on keras Tokenizer API it have &lt;code&gt;oov_token&lt;/code&gt; for handling this but i&amp;#x27;m not sure.&lt;/p&gt;&lt;p&gt;For now i&amp;#x27;m handling this by adding &lt;code&gt;unk&lt;/code&gt; in train dataset so the tokenizer can read it, and then when coming to translation if there is word that our tokenizer don&amp;#x27;t know i&amp;#x27;ll set it by index of &lt;code&gt;unk&lt;/code&gt; not very eficient but it works.&lt;/p&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;# Translate
def Translate(input_raw):
    input_raw = CP(preprocess(input_raw))
    input_lines = [&amp;#x27;bos &amp;#x27;+ input_raw + &amp;#x27;&amp;#x27;]

    input_sequences, unique = [], []
    for line in input_lines:
        temp = []
        for w in line.split(&amp;#x27; &amp;#x27;):
            try:
                temp.append(en_tokenizer.word_index[w])
            except: # Avoid Error
                unique.append(w)
                temp.append(en_tokenizer.word_index[&amp;#x27;unk&amp;#x27;])
        input_sequences.append(temp)

    input_sequences = pad_sequences(input_sequences, maxlen=Tx, padding=&amp;#x27;post&amp;#x27;)
    inp = tf.convert_to_tensor(input_sequences)
    inference_batch_size = input_sequences.shape[0]
    encoder_initial_cell_state = [tf.zeros((inference_batch_size, rnn_units)),
                                  tf.zeros((inference_batch_size, rnn_units))]
    encoder_emb_inp = encoderNetwork.encoder_embedding(inp)
    a, a_tx, c_tx = encoderNetwork.encoder_rnnlayer(encoder_emb_inp,
                                                    initial_state = encoder_initial_cell_state)

    start_tokens = tf.fill([inference_batch_size], jp_tokenizer.word_index[&amp;#x27;bos&amp;#x27;])

    end_token = jp_tokenizer.word_index[&amp;#x27;eos&amp;#x27;]

    greedy_sampler = tfa.seq2seq.GreedyEmbeddingSampler()

    decoder_input = tf.expand_dims([jp_tokenizer.word_index[&amp;#x27;bos&amp;#x27;]] * inference_batch_size,1)
    decoder_emb_inp = decoderNetwork.decoder_embedding(decoder_input)

    decoder_instance = tfa.seq2seq.BasicDecoder(cell = decoderNetwork.rnn_cell,
                                                sampler = greedy_sampler,
                                                output_layer = decoderNetwork.dense_layer)
    decoderNetwork.attention_mechanism.setup_memory(a)

    decoder_initial_state = decoderNetwork.build_decoder_initial_state(
        inference_batch_size, encoder_state=[a_tx, c_tx], Dtype=tf.float32)

    maximum_iterations = tf.round(tf.reduce_max(Tx) * 2)

    decoder_embedding_matrix = decoderNetwork.decoder_embedding.variables[0]
    (first_finished, first_inputs,first_state) = decoder_instance.initialize(
        decoder_embedding_matrix, start_tokens = start_tokens,
        end_token = end_token, initial_state = decoder_initial_state)

    inputs = first_inputs
    state = first_state
    predictions = np.empty((inference_batch_size,0), dtype = np.int32)
    for j in range(maximum_iterations):
        outputs, next_state, next_inputs, finished = decoder_instance.step(j, inputs,state)
        inputs = next_inputs
        state = next_state
        outputs = np.expand_dims(outputs.sample_id,axis = -1)
        predictions = np.append(predictions, outputs, axis = -1)

    res = &amp;#x27;&amp;#x27;
    for i in range(len(predictions)):
        line = predictions[i,:]
        seq = list(itertools.takewhile(lambda index: index !=2, line))
        res += &amp;quot; &amp;quot;.join( [jp_tokenizer.index_word[w] for w in seq])
    res = res.split()

    # Return back Unique words
    for i in range(len(res)):
        if res[i] == &amp;#x27;unk&amp;#x27; and unique != []:
            res[i] = unique.pop(0)

    return &amp;#x27; &amp;#x27;.join(res)

# Calculate BLEU
def BLEU(X, y):
    # Prediction
    pred = [Translate(w) for w in tqdm(X)]
    # Calculate BLEU
    score = sacrebleu.corpus_bleu(pred, [y]).score / 100
    return score, pred
&lt;/code&gt;&lt;/pre&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;# Custom Train Progress
class Progress:
    def __init__(self):
        self.fig = plt.figure(figsize = (8,6))
        self.ax = self.fig.add_subplot(1, 1, 1)
        self.loss, self.val_loss, self.BLEU = [], [], []
        self.epoch_loss = 0

    def get_val_loss(self):
        return [x[1] for x in self.val_loss]

    # Plot
    def dynamic_plot(self):
        self.ax.cla()
        self.ax.plot(range(len(self.loss)), self.loss, label=&amp;#x27;loss&amp;#x27;)
        if len(self.val_loss) &amp;gt;= 1:
            x = [l[0] for l in self.val_loss]
            y = [l[1] for l in self.val_loss]
            self.ax.plot(x, y, color = &amp;#x27;r&amp;#x27;, label=&amp;#x27;val_loss&amp;#x27;)
            self.ax.plot(x, self.BLEU, color = &amp;#x27;purple&amp;#x27;, label=&amp;#x27;BLEU&amp;#x27;)
        self.ax.set_ylim(0,)
        self.ax.legend(loc = 1)
        display(self.fig)

    # Train step progress
    def train_progress(self, epoch, step, steps_per_epoch, start):
        self.dynamic_plot()
        print(f&amp;#x27;Working on Epoch {epoch}&amp;#x27;)
        print(&amp;#x27;[&amp;#x27; + (&amp;#x27;=&amp;#x27; * int((step + 1) / steps_per_epoch * 60)).ljust(61, &amp;#x27; &amp;#x27;)
              + f&amp;#x27;]  {step + 1}/{steps_per_epoch} - loss : {round(self.epoch_loss / step, 4)}&amp;#x27;)
        print(f&amp;#x27;Time per Step {round(timeit.default_timer() - start, 2)} s&amp;#x27;)

    def summary(self):
        loss = np.array_split(np.array(self.loss), len(self.val_loss))
        loss = [np.mean(x) for x in loss]
        val_loss = [x[1] for x in self.val_loss]
        df = pd.DataFrame({&amp;#x27;Epochs&amp;#x27; : range(1, len(val_loss) + 1), &amp;#x27;loss&amp;#x27; : loss,
                           &amp;#x27;val loss&amp;#x27; : val_loss, &amp;#x27;BLEU&amp;#x27; : self.BLEU})

        self.dynamic_plot()
        clear_output(wait = True)
        display(df)
&lt;/code&gt;&lt;/pre&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;# Initialize Train Progress
TP = Progress()
best_prediction = []

for i in range(1, epochs + 1):

    encoder_initial_cell_state = initialize_initial_state()
    total_loss = 0.0
    # Train Loss
    TP.epoch_loss = 0

    # Train
    for (batch , (input_batch, output_batch)) in enumerate(dataset.take(steps_per_epoch)):
        start = timeit.default_timer()
        batch_loss = train_step(input_batch, output_batch, encoder_initial_cell_state)
        total_loss += batch_loss
        TP.loss.append(batch_loss.numpy())
        TP.epoch_loss += batch_loss.numpy()

        if (batch+1) % 30 == 0:
            TP.train_progress(i, batch, steps_per_epoch, start)
            clear_output(wait = True)

    # Validitate
    encoderNetwork.trainable = False  # Freeze our model layer to make sure
    decoderNetwork.trainable = False  # it didn&amp;#x27;t learn anything from val_data

    # Valid loss
    val_loss = 0
    for (batch, (input_batch, output_batch)) in enumerate(val_dataset.take(val_steps_per_epoch)):
        batch_loss = evaluate(input_batch, output_batch, encoder_initial_cell_state)
        val_loss += batch_loss.numpy()
    val_loss /= val_steps_per_epoch

    TP.val_loss.append((i * steps_per_epoch - 1, val_loss))

    # Bleu Score
    bleu_score, pred = BLEU(eng_test, jpn_test)
    TP.BLEU.append(bleu_score)

    encoderNetwork.trainable = True  # Unfreeze layer for next epoch
    decoderNetwork.trainable = True

    # Save best model
    if bleu_score == max(TP.BLEU) and val_loss == min(TP.get_val_loss()):
        best_prediction = pred
        encoderNetwork.save_weights(&amp;#x27;encoderNetwork&amp;#x27;)
        decoderNetwork.save_weights(&amp;#x27;decoderNetwork&amp;#x27;)

TP.summary()
&lt;/code&gt;&lt;/pre&gt;&lt;table&gt;&lt;thead&gt;&lt;tr&gt;&lt;th align=&quot;center&quot;&gt;Epochs&lt;/th&gt;&lt;th align=&quot;center&quot;&gt;loss&lt;/th&gt;&lt;th align=&quot;center&quot;&gt;val loss&lt;/th&gt;&lt;th align=&quot;center&quot;&gt;BLEU&lt;/th&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td align=&quot;center&quot;&gt;1&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;0.755007&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;0.617767&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;0.033657&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;center&quot;&gt;2&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;0.507803&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;0.467358&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;0.100527&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;center&quot;&gt;3&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;0.366943&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;0.404535&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;0.169416&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;center&quot;&gt;4&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;0.278531&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;0.382699&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;0.206622&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;center&quot;&gt;5&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;0.220114&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;0.379538&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;0.223136&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;center&quot;&gt;6&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;0.180076&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;0.384533&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;0.240543&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;center&quot;&gt;7&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;0.152284&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;0.390775&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;0.256055&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;&lt;span class=&quot;gatsby-resp-image-wrapper&quot; style=&quot;position:relative;display:block;margin-left:auto;margin-right:auto;max-width:490px&quot;&gt;
      &lt;a class=&quot;gatsby-resp-image-link&quot; href=&quot;/static/4968cd1de5dc4c8cb3cbadd3b3801588/41d3c/output_49_1.png&quot; style=&quot;display:block&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;
    &lt;span class=&quot;gatsby-resp-image-background-image&quot; style=&quot;padding-bottom:72.78481012658227%;position:relative;bottom:0;left:0;background-image:url(&amp;#x27;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAPCAYAAADkmO9VAAAACXBIWXMAAAsSAAALEgHS3X78AAACXklEQVQ4y52RT08TQRjG9zt48SuYgISEePBuIn6HBoMaE60XPXKRqjF69WI8ATcPRk0jCUYM8k+FQOgChZZu6F/ows7u0m53d3Zn3sdsWy0NJlIneTIzyby/93mfUTKZzFXDMN6apjlpmuZUL2KMTR0Y1lShWJrQdf29pmnXlFxu7y4X+K9FAKRdR7lYRKlcBmPsoVIsFmIuFyACJ6KwF0kpQ243wqOq7hmGEQHvK9ns7qjLKQIKgIjonO46D6NDEN0ZY3GlWMiPuFz2BPz9KNqbFYQgDEOYphlX8vvaTceXkKeA/4KeAQKBEKLlMK9lRku2hJAQRB2HPYzeDazp+ZHUgUTJbjmUktqdW/Gcw203sJBL3zqqEybWIPIW6PRYv4V2g865ncvfRi6WKjFAIlWFeL0c0o90HbYTdrmQ1FKTgTPqBqqqeicaM6rT66B3qy7ezJhY2zmBrjvwTxwgSgOyrRamE01zBWEoYJosrlQqlZiUf365WVGugZIqp8nFBn1YdymVOaGVbYusQ4saVUZulRFsRjAZwYpkcDh1GLYdV9Lp9O3213blLwTgB4BWCTG9EeCzyjGT4vi07uNLysf3HR9fVR/b+z5WM77QDjm8hvVA0TQtFgQBhBBewAPumq5XNxzuGA73bdeTns8R+Bwi8Fwv4DwUvMbJK9nSNxzBNUZe5lg6xzUOix3fUxKJxMVkMnllc2uzb2lhaWDs0dj1+dn5ge3cTv+T50+H55aXBtXsXt/YeOLGys/Fwd0ttf/ls/HhjYXpocAsXXr14vGw+u3jkFHKXp6dW7jwC/SSU1VUgwWcAAAAAElFTkSuQmCC&amp;#x27;);background-size:cover;display:block&quot;&gt;&lt;/span&gt;
  &lt;img class=&quot;gatsby-resp-image-image&quot; alt=&quot;png&quot; title=&quot;png&quot; src=&quot;/static/4968cd1de5dc4c8cb3cbadd3b3801588/41d3c/output_49_1.png&quot; srcSet=&quot;/static/4968cd1de5dc4c8cb3cbadd3b3801588/c26ae/output_49_1.png 158w,/static/4968cd1de5dc4c8cb3cbadd3b3801588/6bdcf/output_49_1.png 315w,/static/4968cd1de5dc4c8cb3cbadd3b3801588/41d3c/output_49_1.png 490w&quot; sizes=&quot;(max-width: 490px) 100vw, 490px&quot; style=&quot;width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0&quot; loading=&quot;lazy&quot; decoding=&quot;async&quot;/&gt;
  &lt;/a&gt;
    &lt;/span&gt;&lt;/p&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;# Load best weights
encoderNetwork.load_weights(&amp;#x27;encoderNetwork&amp;#x27;)
decoderNetwork.load_weights(&amp;#x27;decoderNetwork&amp;#x27;)
&lt;/code&gt;&lt;/pre&gt;&lt;pre&gt;&lt;code&gt;&amp;lt;tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7fb3601083d0&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Let&amp;#x27;s check the best prediction of our model.&lt;/p&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;for i in range(7,16):
    print(&amp;quot;English Sentence:&amp;quot;)
    print(eng_test[i])
    print(&amp;quot;\nJapanese Translation:&amp;quot;)
    print(best_prediction[i])
    print(&amp;quot;\nJapanese Reference:&amp;quot;)
    print(jpn_test[i])
    print(&amp;#x27;&amp;#x27;.ljust(60, &amp;#x27;-&amp;#x27;))
&lt;/code&gt;&lt;/pre&gt;&lt;pre&gt;&lt;code&gt;English Sentence:
in september there are just a few people here and there on the beach

Japanese Translation:
ï¼™ æ™‚ ã« ã¯ ã“ã“ ã« ä»– ã® äºº ãŒ ä½ã‚“ ã§ ã„ ãªã„ äºº ãŒ ãã® ãƒ“ãƒ¼ãƒ ã§ ä»– ã® äººé” ãŒ æ®‹ã‚Š ã® ã‚‚ã¨ ã‚’ ã‚ã¡ã‚‰ ã« å»ºè¨­ è¦‹ ãŸ

Japanese Reference:
ï¼™æœˆ ã® æµ· ã¯ äºº ãŒ ã¾ã°ã‚‰ ã  ã­
------------------------------------------------------------
English Sentence:
while you are young you should read a lot

Japanese Translation:
è‹¥ã„ é ƒ ã¯ èª­ã¿ çµ‚ã‚ã£ ãŸã‚‰ ã„ã„ ã‚ˆ

Japanese Reference:
è‹¥ã„ ã†ã¡ ã« ãŸãã•ã‚“ ã® æœ¬ ã‚’ èª­ã‚€ ã¹ã ã 
------------------------------------------------------------
English Sentence:
here i come

Japanese Translation:
ã“ã“ ã« æ¥ ãŸ ã®

Japanese Reference:
ã„ã¾ è¡Œã ã¾ã™
------------------------------------------------------------
English Sentence:
once you have decided when you will be coming let me know

Japanese Translation:
æ¥ã‚‹ ã‹ å› ã« ã¯ é€£çµ¡ ã‚’ è¨€ã£ ã¦ ã ãŸ ã‚ˆ

Japanese Reference:
ã„ã¤ æ¥ã‚‹ ã‹ æ±ºã¾ã£ ãŸã‚‰ æ•™ãˆ ã¦
------------------------------------------------------------
English Sentence:
he jumped on the train

Japanese Translation:
å½¼ ã¯ é›»è»Š ã« æ—— ã‚’ é£›ã³è¶Šãˆ ãŸ

Japanese Reference:
å½¼ ã¯ é›»è»Š ã« é£›ã³ä¹—ã£ ãŸ
------------------------------------------------------------
English Sentence:
he passed away yesterday

Japanese Translation:
å½¼ ã¯ æ˜¨æ—¥ äº¡ããªã£ ãŸ

Japanese Reference:
å½¼ ã¯ æ˜¨æ—¥ ãŠ äº¡ããªã‚Š ã« ãªã‚Š ã¾ã— ãŸ
------------------------------------------------------------
English Sentence:
i had no other choice

Japanese Translation:
ä»– ã« é¸æŠžè‚¢ ãŒ ãªã‹ã£ ãŸ

Japanese Reference:
ä»– ã« æ‰‹ ãŒ ãªã‹ã£ ãŸ ã® ã 
------------------------------------------------------------
English Sentence:
are you good at bowling

Japanese Translation:
ãƒœãƒ¼ãƒªãƒ³ã‚° ã¯ å¾—æ„ ã§ã™ ã‹

Japanese Reference:
ãƒœã‚¦ãƒªãƒ³ã‚° ã¯ å¾—æ„
------------------------------------------------------------
English Sentence:
it is strange that you do not know anything about that matter

Japanese Translation:
ãã‚Œ ã«ã¤ã„ã¦ ä½• ã‚‚ çŸ¥ã‚‰ ãªã„ ã“ã¨ ãŒ ãªã„ ã¨ã„ã† ã“ã¨ ã¯ å¤‰ ã 

Japanese Reference:
ã‚ãªãŸ ãŒ ãã® ã“ã¨ ã«ã¤ã„ã¦ ä½• ã‚‚ çŸ¥ã‚‰ ãªã„ ã® ã¯ å¤‰ ã 
------------------------------------------------------------
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&quot;test-with-some-raw-input&quot;&gt;Test with Some Raw Input&lt;/h2&gt;&lt;p&gt;Yeay now let&amp;#x27;s play with &lt;strong&gt;our&lt;/strong&gt; Machine Translation with some raw input. We&amp;#x27;ll cross check the prediction from MT with Google Translate API to translate it back to english and see how bad &lt;strong&gt;our&lt;/strong&gt; MT is :).&lt;/p&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;from googletrans import Translator
# Google Translate
translator = Translator()
&lt;/code&gt;&lt;/pre&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;raw_input = [&amp;#x27;i love you&amp;#x27;, &amp;#x27;i am sorry&amp;#x27;, &amp;#x27;hello&amp;#x27;, &amp;#x27;thank you&amp;#x27;,
             &amp;#x27;is there something i can help?&amp;#x27;]

for i in range(len(raw_input)):
    prediction = Translate(raw_input[i])
    print(&amp;quot;English Sentence:&amp;quot;)
    print(raw_input[i])
    print(&amp;quot;\nJapanese Translation:&amp;quot;)
    print(prediction)
    print(&amp;quot;\nEnglish Translation from prediction [GoogleTranslate]:&amp;quot;)
    print(translator.translate(prediction).text)
    print(&amp;#x27;&amp;#x27;.ljust(60, &amp;#x27;-&amp;#x27;))
&lt;/code&gt;&lt;/pre&gt;&lt;pre&gt;&lt;code&gt;English Sentence:
i love you

Japanese Translation:
æ„›ã— ã¦ã‚‹ ã‚ˆ

English Translation from prediction [GoogleTranslate]:
I love you
------------------------------------------------------------
English Sentence:
i am sorry

Japanese Translation:
ã™ã¿ã¾ã›ã‚“

English Translation from prediction [GoogleTranslate]:
Excuse me
------------------------------------------------------------
English Sentence:
hello

Japanese Translation:
ã‚‚ã—ã‚‚ã—

English Translation from prediction [GoogleTranslate]:
Hello
------------------------------------------------------------
English Sentence:
thank you

Japanese Translation:
ã‚ã‚ŠãŒã¨ã† ã”ã–ã„ ã¾ã™

English Translation from prediction [GoogleTranslate]:
Thank you
------------------------------------------------------------
English Sentence:
is there something i can help?

Japanese Translation:
ä½• ã‹ æ‰‹ä¼ãˆã‚‹ ã‚‚ã® ãŒ ã‚ã‚‹ ã®

English Translation from prediction [GoogleTranslate]:
There is something to help
------------------------------------------------------------
&lt;/code&gt;&lt;/pre&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;import pickle

with open(&amp;#x27;en_tokenizer.pickle&amp;#x27;, &amp;#x27;wb&amp;#x27;) as handle:
    pickle.dump(en_tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)
handle.close()

with open(&amp;#x27;jp_tokenizer.pickle&amp;#x27;, &amp;#x27;wb&amp;#x27;) as handle:
    pickle.dump(jp_tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)
handle.close()
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&quot;reference&quot;&gt;Reference&lt;/h2&gt;&lt;ol&gt;&lt;li&gt;TensorFlow Addons Networks : Sequence-to-Sequence NMT with Attention Mechanism &lt;a href=&quot;https://www.tensorflow.org/addons/tutorials/networks_seq2seq_nmt&quot;&gt;Link&lt;/a&gt;&lt;/li&gt;&lt;li&gt;seq2seq (Sequence to Sequence) Model for Deep Learning with PyTorch &lt;a href=&quot;https://www.guru99.com/seq2seq-model.html&quot;&gt;Link&lt;/a&gt;&lt;/li&gt;&lt;/ol&gt;</content:encoded></item></channel></rss>