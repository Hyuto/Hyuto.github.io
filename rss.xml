<?xml version="1.0" encoding="UTF-8"?><rss xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" version="2.0"><channel><title><![CDATA[Hyuto's Blog]]></title><description><![CDATA[Wahyu Setianto Personal Blog.]]></description><link>https://Hyuto.github.io</link><generator>GatsbyJS</generator><lastBuildDate>Thu, 21 Oct 2021 14:50:13 GMT</lastBuildDate><item><title><![CDATA[Sentiment Detector [ID]]]></title><description><![CDATA[Sentiment detector built with focused corona topic dataset using SVM]]></description><link>https://Hyuto.github.io/showcase/sa-corona</link><guid isPermaLink="false">https://Hyuto.github.io/showcase/sa-corona</guid><pubDate>Sat, 12 Jun 2021 05:00:00 GMT</pubDate><content:encoded>
                        &lt;div&gt;
                          &lt;h2&gt;Sentiment Detector [ID]&lt;/h2&gt;
                          &lt;p&gt;Sentiment detector built with focused corona topic dataset using SVM&lt;/p&gt;
                        &lt;/div&gt;
                      </content:encoded></item><item><title><![CDATA[Digit Recognizer]]></title><description><![CDATA[MNIST Digit Recognizer using Tensorflow.js]]></description><link>https://Hyuto.github.io/showcase/digit-recognizer</link><guid isPermaLink="false">https://Hyuto.github.io/showcase/digit-recognizer</guid><pubDate>Sun, 10 Jan 2021 03:00:00 GMT</pubDate><content:encoded>
                        &lt;div&gt;
                          &lt;h2&gt;Digit Recognizer&lt;/h2&gt;
                          &lt;p&gt;MNIST Digit Recognizer using Tensorflow.js&lt;/p&gt;
                        &lt;/div&gt;
                      </content:encoded></item><item><title><![CDATA[Hoax Classification - BDC Satria Data 2020]]></title><description><![CDATA[Notebook ini adalah script yang kelompok  Catatan Cakrawala  gunakan pada saat mengikuti lomba
BDC Satria Data 2020 yang di selenggarakan‚Ä¶]]></description><link>https://Hyuto.github.io/bdc-main-notebook/</link><guid isPermaLink="false">https://Hyuto.github.io/bdc-main-notebook/</guid><pubDate>Fri, 16 Oct 2020 17:28:47 GMT</pubDate><content:encoded>&lt;p&gt;Notebook ini adalah script yang kelompok &lt;strong&gt;Catatan Cakrawala&lt;/strong&gt; gunakan pada saat mengikuti lomba
BDC Satria Data 2020 yang di selenggarakan oleh IPB dan PUSPRESNAS.&lt;/p&gt;&lt;h2 id=&quot;author&quot;&gt;Author&lt;/h2&gt;&lt;ul&gt;&lt;li&gt;&lt;a href=&quot;https://www.linkedin.com/in/dimas-k-jati/&quot;&gt;Dimas Kuncoro Jati&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href=&quot;https://www.linkedin.com/in/muhammadamanda/&quot;&gt;Muhammad Amanda&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href=&quot;https://www.linkedin.com/in/wahyu-setianto/&quot;&gt;Wahyu Setianto&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;h2 id=&quot;running-environment&quot;&gt;Running Environment&lt;/h2&gt;&lt;ul&gt;&lt;li&gt;Platform : Kaggle&lt;/li&gt;&lt;li&gt;Accelerator : TPU&lt;/li&gt;&lt;li&gt;Dataset&lt;/li&gt;&lt;/ul&gt;&lt;table&gt;&lt;thead&gt;&lt;tr&gt;&lt;th align=&quot;left&quot;&gt;Data&lt;/th&gt;&lt;th align=&quot;left&quot;&gt;Keterangan&lt;/th&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://www.kaggle.com/wahyusetianto/data-bdc&quot;&gt;BDC - Satria Data Catatan Cakrawala&lt;/a&gt;&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;Data original dan data yang telah di preprocess oleh tim Catatan Cakrawala&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://www.kaggle.com/pencarikebahagiaan/modelku&quot;&gt;Catatan Cakrawala Model&lt;/a&gt; [Optional]&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;Model terbaik yang telah di train tim Catatan Cakrawala&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;h3 id=&quot;note&quot;&gt;Note&lt;/h3&gt;&lt;p&gt;Github repository : &lt;a href=&quot;https://github.com/Hyuto/BDC-Satria-Data&quot;&gt;BDC-Satria-Data&lt;/a&gt;&lt;/p&gt;&lt;p&gt;Notebook ini adalah notebook yang kami run di &lt;a href=&quot;https://www.kaggle.com/&quot;&gt;kaggle&lt;/a&gt;, jika anda ingin
mencoba running notebook ini dengan environment yang sama dengan yang kami gunakan kunjungi link berikut&lt;/p&gt;&lt;p&gt;Kaggle Notebook : &lt;a href=&quot;https://www.kaggle.com/wahyusetianto/bdc-main-notebook&quot;&gt;Catatan Cakrawala Notebook - BDC Satria Data&lt;/a&gt;&lt;/p&gt;&lt;p&gt;P.S. Jangan lupa untuk &lt;em&gt;upvote&lt;/em&gt; üòä.&lt;/p&gt;&lt;h1 id=&quot;bdc---satria-data-2020&quot;&gt;BDC - Satria Data 2020&lt;/h1&gt;&lt;h2 id=&quot;task&quot;&gt;Task&lt;/h2&gt;&lt;p&gt;Mendeteksi apakah suatu berita adalah hoax atau bukan hoax (&lt;code&gt;Binary Classification&lt;/code&gt;).&lt;/p&gt;&lt;h2 id=&quot;data&quot;&gt;Data&lt;/h2&gt;&lt;ul&gt;&lt;li&gt;Data Gambar&lt;/li&gt;&lt;li&gt;Data Text (judul dan narasi berita)&lt;/li&gt;&lt;/ul&gt;&lt;h2 id=&quot;first-things-first&quot;&gt;First Things First&lt;/h2&gt;&lt;ul&gt;&lt;li&gt;Install &amp;amp; Load Library&lt;/li&gt;&lt;li&gt;Global SEED-ing&lt;/li&gt;&lt;li&gt;Keeping Google Cloud Storage PATH&lt;/li&gt;&lt;/ul&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;# Install Package yang Dibutuhkan
!pip install -q --upgrade pip
!pip install -q efficientnet  # EfficientNet
!pip -q install sastrawi      # Santrawi

# Imports
import os, random, re, string, emoji
from timeit import default_timer
from tqdm.notebook import tqdm

# Kaggle Datasets for checking GCS
from kaggle_datasets import KaggleDatasets

# Scientific tools
import numpy as np
import pandas as pd

# Plotting tools
import matplotlib.pyplot as plt
import seaborn as sns
plt.style.use(&amp;quot;seaborn-notebook&amp;quot;)

# Interactive
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots

# Image processing
from PIL import Image
from skimage.transform import rotate

# Tensorflow and Keras
import tensorflow as tf
from tensorflow.keras.preprocessing.image import load_img, img_to_array

# Scikit Learn
from sklearn.utils import shuffle
from sklearn.metrics import *
from sklearn.model_selection import *
from sklearn.feature_extraction.text import CountVectorizer

# SEED ALL
SEED = 42

os.environ[&amp;#x27;PYTHONHASHSEED&amp;#x27;] = str(SEED)
random.seed(SEED)
np.random.seed(SEED)

os.environ[&amp;#x27;TF_DETERMINISTIC_OPS&amp;#x27;] = str(SEED)
tf.random.set_seed(SEED)

# GCS PATH
GCS_PATH = KaggleDatasets().get_gcs_path(&amp;#x27;data-bdc&amp;#x27;)

# Out
print(f&amp;#x27;Using Tensorflow Version       : {tf.__version__}&amp;#x27;)
print(f&amp;#x27;Google Cloud Storage Data Path : {GCS_PATH}&amp;#x27;)
&lt;/code&gt;&lt;/pre&gt;&lt;pre&gt;&lt;code&gt;Using Tensorflow Version       : 2.2.0
Google Cloud Storage Data Path : gs://kds-7a3b4bb00789754aa5925da7c7a9217df698684ced337b42a9790257
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&quot;preprocessing-data&quot;&gt;Preprocessing Data&lt;/h2&gt;&lt;p&gt;Melakukan preprocessing data sebelum diolah&lt;/p&gt;&lt;h3 id=&quot;load-dataset&quot;&gt;Load Dataset&lt;/h3&gt;&lt;p&gt;Load dataset ke memory&lt;/p&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;train = pd.read_excel(&amp;#x27;../input/data-bdc/Data BDC - Satria Data 2020/Data Latih/Data Latih BDC.xlsx&amp;#x27;)
test = pd.read_excel(&amp;#x27;../input/data-bdc/Data BDC - Satria Data 2020/Data Uji/Data Uji BDC.xlsx&amp;#x27;)
train.head()
&lt;/code&gt;&lt;/pre&gt;&lt;table&gt;&lt;thead&gt;&lt;tr&gt;&lt;th&gt;ID&lt;/th&gt;&lt;th&gt;label&lt;/th&gt;&lt;th&gt;tanggal&lt;/th&gt;&lt;th&gt;judul&lt;/th&gt;&lt;th&gt;narasi&lt;/th&gt;&lt;th&gt;nama file gambar&lt;/th&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;71&lt;/td&gt;&lt;td&gt;1&lt;/td&gt;&lt;td&gt;2020-08-17 00:00:00&lt;/td&gt;&lt;td&gt;Pemakaian Masker Menyebabkan Penyakit Legionna...&lt;/td&gt;&lt;td&gt;A caller to a radio talk show recently shared ...&lt;/td&gt;&lt;td&gt;71.jpg&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;461&lt;/td&gt;&lt;td&gt;1&lt;/td&gt;&lt;td&gt;2020-07-17 00:00:00&lt;/td&gt;&lt;td&gt;Instruksi Gubernur Jateng tentang penilangan ...&lt;/td&gt;&lt;td&gt;Yth.Seluruh Anggota Grup Sesuai Instruksi Gube...&lt;/td&gt;&lt;td&gt;461.png&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;495&lt;/td&gt;&lt;td&gt;1&lt;/td&gt;&lt;td&gt;2020-07-13 00:00:00&lt;/td&gt;&lt;td&gt;Foto Jim Rohn: Jokowi adalah presiden terbaik ...&lt;/td&gt;&lt;td&gt;Jokowi adalah presiden terbaik dlm sejarah ban...&lt;/td&gt;&lt;td&gt;495.png&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;550&lt;/td&gt;&lt;td&gt;1&lt;/td&gt;&lt;td&gt;2020-07-08 00:00:00&lt;/td&gt;&lt;td&gt;ini bukan politik, tapi kenyataan Pak Jokowi b...&lt;/td&gt;&lt;td&gt;Maaf Mas2 dan Mbak2, ini bukan politik, tapi k...&lt;/td&gt;&lt;td&gt;550.png&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;681&lt;/td&gt;&lt;td&gt;1&lt;/td&gt;&lt;td&gt;2020-06-24 00:00:00&lt;/td&gt;&lt;td&gt;Foto Kadrun kalo lihat foto ini panas dingin&lt;/td&gt;&lt;td&gt;Kadrun kalo lihat foto ini panas dingin . .&lt;/td&gt;&lt;td&gt;681.jpg&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;h3 id=&quot;download-fungsi-tambahan&quot;&gt;Download Fungsi Tambahan&lt;/h3&gt;&lt;p&gt;Mendownload fungsi yang sudah dibuat dari responsitory kami.&lt;/p&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;!wget -q https://raw.githubusercontent.com/Hyuto/BDC-Satria-Data/master/Preprocess%20code/RPU.py
!wget -q https://raw.githubusercontent.com/Hyuto/BDC-Satria-Data/master/Preprocess%20code/Preprocess.py
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&quot;data-gambar&quot;&gt;Data Gambar&lt;/h2&gt;&lt;p&gt;Melakukan preprocessing pada data gambar&lt;/p&gt;&lt;h3 id=&quot;sesuaikan-path-data-gambar&quot;&gt;Sesuaikan Path data gambar&lt;/h3&gt;&lt;p&gt;Menyesuaikan direktori data gambar&lt;/p&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;# Train &amp;amp; Test Image PATH
TRAIN_PATH = &amp;#x27;../input/data-bdc/Data BDC - Satria Data 2020/Data Latih/File Gambar Data Latih/&amp;#x27;
TEST_PATH = &amp;#x27;../input/data-bdc/Data BDC - Satria Data 2020/Data Uji/File Gambar Data Uji/&amp;#x27;
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;tambahkan Path ke nama file gambar&lt;/p&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;# Apply to nama file
TRAIN_IMG = [TRAIN_PATH + x for x in train[&amp;#x27;nama file gambar&amp;#x27;].values]
TEST_IMG = [TEST_PATH + x for x in test[&amp;#x27;nama file gambar&amp;#x27;].values]
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&quot;checking-missing-file&quot;&gt;Checking Missing File&lt;/h3&gt;&lt;p&gt;Mengecek apakah ada file yang hilang / tidak bisa terbaca pada direktori gambar.&lt;/p&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;def check_missing(files, return_missing_ID = True):
    &amp;quot;&amp;quot;&amp;quot;
    Mengecek keberadaan data gambar berdasarkan direktori
    &amp;quot;&amp;quot;&amp;quot;
    missing = []
    for file in files:
        if not os.path.isfile(file):
            missing.append(file)
    print(f&amp;#x27;[INFO] Missing {len(missing)} file&amp;#x27;)
    if return_missing_ID:
        return sorted([int(x.split(&amp;#x27;/&amp;#x27;)[-1][:-4]) for x in missing])
&lt;/code&gt;&lt;/pre&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;# Missing pada data TRAIN
missing_train = check_missing(TRAIN_IMG)
missing_train
&lt;/code&gt;&lt;/pre&gt;&lt;pre&gt;&lt;code&gt;[INFO] Missing 9 file

[48121, 275477, 343052, 367583, 555990, 697754, 697955, 742855, 743885]
&lt;/code&gt;&lt;/pre&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;# Missing pada data TEST
missing_test = check_missing(TEST_IMG)
missing_test
&lt;/code&gt;&lt;/pre&gt;&lt;pre&gt;&lt;code&gt;[INFO] Missing 2 file

[690192, 693499]
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Terdapat 9 file gambar pada data train dan 2 file gambar pada data test yang hilang. Hal ini diduga karena nama file berbeda pada direktori gambar dengan nama file yang ada pada dataframe. Maka dari itu kami melakukan pengecekan nama file pada direktori gambar dengan menyamakan &lt;code&gt;ID&lt;/code&gt;-nya.&lt;/p&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;def fixing_extensions(missing, path):
    &amp;quot;&amp;quot;&amp;quot;
    Membenarkan ekstensi file gambar yang di anggap hilang
    dari direktori.
    &amp;quot;&amp;quot;&amp;quot;
    res = []
    for miss in missing:
        fixed = False
        list_dir = os.listdir(path)
        for i in range(len(list_dir)):
            if miss == int(list_dir[i][:-4]):
                fixed = True
                res.append((miss, list_dir[i]))
                break
        if not fixed:
            res.append((miss, &amp;#x27;404&amp;#x27;)) # Not Found
    return res
&lt;/code&gt;&lt;/pre&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;# Apply function
fixed_missing_train = fixing_extensions(missing_train, TRAIN_PATH)
fixed_missing_test = fixing_extensions(missing_test, TEST_PATH)
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Setelah nama file yang hilang dibenarkan, apply nama file yang benar ke main dataframe.&lt;/p&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;# Apply to train DF
for Id, filename in fixed_missing_train:
    index = train.ID.tolist().index(Id)
    train.loc[index, &amp;#x27;nama file gambar&amp;#x27;] = filename

# Apply to test DF
for Id, filename in fixed_missing_test:
    index = test.ID.tolist().index(Id)
    test.loc[index, &amp;#x27;nama file gambar&amp;#x27;] = filename
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Spesifikasi ulang nama file gambar&lt;/p&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;TRAIN_X = [TRAIN_PATH + x for x in train[&amp;#x27;nama file gambar&amp;#x27;].values if x != &amp;#x27;404&amp;#x27;]
TRAIN_y = train.label.values
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&quot;menampilkan-data-gambar&quot;&gt;Menampilkan Data Gambar&lt;/h3&gt;&lt;p&gt;Menampilkan beberapa data gambar yang ada&lt;/p&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;def read_and_resize(path):
    &amp;quot;&amp;quot;&amp;quot;
    Read &amp;amp; Resize data gambar
    &amp;quot;&amp;quot;&amp;quot;
    img = Image.open(path)
    img.resize((256, 256), Image.ANTIALIAS)
    return img

def show_images(list_dir, label, load_image = read_and_resize, seed = SEED):
    &amp;quot;&amp;quot;&amp;quot;
    Menampilkan Gambar Secara acak berdasarkan kelasnya
    masing - masing sebanyak 5 buah.
    &amp;quot;&amp;quot;&amp;quot;
    random.seed(seed)
    data_0 = random.sample([x for x in zip(list_dir, label) if x[1] == 0], 5)
    data_1 = random.sample([x for x in zip(list_dir, label) if x[1] == 1], 5)
    fig, axes = plt.subplots(2, 5, figsize = (20, 10))
    for i in range(2):
        if i == 0:
            data = data_0
        else:
            data = data_1
        for j in range(5):
            img = load_image(data[j][0])
            axes[i, j].imshow(img)
            axes[i, j].set_title(f&amp;#x27;Label : {data[j][1]}&amp;#x27;, fontsize = 14)
            axes[i, j].axis(&amp;#x27;off&amp;#x27;)
    fig.tight_layout()
    plt.show()
&lt;/code&gt;&lt;/pre&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;show_images(TRAIN_X, TRAIN_y)
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;span class=&quot;gatsby-resp-image-wrapper&quot; style=&quot;position:relative;display:block;margin-left:auto;margin-right:auto;max-width:630px&quot;&gt;
      &lt;a class=&quot;gatsby-resp-image-link&quot; href=&quot;/static/545399b05d7ca1491d61ceacd714411c/5bd27/output_27_0.png&quot; style=&quot;display:block&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;
    &lt;span class=&quot;gatsby-resp-image-background-image&quot; style=&quot;padding-bottom:50%;position:relative;bottom:0;left:0;background-image:url(&amp;#x27;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAKCAYAAAC0VX7mAAAACXBIWXMAAAsSAAALEgHS3X78AAADGUlEQVQoz12SX0xTBxyFf4PhY8GVF+OyhwUDM6iIhOKWGgnEwBJd4txMo4LRJXuZkDmdzogsW5wErYiKDixEENIKSKAFApYJiAW04ril/0vh1tLbll4Y3YXWFuQsNfFlJ/neTr6ch0MsyxKAd9y4XUM7s3Kysz7fo0pLz2xN37U7w84hdWTUsFfzxCBtVk9mvO/GcC+tEuedp3WAloVlMhqmiCwWS9P4+HgfwzBPAFRu2S45vi0nD6mZUuQfOFw0xwX1DtssPF4eVqefDUdXrwCojq5Gby2urF0MBBZlc5z3hIt1nbRbrYdiwpDBMAWGMWCB5+1pmdIjKelZ+CRlK3ILvpKF3oT10y43Xrs86yFB8LTdu7dWf60Cg0PPIIRCYb+fB88vwMN5YTSa18lsNrsZhonGhCvCv2Np0v3F0uztKJR8CtnXhUVRwFlXo4b85+swOfhQj6LGqir/kdcNDi1Eom8mvZy/2+fzDwaFlWGfn28nk8n0Gcuy2+x2+04AmxM2frw/JT0rkLZD8k9mzheFAEqVdzTyWw11lUbXchmATQCSAIjDQSGpt/zCh6Gq7+Kf/3k/3mB7HU/Hjh2lYFAgm9VGcvk1MlvNMufsbNfC0pI6HI3kl589I9L9baox2dgq10TXR68mJ+gV84Ii3DTNDD0mzfkL9KysjEYU9TTj4YhEiUnXh5/qmpSqNiUR/cTa5mvnXAE8H9aDNbrPWuz27KdjFlgcHADkNzS3fH+3rvqmWftIbtW0/qI+VXr6YVHxrx2X/yh3ergSEovF61crf8P5cyWIi/vAqdcOVLUolGi8cxcdivpT4dXIjo6bv0OrvP8WQF5ndw+n7evApH4QvW2qtZfqToz192CsW4N53g8SJydP7duX6yssyA8mbNjQU3+7rvrIQRkuXzqNzubaEiEUydC9MMPhdMcW5t64WqH+9ksJd+aHg6yqRTHqGOofMekGDKN9j6d8AX6ARCKRyOZwJz5o6UqOnbW1XVvVoGhCU2Mj/tJqS6adMxleP4/FJQEmm0tySdFPtYe+SSCieJwrjaP/5T/2SQSXGKzf4QAAAABJRU5ErkJggg==&amp;#x27;);background-size:cover;display:block&quot;&gt;&lt;/span&gt;
  &lt;img class=&quot;gatsby-resp-image-image&quot; alt=&quot;png&quot; title=&quot;png&quot; src=&quot;/static/545399b05d7ca1491d61ceacd714411c/f058b/output_27_0.png&quot; srcSet=&quot;/static/545399b05d7ca1491d61ceacd714411c/c26ae/output_27_0.png 158w,/static/545399b05d7ca1491d61ceacd714411c/6bdcf/output_27_0.png 315w,/static/545399b05d7ca1491d61ceacd714411c/f058b/output_27_0.png 630w,/static/545399b05d7ca1491d61ceacd714411c/40601/output_27_0.png 945w,/static/545399b05d7ca1491d61ceacd714411c/78612/output_27_0.png 1260w,/static/545399b05d7ca1491d61ceacd714411c/5bd27/output_27_0.png 1432w&quot; sizes=&quot;(max-width: 630px) 100vw, 630px&quot; style=&quot;width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0&quot; loading=&quot;lazy&quot; decoding=&quot;async&quot;/&gt;
  &lt;/a&gt;
    &lt;/span&gt;&lt;/p&gt;&lt;p&gt;Dapat dilihat gambar memiliki resolusi yang &lt;code&gt;berbeda - beda&lt;/code&gt;.&lt;/p&gt;&lt;h3 id=&quot;preprocess-and-resizing&quot;&gt;Preprocess and Resizing&lt;/h3&gt;&lt;p&gt;Untuk mengatasi data gambar dengan resolusi yang berbeda - beda maka dilakukan dengan metode croping pada bagian &lt;code&gt;center&lt;/code&gt; gambar dan &lt;code&gt;resizing&lt;/code&gt; gambar tersebut ke ukuran yang di tetapkan.&lt;/p&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;def load_and_preprocess_image(path: str, size = [256, 256]):
    &amp;quot;&amp;quot;&amp;quot;
    Load &amp;amp; Preprocess data gambar
    &amp;quot;&amp;quot;&amp;quot;
    image = img_to_array(load_img(path))
    img = tf.convert_to_tensor(image, dtype=tf.float32)
    shapes = tf.shape(img)
    h, w = shapes[-3], shapes[-2]
    dim = tf.minimum(h, w)
    img = tf.image.resize_with_crop_or_pad(img, dim, dim)
    img = tf.image.resize(img, size)
    img = tf.cast(img, tf.float32) / 255.0
    return img.numpy()
&lt;/code&gt;&lt;/pre&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;show_images(TRAIN_X, TRAIN_y, load_and_preprocess_image)
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;span class=&quot;gatsby-resp-image-wrapper&quot; style=&quot;position:relative;display:block;margin-left:auto;margin-right:auto;max-width:630px&quot;&gt;
      &lt;a class=&quot;gatsby-resp-image-link&quot; href=&quot;/static/f23197e17aa8023fc2c7acd396292c4d/5bd27/output_31_0.png&quot; style=&quot;display:block&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;
    &lt;span class=&quot;gatsby-resp-image-background-image&quot; style=&quot;padding-bottom:44.93670886075949%;position:relative;bottom:0;left:0;background-image:url(&amp;#x27;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAJCAYAAAAywQxIAAAACXBIWXMAAAsSAAALEgHS3X78AAAC2UlEQVQozwXBiU9SARwA4PdntGWWeWTLI9O5LNfMsUJdp7C0tWqaa820LZ2r2eHWMRWzrWbOqSW4hitX3hfCQhP1oY/34AEiJALv6IE8OQIFj1/fh5AkKdFoNDI9QXxZxnTvY09kN2bkXpBl5gp6zgguvVPNGpoJwx8pStilynlbMwC07OztSXeiUene7m4jx3lanRQj9fJ8j9fjbUJIkgwRBAFWqxUMpHEnMT3Hl5YjgPj0s5B6On+LMFiDLM0C7/MDzXiCRtK0ZVsxg3V9Hfz/Qj6GYXb8/gBsb0cB0+EBBMdxGsMw0Ov1+2r1jCc2+ZQj4XgKxByJg2MpJ1m7y0WptRaYHluEVcsaVVdazFYV5kHt43qgWLfDvrbmYWh6PxzeBkJvcCJms3nDaDSCzboKpNkSSMzKY6/lJkBJ9gG4ei6Zd7g33Z8l30FS0wCYZcOt+vSa/91cB1Njk7C56WMdDioQ3IrCdnQXAsEwhywtLRdgGCbGcZ1obmGx6HxhnrDsZoH4zfOHohf1lYUWm71gbBwT95OYaMXlKyAtpkK7wykOR6Jinl4XLil+FpHKabEZw0U6vUmIJCQmDXv5AGq12RdjDh6ecJj5gU1vWDs1qFy0EY5R3usfYv5yWtgHNBwODT1peDna2SVB9epB1CTvGOi6fWdCVf0I7SkrRycVykEkKzMjaCKXYHi4DzIzUiKoSssP9KugXdIGM2PTIa1y3N9ZUwG9r56Bh+P8TooJWfBZsJp1sG608OqJkciabRXm+r8BoV3wIfFxsayoWAhXLudDemryxpCs3VlVUQndbW9BMSjn5lWTdOvTKuj92AK0y0WLS29xvbIm+DUrh9aOZuekvGOD93Kg+CoDHbpAITGHYrnye3ehqvIBHE1I5j+0dFPFF69DbfV9mBroc7PeCKtZjQDlAwgFfGxuWpK7VJgJNwRpUCLIppZH5Pwez4Gm/wfocZL5D4xx/ne1mvD3AAAAAElFTkSuQmCC&amp;#x27;);background-size:cover;display:block&quot;&gt;&lt;/span&gt;
  &lt;img class=&quot;gatsby-resp-image-image&quot; alt=&quot;png&quot; title=&quot;png&quot; src=&quot;/static/f23197e17aa8023fc2c7acd396292c4d/f058b/output_31_0.png&quot; srcSet=&quot;/static/f23197e17aa8023fc2c7acd396292c4d/c26ae/output_31_0.png 158w,/static/f23197e17aa8023fc2c7acd396292c4d/6bdcf/output_31_0.png 315w,/static/f23197e17aa8023fc2c7acd396292c4d/f058b/output_31_0.png 630w,/static/f23197e17aa8023fc2c7acd396292c4d/40601/output_31_0.png 945w,/static/f23197e17aa8023fc2c7acd396292c4d/78612/output_31_0.png 1260w,/static/f23197e17aa8023fc2c7acd396292c4d/5bd27/output_31_0.png 1432w&quot; sizes=&quot;(max-width: 630px) 100vw, 630px&quot; style=&quot;width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0&quot; loading=&quot;lazy&quot; decoding=&quot;async&quot;/&gt;
  &lt;/a&gt;
    &lt;/span&gt;&lt;/p&gt;&lt;h3 id=&quot;image-augmentation&quot;&gt;Image Augmentation&lt;/h3&gt;&lt;p&gt;Karena data yang ada tidak seimbang maka perlu dilakukan penambahan data. Metode yang digunakan untuk menambahkan data yaitu augmentasi data gambar pada kelas yang sedikit dengan melakukan &lt;code&gt;random rotation&lt;/code&gt;.&lt;/p&gt;&lt;p&gt;Metode Augmentasi yang digunakan.&lt;/p&gt;&lt;pre&gt;&lt;code&gt;Rotasi Acak dengan nilai rotasi di random pada kisaran -70 sd 70 derajad
&lt;/code&gt;&lt;/pre&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;data_augmentation = lambda x : (
    rotate(x, random.randint(-70, 70), mode=&amp;#x27;reflect&amp;#x27;)
)
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Menampilkan hasil dari augmentasi.&lt;/p&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;def AUG_test(X):
    &amp;quot;&amp;quot;&amp;quot;
    Plot gambar dengan fungsi Augmentasi
    &amp;quot;&amp;quot;&amp;quot;
    X = load_and_preprocess_image(X)
    fig, axes = plt.subplots(1, 5, figsize = (20,10))
    axes[0].imshow(X)
    axes[0].set_title(&amp;#x27;Actual&amp;#x27;, fontsize = 14)
    axes[0].axis(&amp;#x27;off&amp;#x27;)
    for i in range(1, 5):
        aug = data_augmentation(X)
        axes[i].imshow(aug)
        axes[i].set_title(&amp;#x27;Augmented&amp;#x27;, fontsize = 14)
        axes[i].axis(&amp;#x27;off&amp;#x27;)
    fig.tight_layout()
    return plt.show()
&lt;/code&gt;&lt;/pre&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;for i in random.sample(TRAIN_X, 2):
    AUG_test(i)
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;span class=&quot;gatsby-resp-image-wrapper&quot; style=&quot;position:relative;display:block;margin-left:auto;margin-right:auto;max-width:630px&quot;&gt;
      &lt;a class=&quot;gatsby-resp-image-link&quot; href=&quot;/static/6272eec8c1a27d92cf905df980bff40f/5bd27/output_36_0.png&quot; style=&quot;display:block&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;
    &lt;span class=&quot;gatsby-resp-image-background-image&quot; style=&quot;padding-bottom:21.51898734177215%;position:relative;bottom:0;left:0;background-image:url(&amp;#x27;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAECAYAAACOXx+WAAAACXBIWXMAAAsSAAALEgHS3X78AAABRklEQVQY0wXB20vCUBwA4P15FeFDNwqCXoKILtBLF+zF1yISKcMeAtGofOlFFpaWGArLhZcm1nS6Y9uZ21md1Gkz5n59H6OTr4SqamVCzFITKelyKc8qUrFSfReFbs9K8XwuoTZKglCpCgDuPQxxqlR4EWT0WQHXZT/EWlqSmm8GMQXHcVgGt0RKDAVGjg06btlLs1ME1wtg6BhUFfeWF+aoIr4CMTAoqkYjF6Fe16iDpmH4oZTYHdU22ggsqwMmMQiTScUJx94Ayj+CbmDraGdVy0ROANw+KLpO/Xvr5PkyCAADaCBE1uY99CkcABh1oa23NZ9318pch4AiAX6tb42ZHJ/gNmbG0Jl3U04+JIvx22g2EjhEzp8lm6bOx6Ln3FXQj1xngDSscIm7GH+6vyXLtSoa9ml2cdpTPN5ekcMHPiRKcu4fWZADu740UWMAAAAASUVORK5CYII=&amp;#x27;);background-size:cover;display:block&quot;&gt;&lt;/span&gt;
  &lt;img class=&quot;gatsby-resp-image-image&quot; alt=&quot;png&quot; title=&quot;png&quot; src=&quot;/static/6272eec8c1a27d92cf905df980bff40f/f058b/output_36_0.png&quot; srcSet=&quot;/static/6272eec8c1a27d92cf905df980bff40f/c26ae/output_36_0.png 158w,/static/6272eec8c1a27d92cf905df980bff40f/6bdcf/output_36_0.png 315w,/static/6272eec8c1a27d92cf905df980bff40f/f058b/output_36_0.png 630w,/static/6272eec8c1a27d92cf905df980bff40f/40601/output_36_0.png 945w,/static/6272eec8c1a27d92cf905df980bff40f/78612/output_36_0.png 1260w,/static/6272eec8c1a27d92cf905df980bff40f/5bd27/output_36_0.png 1432w&quot; sizes=&quot;(max-width: 630px) 100vw, 630px&quot; style=&quot;width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0&quot; loading=&quot;lazy&quot; decoding=&quot;async&quot;/&gt;
  &lt;/a&gt;
    &lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span class=&quot;gatsby-resp-image-wrapper&quot; style=&quot;position:relative;display:block;margin-left:auto;margin-right:auto;max-width:630px&quot;&gt;
      &lt;a class=&quot;gatsby-resp-image-link&quot; href=&quot;/static/36ff25b583392546d2970760c1d635cc/5bd27/output_36_1.png&quot; style=&quot;display:block&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;
    &lt;span class=&quot;gatsby-resp-image-background-image&quot; style=&quot;padding-bottom:21.51898734177215%;position:relative;bottom:0;left:0;background-image:url(&amp;#x27;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAECAYAAACOXx+WAAAACXBIWXMAAAsSAAALEgHS3X78AAABQklEQVQY0wXBTUsCQRgAYP9Xx6hbp+gDT2FYQVIkSCEFHroUZERIUpduQQRBB8lC0oOBpO3M7uzXzM7uzOq64mpb6MHb2/MkDNOuqpquYpXgXtCv25RVENZ0BWFiWrSmanrVsigZhF3CGXprfVZrtmUSBWGd6GbFpk5dQaqmIJVglVQSlLKYUQa+78NoFM1c7kXc4eB5AoIgmBimFTPmwG88hofHUlwuH066vgeWRUEjeiQ8b+ZyDg7nIKQfJXCnERkWBi4ccKgyfX+9D1X9G/r9HgxDGWvEiITLoYO/YGtzKTrIrMaIKDAcDIAyGj4/XU6R1oHhcAA/4zBMFM/TrcJxSuaOUmI3u44y6cVmPrch84UdUbo7a9s2bbmOLq+Ke3J7ZaG1tjzXvrk9FZO/sewGsplOzqPsflJcXJ/Ij8bL5z8cvPtEvFtgDgAAAABJRU5ErkJggg==&amp;#x27;);background-size:cover;display:block&quot;&gt;&lt;/span&gt;
  &lt;img class=&quot;gatsby-resp-image-image&quot; alt=&quot;png&quot; title=&quot;png&quot; src=&quot;/static/36ff25b583392546d2970760c1d635cc/f058b/output_36_1.png&quot; srcSet=&quot;/static/36ff25b583392546d2970760c1d635cc/c26ae/output_36_1.png 158w,/static/36ff25b583392546d2970760c1d635cc/6bdcf/output_36_1.png 315w,/static/36ff25b583392546d2970760c1d635cc/f058b/output_36_1.png 630w,/static/36ff25b583392546d2970760c1d635cc/40601/output_36_1.png 945w,/static/36ff25b583392546d2970760c1d635cc/78612/output_36_1.png 1260w,/static/36ff25b583392546d2970760c1d635cc/5bd27/output_36_1.png 1432w&quot; sizes=&quot;(max-width: 630px) 100vw, 630px&quot; style=&quot;width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0&quot; loading=&quot;lazy&quot; decoding=&quot;async&quot;/&gt;
  &lt;/a&gt;
    &lt;/span&gt;&lt;/p&gt;&lt;h3 id=&quot;up-sampling-data&quot;&gt;Up Sampling Data&lt;/h3&gt;&lt;p&gt;Mengapply Preprocess &amp;amp; Augmentasi ke data gambar dan menyimpan gambar pada direktori baru.
Menggunakan Fungsi tambahan yang sudah di download pada file &lt;code&gt;RPU.py&lt;/code&gt;&lt;/p&gt;&lt;p&gt;Fungsi akan mengembalikan List direktori gambar dan juga labelnya.&lt;/p&gt;&lt;p&gt;&lt;code&gt;ApplyAUG -&amp;gt; IMAGE_DIR, Label&lt;/code&gt;&lt;/p&gt;&lt;p&gt;Contoh Penggunaan&lt;/p&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;from RPU import ApplyAUG

TRAIN_X, TRAIN_y = (
       ApplyAUG(TRAIN_X,               # List atau Array direktori dari gambar
                TRAIN_y,               # List atau Array kelas(label) dari TRAIN_X [One Hot Encoding]
                PATH,                  # Direktori data gambar
                up_sample_ratio = 1,   # Rasio Up Sample
                up_sample_class = &amp;#x27;0&amp;#x27;, # Spesifikasi Class yang akan di Up Sample
                data_aug = data_augmentation,  # Fungsi Augmentasi
                LP = load_and_preprocess_image # Fungsi Load &amp;amp; Preprocess
                )
)
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Atau bisa juga menggunakan shell&lt;/p&gt;&lt;pre&gt;&lt;code&gt;!python RPU.py PATH SIZE TEST_SIZE UP_SAMPLES UP_SAMPLE_CLASS
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Namun pada kernel ini tidak akan di gunakan fungsi tersebut. Melaikan menggunakan hasil dari fungsi tersebut, karena kernel ini di run pada TPU &amp;amp; TPU tidak dapat membaca file diluar dari &lt;code&gt;GCS&lt;/code&gt; (lokal direktori).&lt;/p&gt;&lt;p&gt;Note : Up Sample data pada Notebook ini didapatkan dengan command berikut&lt;/p&gt;&lt;pre&gt;&lt;code&gt;!python RPU.py &amp;#x27;../input/data-bdc/Data BDC - Satria Data 2020/Data Latih/File Gambar Data Latih/&amp;#x27; \
                SIZE:512 TEST_SIZE:0.15 UP_SAMPLES:0.5-1-2 UP_SAMPLE_CLASS:0
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&quot;data-text&quot;&gt;Data Text&lt;/h2&gt;&lt;p&gt;Melakukan preprocessing pada data text.&lt;/p&gt;&lt;p&gt;Load fungsi pada file &lt;code&gt;Preprocess.py&lt;/code&gt; terlebih dahulu&lt;/p&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;from Preprocess import *
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Pada notebook ini pengolahan data text dilakukan menggunakan &lt;code&gt;judul&lt;/code&gt; dan &lt;code&gt;narasi&lt;/code&gt;, sehingga perlu dilakukan penyatuan kolom &lt;code&gt;judul&lt;/code&gt; dan kolom &lt;code&gt;narasi&lt;/code&gt; dipisahkan dengan spasi.&lt;/p&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;train[&amp;#x27;text&amp;#x27;] = (train[&amp;#x27;judul&amp;#x27;] + &amp;#x27; &amp;#x27; + train[&amp;#x27;narasi&amp;#x27;]).apply(lambda x : x.lower())
test[&amp;#x27;text&amp;#x27;] = (test[&amp;#x27;judul&amp;#x27;] + &amp;#x27; &amp;#x27; + test[&amp;#x27;narasi&amp;#x27;]).apply(lambda x : x.lower())
train.head()
&lt;/code&gt;&lt;/pre&gt;&lt;table&gt;&lt;thead&gt;&lt;tr&gt;&lt;th&gt;ID&lt;/th&gt;&lt;th&gt;label&lt;/th&gt;&lt;th&gt;tanggal&lt;/th&gt;&lt;th&gt;judul&lt;/th&gt;&lt;th&gt;narasi&lt;/th&gt;&lt;th&gt;nama file gambar&lt;/th&gt;&lt;th&gt;text&lt;/th&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;71&lt;/td&gt;&lt;td&gt;1&lt;/td&gt;&lt;td&gt;2020-08-17 00:00:00&lt;/td&gt;&lt;td&gt;Pemakaian Masker Menyebabkan Penyakit Legionna...&lt;/td&gt;&lt;td&gt;A caller to a radio talk show recently shared ...&lt;/td&gt;&lt;td&gt;71.jpg&lt;/td&gt;&lt;td&gt;pemakaian masker menyebabkan penyakit legionna...&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;461&lt;/td&gt;&lt;td&gt;1&lt;/td&gt;&lt;td&gt;2020-07-17 00:00:00&lt;/td&gt;&lt;td&gt;Instruksi Gubernur Jateng tentang penilangan ...&lt;/td&gt;&lt;td&gt;Yth.Seluruh Anggota Grup Sesuai Instruksi Gube...&lt;/td&gt;&lt;td&gt;461.png&lt;/td&gt;&lt;td&gt;instruksi gubernur jateng tentang penilangan ...&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;495&lt;/td&gt;&lt;td&gt;1&lt;/td&gt;&lt;td&gt;2020-07-13 00:00:00&lt;/td&gt;&lt;td&gt;Foto Jim Rohn: Jokowi adalah presiden terbaik ...&lt;/td&gt;&lt;td&gt;Jokowi adalah presiden terbaik dlm sejarah ban...&lt;/td&gt;&lt;td&gt;495.png&lt;/td&gt;&lt;td&gt;foto jim rohn: jokowi adalah presiden terbaik ...&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;550&lt;/td&gt;&lt;td&gt;1&lt;/td&gt;&lt;td&gt;2020-07-08 00:00:00&lt;/td&gt;&lt;td&gt;ini bukan politik, tapi kenyataan Pak Jokowi b...&lt;/td&gt;&lt;td&gt;Maaf Mas2 dan Mbak2, ini bukan politik, tapi k...&lt;/td&gt;&lt;td&gt;550.png&lt;/td&gt;&lt;td&gt;ini bukan politik, tapi kenyataan pak jokowi b...&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;681&lt;/td&gt;&lt;td&gt;1&lt;/td&gt;&lt;td&gt;2020-06-24 00:00:00&lt;/td&gt;&lt;td&gt;Foto Kadrun kalo lihat foto ini panas dingin&lt;/td&gt;&lt;td&gt;Kadrun kalo lihat foto ini panas dingin . .&lt;/td&gt;&lt;td&gt;681.jpg&lt;/td&gt;&lt;td&gt;foto kadrun kalo lihat foto ini panas dingin k...&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;h3 id=&quot;mengecek-duplicated-values&quot;&gt;Mengecek Duplicated Values&lt;/h3&gt;&lt;p&gt;Mengecek keberadaan data yang berduplikasi.&lt;/p&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;train[train.duplicated(subset=[&amp;#x27;text&amp;#x27;], keep=False)]
&lt;/code&gt;&lt;/pre&gt;&lt;table&gt;&lt;thead&gt;&lt;tr&gt;&lt;th align=&quot;center&quot;&gt;ID&lt;/th&gt;&lt;th align=&quot;center&quot;&gt;label&lt;/th&gt;&lt;th align=&quot;left&quot;&gt;tanggal&lt;/th&gt;&lt;th align=&quot;left&quot;&gt;judul&lt;/th&gt;&lt;th align=&quot;center&quot;&gt;narasi&lt;/th&gt;&lt;th align=&quot;left&quot;&gt;nama file gambar&lt;/th&gt;&lt;th&gt;text&lt;/th&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td align=&quot;center&quot;&gt;130974&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;0&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;2015-11-28 00:00:00&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;Jokowi Lebih Memilih Helikopter Buatan Luar Ne...&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;INSTING MAKELAR\n \n AKU Awalnya kaget, membac...&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;130974.png&lt;/td&gt;&lt;td&gt;jokowi lebih memilih helikopter buatan luar ne...&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;center&quot;&gt;140123&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;1&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;2020-06-02 00:00:00&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;Foto Sekarang malesiya sapu habis penduduk asi...&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;Perhatian perhatian Sekarang malesiya makin da...&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;140123.jpg&lt;/td&gt;&lt;td&gt;foto sekarang malesiya sapu habis penduduk asi...&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;center&quot;&gt;188319&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;0&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;2015-11-28 00:00:00&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;Jokowi Lebih Memilih Helikopter Buatan Luar Ne...&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;INSTING MAKELAR\n \n AKU Awalnya kaget, membac...&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;188319.png&lt;/td&gt;&lt;td&gt;jokowi lebih memilih helikopter buatan luar ne...&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;center&quot;&gt;312152&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;1&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;2020-06-02 00:00:00&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;Foto Sekarang malesiya sapu habis penduduk asi...&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;Perhatian perhatian Sekarang malesiya makin da...&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;312152.jpg&lt;/td&gt;&lt;td&gt;foto sekarang malesiya sapu habis penduduk asi...&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;center&quot;&gt;898927&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;1&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;2018-03-12 00:00:00&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;Soal Bocornya NIK dan Nomor KK Sekarang dilemp...&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;Siapa yang disalahkan ?\n Saling lempar tanggu...&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;898927.jpg&lt;/td&gt;&lt;td&gt;soal bocornya nik dan nomor kk sekarang dilemp...&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;center&quot;&gt;923503&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;1&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;2018-03-12 00:00:00&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;Soal Bocornya NIK dan Nomor KK Sekarang dilemp...&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;Siapa yang disalahkan ?\n Saling lempar tanggu...&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;923503.png&lt;/td&gt;&lt;td&gt;soal bocornya nik dan nomor kk sekarang dilemp...&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;Menghapus data yang berduplikasi&lt;/p&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;# Drop duplicates row
train = train.drop_duplicates(subset =&amp;quot;text&amp;quot;).reset_index()
&lt;/code&gt;&lt;/pre&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;def clean_up(arr):
    r&amp;quot;&amp;quot;&amp;quot;
    Cleanup \n and lowering text
    &amp;quot;&amp;quot;&amp;quot;
    for i in range(len(arr)):
        arr[i] = arr[i].lower()
        arr[i] = re.sub(&amp;#x27;\n&amp;#x27;, &amp;#x27; &amp;#x27;, arr[i])
        arr[i] = &amp;#x27; &amp;#x27;.join(arr[i].split())
    return arr
&lt;/code&gt;&lt;/pre&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;# Init text
train_text = clean_up(train.text.values)
test_text = clean_up(test.text.values)
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&quot;mengekstrack-feature-dari-text&quot;&gt;Mengekstrack Feature dari Text&lt;/h3&gt;&lt;p&gt;Mengekstrack feature seperti &lt;code&gt;URL&lt;/code&gt;, &lt;code&gt;Hashtag&lt;/code&gt;, &lt;code&gt;Tag&lt;/code&gt;, dan &lt;code&gt;Emoji&lt;/code&gt; dari text untuk melihat informasi dan frekuensi kemunculannya dalam setiap kelas pada data&lt;/p&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;%%time
FE = FeatureExtraction()
FE_test = FeatureExtraction()
FE.fit(train_text, train.label.values)
FE_test.fit(test_text)
&lt;/code&gt;&lt;/pre&gt;&lt;pre&gt;&lt;code&gt;CPU times: user 6.09 s, sys: 18 ms, total: 6.11 s
Wall time: 6.11 s
&lt;/code&gt;&lt;/pre&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;FE.get_table(&amp;#x27;urls&amp;#x27;, return_prop = True)
&lt;/code&gt;&lt;/pre&gt;&lt;table&gt;&lt;thead&gt;&lt;tr&gt;&lt;th align=&quot;left&quot;&gt;urls&lt;/th&gt;&lt;th align=&quot;center&quot;&gt;0&lt;/th&gt;&lt;th align=&quot;center&quot;&gt;1&lt;/th&gt;&lt;th align=&quot;right&quot;&gt;frekuensi&lt;/th&gt;&lt;th align=&quot;right&quot;&gt;max_prop&lt;/th&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;kompas.com&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;0.333333&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;0.666667&lt;/td&gt;&lt;td align=&quot;right&quot;&gt;3&lt;/td&gt;&lt;td align=&quot;right&quot;&gt;0.666667&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;viva.co.id&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;0.000000&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;1.000000&lt;/td&gt;&lt;td align=&quot;right&quot;&gt;2&lt;/td&gt;&lt;td align=&quot;right&quot;&gt;1.000000&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;seword.com&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;0.500000&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;0.500000&lt;/td&gt;&lt;td align=&quot;right&quot;&gt;2&lt;/td&gt;&lt;td align=&quot;right&quot;&gt;0.500000&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;opishposh.com&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;0.500000&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;0.500000&lt;/td&gt;&lt;td align=&quot;right&quot;&gt;2&lt;/td&gt;&lt;td align=&quot;right&quot;&gt;0.500000&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://t.co/jvjxc0jhmp&quot;&gt;https://t.co/jvjxc0jhmp&lt;/a&gt;&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;0.000000&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;1.000000&lt;/td&gt;&lt;td align=&quot;right&quot;&gt;1&lt;/td&gt;&lt;td align=&quot;right&quot;&gt;1.000000&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;...&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;...&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;...&lt;/td&gt;&lt;td align=&quot;right&quot;&gt;...&lt;/td&gt;&lt;td align=&quot;right&quot;&gt;...&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;http://poskotanews.com/%E2%80%A6/pa-212-batalkan-acara&quot;&gt;http://poskotanews.com/‚Ä¶/pa-212-batalkan-acara&lt;/a&gt;...&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;0.000000&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;1.000000&lt;/td&gt;&lt;td align=&quot;right&quot;&gt;1&lt;/td&gt;&lt;td align=&quot;right&quot;&gt;1.000000&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;http://www.piyunganonline.co&quot;&gt;www.piyunganonline.co&lt;/a&gt;&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;0.000000&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;1.000000&lt;/td&gt;&lt;td align=&quot;right&quot;&gt;1&lt;/td&gt;&lt;td align=&quot;right&quot;&gt;1.000000&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://goo.gl/2xz5yn&quot;&gt;https://goo.gl/2xz5yn&lt;/a&gt;&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;0.000000&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;1.000000&lt;/td&gt;&lt;td align=&quot;right&quot;&gt;1&lt;/td&gt;&lt;td align=&quot;right&quot;&gt;1.000000&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;corona-virus-map.com.exe.&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;1.000000&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;0.000000&lt;/td&gt;&lt;td align=&quot;right&quot;&gt;1&lt;/td&gt;&lt;td align=&quot;right&quot;&gt;1.000000&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://sekolahnews.com/daftar-passing-grade-u&quot;&gt;https://sekolahnews.com/daftar-passing-grade-u&lt;/a&gt;...&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;0.000000&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;1.000000&lt;/td&gt;&lt;td align=&quot;right&quot;&gt;1&lt;/td&gt;&lt;td align=&quot;right&quot;&gt;1.000000&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;pre&gt;&lt;code&gt;85 rows √ó 5 columns
&lt;/code&gt;&lt;/pre&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;FE.get_table(&amp;#x27;hashtags&amp;#x27;, return_prop = True)
&lt;/code&gt;&lt;/pre&gt;&lt;table&gt;&lt;thead&gt;&lt;tr&gt;&lt;th align=&quot;center&quot;&gt;hashtags&lt;/th&gt;&lt;th align=&quot;center&quot;&gt;0&lt;/th&gt;&lt;th align=&quot;center&quot;&gt;1&lt;/th&gt;&lt;th align=&quot;right&quot;&gt;frekuensi&lt;/th&gt;&lt;th align=&quot;right&quot;&gt;max_prop&lt;/th&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td align=&quot;center&quot;&gt;#2019gantipresiden&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;0.133333&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;0.866667&lt;/td&gt;&lt;td align=&quot;right&quot;&gt;15&lt;/td&gt;&lt;td align=&quot;right&quot;&gt;0.866667&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;center&quot;&gt;#covid19&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;0.000000&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;1.000000&lt;/td&gt;&lt;td align=&quot;right&quot;&gt;5&lt;/td&gt;&lt;td align=&quot;right&quot;&gt;1.000000&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;center&quot;&gt;#2019&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;0.000000&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;1.000000&lt;/td&gt;&lt;td align=&quot;right&quot;&gt;4&lt;/td&gt;&lt;td align=&quot;right&quot;&gt;1.000000&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;center&quot;&gt;#coronavirus&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;0.000000&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;1.000000&lt;/td&gt;&lt;td align=&quot;right&quot;&gt;3&lt;/td&gt;&lt;td align=&quot;right&quot;&gt;1.000000&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;center&quot;&gt;#stayhome&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;0.000000&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;1.000000&lt;/td&gt;&lt;td align=&quot;right&quot;&gt;3&lt;/td&gt;&lt;td align=&quot;right&quot;&gt;1.000000&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;center&quot;&gt;...&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;...&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;...&lt;/td&gt;&lt;td align=&quot;right&quot;&gt;...&lt;/td&gt;&lt;td align=&quot;right&quot;&gt;...&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;center&quot;&gt;#sekolahnews&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;0.000000&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;1.000000&lt;/td&gt;&lt;td align=&quot;right&quot;&gt;1&lt;/td&gt;&lt;td align=&quot;right&quot;&gt;1.000000&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;center&quot;&gt;#wkwkland&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;0.000000&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;1.000000&lt;/td&gt;&lt;td align=&quot;right&quot;&gt;1&lt;/td&gt;&lt;td align=&quot;right&quot;&gt;1.000000&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;center&quot;&gt;#abusendal&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;0.000000&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;1.000000&lt;/td&gt;&lt;td align=&quot;right&quot;&gt;1&lt;/td&gt;&lt;td align=&quot;right&quot;&gt;1.000000&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;center&quot;&gt;#sayapancasila&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;1.000000&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;0.000000&lt;/td&gt;&lt;td align=&quot;right&quot;&gt;1&lt;/td&gt;&lt;td align=&quot;right&quot;&gt;1.000000&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;center&quot;&gt;#sayaindonesia&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;1.000000&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;0.000000&lt;/td&gt;&lt;td align=&quot;right&quot;&gt;1&lt;/td&gt;&lt;td align=&quot;right&quot;&gt;1.000000&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;pre&gt;&lt;code&gt;356 rows √ó 5 columns
&lt;/code&gt;&lt;/pre&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;FE.get_table(&amp;#x27;tags&amp;#x27;, return_prop = True)
&lt;/code&gt;&lt;/pre&gt;&lt;table&gt;&lt;thead&gt;&lt;tr&gt;&lt;th align=&quot;center&quot;&gt;tags&lt;/th&gt;&lt;th align=&quot;center&quot;&gt;0&lt;/th&gt;&lt;th align=&quot;center&quot;&gt;1&lt;/th&gt;&lt;th align=&quot;right&quot;&gt;frekuensi&lt;/th&gt;&lt;th align=&quot;right&quot;&gt;max_prop&lt;/th&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td align=&quot;center&quot;&gt;@jokowi&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;0.111111&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;0.888889&lt;/td&gt;&lt;td align=&quot;right&quot;&gt;9&lt;/td&gt;&lt;td align=&quot;right&quot;&gt;0.888889&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;center&quot;&gt;@aniesbaswedan&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;0.000000&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;1.000000&lt;/td&gt;&lt;td align=&quot;right&quot;&gt;6&lt;/td&gt;&lt;td align=&quot;right&quot;&gt;1.000000&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;center&quot;&gt;@prabowo&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;0.166667&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;0.833333&lt;/td&gt;&lt;td align=&quot;right&quot;&gt;6&lt;/td&gt;&lt;td align=&quot;right&quot;&gt;0.833333&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;center&quot;&gt;@fadlizon&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;0.200000&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;0.800000&lt;/td&gt;&lt;td align=&quot;right&quot;&gt;5&lt;/td&gt;&lt;td align=&quot;right&quot;&gt;0.800000&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;center&quot;&gt;@netizentofa&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;0.000000&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;1.000000&lt;/td&gt;&lt;td align=&quot;right&quot;&gt;2&lt;/td&gt;&lt;td align=&quot;right&quot;&gt;1.000000&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;center&quot;&gt;...&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;...&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;...&lt;/td&gt;&lt;td align=&quot;right&quot;&gt;...&lt;/td&gt;&lt;td align=&quot;right&quot;&gt;...&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;center&quot;&gt;@kemenhub151&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;0.000000&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;1.000000&lt;/td&gt;&lt;td align=&quot;right&quot;&gt;1&lt;/td&gt;&lt;td align=&quot;right&quot;&gt;1.000000&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;center&quot;&gt;@ricky_hf&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;0.000000&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;1.000000&lt;/td&gt;&lt;td align=&quot;right&quot;&gt;1&lt;/td&gt;&lt;td align=&quot;right&quot;&gt;1.000000&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;center&quot;&gt;@kai121&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;0.000000&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;1.000000&lt;/td&gt;&lt;td align=&quot;right&quot;&gt;1&lt;/td&gt;&lt;td align=&quot;right&quot;&gt;1.000000&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;center&quot;&gt;@ankertwiter&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;0.000000&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;1.000000&lt;/td&gt;&lt;td align=&quot;right&quot;&gt;1&lt;/td&gt;&lt;td align=&quot;right&quot;&gt;1.000000&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;center&quot;&gt;@commuterline&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;0.000000&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;1.000000&lt;/td&gt;&lt;td align=&quot;right&quot;&gt;1&lt;/td&gt;&lt;td align=&quot;right&quot;&gt;1.000000&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;pre&gt;&lt;code&gt;147 rows √ó 5 columns
&lt;/code&gt;&lt;/pre&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;FE.get_table(&amp;#x27;emojis&amp;#x27;, return_prop = True)
&lt;/code&gt;&lt;/pre&gt;&lt;table&gt;&lt;thead&gt;&lt;tr&gt;&lt;th align=&quot;center&quot;&gt;emojis&lt;/th&gt;&lt;th align=&quot;center&quot;&gt;0&lt;/th&gt;&lt;th align=&quot;center&quot;&gt;1&lt;/th&gt;&lt;th align=&quot;right&quot;&gt;frekuensi&lt;/th&gt;&lt;th align=&quot;right&quot;&gt;max_prop&lt;/th&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td align=&quot;center&quot;&gt;üòÇ&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;0.076923&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;0.923077&lt;/td&gt;&lt;td align=&quot;right&quot;&gt;13&lt;/td&gt;&lt;td align=&quot;right&quot;&gt;0.923077&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;center&quot;&gt;üò≠&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;0.142857&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;0.857143&lt;/td&gt;&lt;td align=&quot;right&quot;&gt;7&lt;/td&gt;&lt;td align=&quot;right&quot;&gt;0.857143&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;center&quot;&gt;üôè&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;0.285714&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;0.714286&lt;/td&gt;&lt;td align=&quot;right&quot;&gt;7&lt;/td&gt;&lt;td align=&quot;right&quot;&gt;0.714286&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;center&quot;&gt;üò¢&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;0.166667&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;0.833333&lt;/td&gt;&lt;td align=&quot;right&quot;&gt;6&lt;/td&gt;&lt;td align=&quot;right&quot;&gt;0.833333&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;center&quot;&gt;üòç&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;0.000000&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;1.000000&lt;/td&gt;&lt;td align=&quot;right&quot;&gt;5&lt;/td&gt;&lt;td align=&quot;right&quot;&gt;1.000000&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;center&quot;&gt;...&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;...&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;...&lt;/td&gt;&lt;td align=&quot;right&quot;&gt;...&lt;/td&gt;&lt;td align=&quot;right&quot;&gt;...&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;center&quot;&gt;üá≤üá®&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;0.000000&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;1.000000&lt;/td&gt;&lt;td align=&quot;right&quot;&gt;1&lt;/td&gt;&lt;td align=&quot;right&quot;&gt;1.000000&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;center&quot;&gt;‚è¨&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;0.000000&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;1.000000&lt;/td&gt;&lt;td align=&quot;right&quot;&gt;1&lt;/td&gt;&lt;td align=&quot;right&quot;&gt;1.000000&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;center&quot;&gt;üòõ&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;0.000000&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;1.000000&lt;/td&gt;&lt;td align=&quot;right&quot;&gt;1&lt;/td&gt;&lt;td align=&quot;right&quot;&gt;1.000000&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;center&quot;&gt;üòü&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;0.000000&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;1.000000&lt;/td&gt;&lt;td align=&quot;right&quot;&gt;1&lt;/td&gt;&lt;td align=&quot;right&quot;&gt;1.000000&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;center&quot;&gt;2‚É£&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;0.000000&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;1.000000&lt;/td&gt;&lt;td align=&quot;right&quot;&gt;1&lt;/td&gt;&lt;td align=&quot;right&quot;&gt;1.000000&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;pre&gt;&lt;code&gt;71 rows √ó 5 columns
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Karena jumlah data yang cukup kecil dan sepertinya setiap feature memiliki informasi yang cukup berpengaruh terhadap kelasnya, kami memutuskan untuk tidak menghapus satupun feature yang ada.&lt;/p&gt;&lt;h3 id=&quot;masking-feature-encode&quot;&gt;Masking Feature (Encode)&lt;/h3&gt;&lt;p&gt;Masking feature pada text. Feauture yang sudah di &lt;code&gt;.fit&lt;/code&gt; pada data sebelumnya, proses ini dilakukan karena pada feature ini terdapat peletakan tanda baca yang random / tidak beraturan, sedangkan pada proses berikutnya akan dilakukan penormalisasian tanda baca sehingga akan mengganggu proses tersebut. Maka dari itu perlu dilakukan masking agar proses normalisasi kalimat berjalan dengan lancar.&lt;/p&gt;&lt;pre&gt;&lt;code&gt;# Contoh
Website Google adalah http://google.com/
Trending #MosiTidakPercaya
@jokowi adalah presiden RI
Lucu üòÇ

# Encode
Website Google adalah MASKURLS1MASK
Trending MASKHASHTAGS1MASK
MASKTAGS1MASK adalah presiden RI
Lucu MASKEMOJIS1MASK
&lt;/code&gt;&lt;/pre&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;# Build Mask Code
FE.build_mask_code(0)
FE_test.build_mask_code(0)
# Apply to data
train_text = FE.encode(train_text)    # Train
test_text = FE_test.encode(test_text) # Test
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&quot;normalisasi-tanda-baca&quot;&gt;Normalisasi Tanda Baca&lt;/h3&gt;&lt;p&gt;Melakukan normalisasi tanda baca pada text. Karena banyak peletakan tanda baca yang salah pada text yang akan mengganggu &lt;code&gt;tokenizer&lt;/code&gt; dalam melakukan segmentasi pada kalimat.&lt;/p&gt;&lt;pre&gt;&lt;code&gt;# Contoh
Budi membayar2.000 ban yang dibelinya senilai rp.2.000.000

# Preprocessed
Budi membayar 2.000 ban yang dibelinya senilai rp. 2.000.000
&lt;/code&gt;&lt;/pre&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;# Add more string punctuation
string.punctuation += &amp;#x27;‚Äò‚Äô‚Ä¶‚Äú‚Äù‚Äì&amp;#x27;
&lt;/code&gt;&lt;/pre&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;def normalize(array):
    &amp;quot;&amp;quot;&amp;quot;
    Normalize text
    &amp;quot;&amp;quot;&amp;quot;
    punc, arr = string.punctuation, array.copy()
    for i in range(len(arr)):
        temp = list(arr[i])
        for j in range(1, len(temp) - 1):
            if (temp[j] in punc) and not\
            all([x in string.digits for x in [temp[j-1], temp[j+1]]]):
                temp[j] = &amp;#x27; &amp;#x27; + temp[j] + &amp;#x27; &amp;#x27;
            elif (temp[j] in string.ascii_lowercase) and (temp[j + 1] \
            in string.digits or temp[j + 1] in string.punctuation):
                temp[j] += &amp;#x27; &amp;#x27;
        arr[i] = &amp;#x27;&amp;#x27;.join(temp)
        arr[i] = &amp;#x27; &amp;#x27;.join(arr[i].split())
    return arr
&lt;/code&gt;&lt;/pre&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;# Apply to text
train_text = normalize(train_text) # Train
test_text = normalize(test_text)   # Test
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&quot;decode--menghapus-tanda-baca&quot;&gt;Decode &amp;amp; Menghapus Tanda Baca&lt;/h3&gt;&lt;p&gt;Men-Decode (mengembalikan feature) serta menghapus tanda baca yang ada pada text.&lt;/p&gt;&lt;pre&gt;&lt;code&gt;# Contoh
Website Google adalah MASKURLS1MASK

# Decode
Website Google adalah http://google.com/

# Remove Punctuation
Website Google adalah httpgooglecom
&lt;/code&gt;&lt;/pre&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;def remove_punc(arr, punc_):
    &amp;quot;&amp;quot;&amp;quot;
    Remove string punctuation
    &amp;quot;&amp;quot;&amp;quot;
    return asarray([x.translate(str.maketrans(&amp;#x27;&amp;#x27;, &amp;#x27;&amp;#x27;, punc_))
                    for x in arr])
&lt;/code&gt;&lt;/pre&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;# Apply to data
train_text = FE.decode(train_text)    # Train
train_text = remove_punc(train_text, string.punctuation)
test_text = FE_test.decode(test_text) # Test
test_text = remove_punc(test_text, string.punctuation)
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&quot;de-emojized&quot;&gt;De-Emojized&lt;/h3&gt;&lt;p&gt;Mengubah kode emoji yang ada pada text menjadi kata kata yang dapat dimengerti.&lt;/p&gt;&lt;pre&gt;&lt;code&gt;üôè -&amp;gt; folded hands
üòÉ -&amp;gt; grinning face with big eyes
&lt;/code&gt;&lt;/pre&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;def deemojized(arr):
    &amp;quot;&amp;quot;&amp;quot;
    De Emojized text
    &amp;quot;&amp;quot;&amp;quot;
    for i in range(len(arr)):
        arr[i] = emoji.demojize(arr[i])
        arr[i] = re.sub(&amp;#x27;:&amp;#x27;, &amp;#x27; &amp;#x27;, arr[i])
        arr[i] = re.sub(&amp;#x27;_&amp;#x27;, &amp;#x27; &amp;#x27;, arr[i])
        arr[i] = &amp;#x27; &amp;#x27;.join(arr[i].split())
    return arr
&lt;/code&gt;&lt;/pre&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;# Apply to data
train_text = deemojized(train_text) # Train
test_text = deemojized(test_text)   # Test
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&quot;mengecek-kata-yang-misspell-typo&quot;&gt;Mengecek Kata yang Misspell (typo)&lt;/h3&gt;&lt;p&gt;Mengecek kata-kata yang misspell atau typo serta kata - kata singkatan.&lt;/p&gt;&lt;ol&gt;&lt;li&gt;Membangun Vocabulary&lt;br/&gt;
Membangun Vocabulary dari data untuk di cek secara &lt;strong&gt;Manual&lt;/strong&gt;&lt;/li&gt;&lt;/ol&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;# Count Vectorizer
count = CountVectorizer()
count.fit(train_text)
&lt;/code&gt;&lt;/pre&gt;&lt;pre&gt;&lt;code&gt;CountVectorizer()
&lt;/code&gt;&lt;/pre&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;# Vocabulary to DataFrame
vocab = pd.DataFrame({&amp;#x27;Vocab&amp;#x27; : list(count.vocabulary_.keys()),
                     &amp;#x27;Word Index&amp;#x27; : list(count.vocabulary_.values())})
vocab = vocab.sort_values(by=[&amp;#x27;Word Index&amp;#x27;], ascending = False)
vocab[:30].style.hide_index()
&lt;/code&gt;&lt;/pre&gt;&lt;table&gt;&lt;thead&gt;&lt;tr&gt;&lt;th align=&quot;center&quot;&gt;Vocab&lt;/th&gt;&lt;th align=&quot;center&quot;&gt;Word Index&lt;/th&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td align=&quot;center&quot;&gt;ÔæèÔæª&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;20000&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;center&quot;&gt;Âä†Ê≤π jiayou&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;19999&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;center&quot;&gt;‡¶π‡ßü&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;19998&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;center&quot;&gt;‡¶∏‡¶≤&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;19997&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;center&quot;&gt;‡¶∏‡¶¨&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;19996&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;center&quot;&gt;‡¶®‡¶Æ&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;19995&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;center&quot;&gt;‡¶®‡¶¨&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;19994&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;center&quot;&gt;‡¶ß‡¶®&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;19993&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;center&quot;&gt;‡¶ö‡¶≤‡¶õ&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;19992&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;center&quot;&gt;‡¶ï‡¶ñ&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;19991&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;center&quot;&gt;‡¶ì‡¶Ü‡¶á‡¶∏&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;19990&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;center&quot;&gt;‡¶Ü‡¶∞&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;19989&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;center&quot;&gt;‡¶Ö‡¶®&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;19988&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;center&quot;&gt;ŸàÿßŸÜÿß&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;19987&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;center&quot;&gt;ŸÑŸÑŸá&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;19986&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;center&quot;&gt;ŸÑÿ≥&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;19985&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;center&quot;&gt;ŸÑÿß&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;19984&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;center&quot;&gt;ÿ±ÿßÿ¨ÿπŸàŸÜ&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;19983&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;center&quot;&gt;ÿßŸÜÿß&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;19982&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;center&quot;&gt;ÿßŸÑŸäŸá&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;19981&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;center&quot;&gt;ÿßŸÑŸÑŸá&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;19980&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;center&quot;&gt;ÿßŸÑŸÑ&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;19979&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;center&quot;&gt;ÿßŸÑÿ≥&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;19978&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;center&quot;&gt;ÿßÿ™&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;19977&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;center&quot;&gt;≈•ŒØŒ¥Œ±ƒ∏&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;19976&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;center&quot;&gt;√¨n√¨&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;19975&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;center&quot;&gt;√°t√°u&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;19974&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;center&quot;&gt;√°d√°&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;19973&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;center&quot;&gt;zurina&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;19972&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;center&quot;&gt;zumi&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;19971&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;Dapat dilihat dari tabel diatas bahwa data terdiri dari beberapa bahasa karena terdapat beberapa kata yang bukan berasal dari bahasa Indonesia seperti &lt;code&gt;ÔæüÔæèÔæª&lt;/code&gt;, &lt;code&gt;Âä†Ê≤πjiayou&lt;/code&gt;, &lt;code&gt;‡¶π‡ßü&lt;/code&gt;, &lt;code&gt;‡¶∏‡¶≤&lt;/code&gt;, dll.&lt;/p&gt;&lt;ol start=&quot;2&quot;&gt;&lt;li&gt;Mengeksport Vocabulary &lt;br/&gt;
Mengeksport vocabulary telah dibangun kedalam file &lt;code&gt;.txt&lt;/code&gt; untuk dapat dilakukan pengecekan secara manual terhadap kata-kata tersebut.&lt;/li&gt;&lt;/ol&gt;&lt;pre&gt;&lt;code&gt;f = open(&amp;quot;vocab.txt&amp;quot;, &amp;quot;w&amp;quot;)
f.write(&amp;quot; \n&amp;quot;.join(sorted(vocab.Vocab.values)))
f.close()
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Selanjutnya, mengimport file yang berisi kata - kata yang telah di cek secara manual lalu mengapikasikannya pada data text menggunakan kelas &lt;code&gt;SpellChecker&lt;/code&gt;, fitting didapat dilakukan pada file yang diinginkan atau dapat menggunakan file yang telah di benarkan oleh tim Catatan Cakrawala dengan mengisikan &lt;code&gt;cc-hand-fixed&lt;/code&gt; pada saat fitting class.&lt;/p&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;spellchecker = SpellChecker()
spellchecker.fit(&amp;#x27;cc-hand-fixed&amp;#x27;)
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;code&gt;Transform&lt;/code&gt; untuk apply pada data.&lt;/p&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;# Apply to Data
train_text = spellchecker.transform(train_text) # Train
test_text = spellchecker.transform(test_text)   # Test
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&quot;memhapus-kata-yang-kurang-penting--stopwords&quot;&gt;Memhapus Kata yang Kurang Penting &amp;amp; Stopwords&lt;/h3&gt;&lt;p&gt;Menghapus kata - kata yang kurang penting seperti kata yang hanya terdiri dari 1 karakter dan Stopwords. Untuk Stopwords kami menggabungkan stopwords bahasa Indonesia dari &lt;code&gt;Spacy&lt;/code&gt; dan &lt;code&gt;Sastrawi&lt;/code&gt; serta bahasa Inggris dari &lt;code&gt;Spacy&lt;/code&gt;.&lt;/p&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;from spacy.lang.id.stop_words import STOP_WORDS as ID
from spacy.lang.en.stop_words import STOP_WORDS as EN
from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory

stopwords = set(StopWordRemoverFactory().get_stop_words())
stopwords.update(EN)
stopwords.update(ID)
&lt;/code&gt;&lt;/pre&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;def RUnecesarry(array, stopwords):
    &amp;quot;&amp;quot;&amp;quot;
    Remove Unnecessary words
    &amp;quot;&amp;quot;&amp;quot;
    arr = array.copy()
    for i in range(len(arr)):
        temp = arr[i].split()
        temp = [x for x in temp if not (len(x) == 1 and \
                (x in string.ascii_lowercase or x in string.digits))]
        temp = [x for x in temp if x not in stopwords]
        arr[i] = &amp;#x27; &amp;#x27;.join(temp).lower()
    return arr
&lt;/code&gt;&lt;/pre&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;# Apply to data
train_text = RUnecesarry(train_text, stopwords) # Train
test_text = RUnecesarry(test_text, stopwords)   # Test
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Melihat hasil dari tahap preprocess data text&lt;/p&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;for i in [0, 803, 1002]:
    print(f&amp;quot;Actual : \n{train.text.values[i]}\n\nPreprocessed : \n{train_text[i]}&amp;quot;)
    print(&amp;#x27;&amp;#x27;.rjust(80, &amp;#x27;-&amp;#x27;))
&lt;/code&gt;&lt;/pre&gt;&lt;pre&gt;&lt;code&gt;Actual :
pemakaian masker menyebabkan penyakit legionnaires a caller to a radio talk show recently shared that his wife was hospitalized n told she had covid n only a couple of days left to live . a doctor friend suggested she be tested for legionnaires disease because she wore the same mask every day all day long . turns out it was legionnaires disease from the moisture n bacteria in her mask . she was given antibiotics n within two days was better . what if these ‚Äòspikes‚Äô in covid are really something else due to ‚Äòmask induced infections‚Äô .??ü§îü§îü§î

Preprocessed :
pemakaian masker menyebabkan penyakit legionnaires caller radio talk recently shared wife hospitalized told covid couple days left live doctor friend suggested tested legionnaires disease wore mask day day long turns legionnaires disease moisture bacteria mask given antibiotics days better spikes covid mask induced infections thinking face thinking face thinking face
--------------------------------------------------------------------------------
Actual :
kk dan nik anda bisa dipakai orang lain, waspadalah! bingung kan? speechless lah. . https://www.jpnn.com/kemenpan/news/kk-dan-nik-anda-bisa-dipakai-orang-lain-waspadalah?page=1

Preprocessed :
kk nik dipakai orang waspadalah bingung speechless httpswwwjpnncomkemenpannewskkdannikandabisadipakaioranglainwaspadalahpage1
--------------------------------------------------------------------------------
Actual :
ini gunung dieng salatiga lihat baik2..ini bukan gn.fuji.di jepang‚Ä¶bukan juga pegunungan alpen di eropa‚Ä¶tapi ini gn.dieng salatiga‚Ä¶indonesia..suhu -9¬∞c.

Preprocessed :
gunung dieng salatiga lihat gn fuji jepang pegunungan alpen eropa gn dieng salatiga indonesia suhu 9¬∞c
--------------------------------------------------------------------------------
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&quot;explorasi-data&quot;&gt;Explorasi Data&lt;/h2&gt;&lt;p&gt;Melakukan explorasi pada data untuk mendapatkan informasi - informasi lebih jauh dari data.&lt;/p&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;train.info()
&lt;/code&gt;&lt;/pre&gt;&lt;pre&gt;&lt;code&gt;&amp;lt;class &amp;#x27;pandas.core.frame.DataFrame&amp;#x27;&amp;gt;
RangeIndex: 4228 entries, 0 to 4227
Data columns (total 8 columns):
 #   Column            Non-Null Count  Dtype
---  ------            --------------  -----
 0   index             4228 non-null   int64
 1   ID                4228 non-null   int64
 2   label             4228 non-null   int64
 3   tanggal           4228 non-null   object
 4   judul             4228 non-null   object
 5   narasi            4228 non-null   object
 6   nama file gambar  4228 non-null   object
 7   text              4228 non-null   object
dtypes: int64(3), object(5)
memory usage: 264.4+ KB
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Merubah tipe pada kolom tanggal menjadi &lt;code&gt;datetime&lt;/code&gt;. Kolom tanggal tidak dapat langsung dirubah tipe datanya menjadi &lt;code&gt;datetime&lt;/code&gt; dikarenakan ada nya beberapa element yang mengandung kata &lt;code&gt;Okt&lt;/code&gt; dan &lt;code&gt;Agu&lt;/code&gt; sehingga harus diubah menjadi &lt;code&gt;Oct&lt;/code&gt; dan &lt;code&gt;Aug&lt;/code&gt;&lt;/p&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;def FixMonth(text):
    &amp;quot;&amp;quot;&amp;quot;
    Fungsi untuk merubah kata &amp;#x27;Okt&amp;#x27; menjadi &amp;#x27;Oct&amp;#x27; dan &amp;#x27;Agu&amp;#x27; menjadi &amp;#x27;Aug&amp;#x27;
    &amp;quot;&amp;quot;&amp;quot;
    if type(text) == str:
        if &amp;#x27;Okt&amp;#x27; in text: return text[:3]+&amp;#x27;Oct&amp;#x27;+text[6:]
        else: return text[:3]+&amp;#x27;Aug&amp;#x27;+text[6:]
    else:
        return text
&lt;/code&gt;&lt;/pre&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;# Apply to data
train[&amp;#x27;tanggal&amp;#x27;] = train[&amp;#x27;tanggal&amp;#x27;].apply(FixMonth)
train[&amp;#x27;tanggal&amp;#x27;] = pd.to_datetime(train[&amp;#x27;tanggal&amp;#x27;])
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&quot;mengecek-frekuensi-setiap-kelas&quot;&gt;Mengecek Frekuensi Setiap Kelas&lt;/h3&gt;&lt;p&gt;Menegecek frekuensi tiap kelasnya pada data.&lt;/p&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;fig_1 = go.Figure(go.Pie(labels=[&amp;#x27;Bukan Hoax&amp;#x27;, &amp;#x27;Hoax&amp;#x27;],
                       values=train.groupby(&amp;#x27;label&amp;#x27;).size().values,
                       textinfo=&amp;#x27;percent+label+value&amp;#x27;,
                       textfont_color=&amp;#x27;#ffffff&amp;#x27;,
                       marker_colors=[&amp;#x27;#0db7c5&amp;#x27;,&amp;#x27;#d03850&amp;#x27;])
               )
fig_1.update_layout(
    title={
        &amp;#x27;text&amp;#x27;: &amp;quot;Frekuensi Perkelasnya&amp;quot;,
        &amp;#x27;y&amp;#x27;:0.93,
        &amp;#x27;x&amp;#x27;:0.5,
        &amp;#x27;font_size&amp;#x27;:23},
    width=550, height=550
)

fig_1.show()
&lt;/code&gt;&lt;/pre&gt;&lt;div style=&quot;margin-bottom:32px&quot;&gt;&lt;canvas style=&quot;min-height:400px&quot; height=&quot;150&quot; width=&quot;300&quot; data-testid=&quot;canvas&quot; role=&quot;img&quot;&gt;&lt;/canvas&gt;&lt;/div&gt;&lt;p&gt;Dapat dilihat bahwa kelas 1 (&lt;code&gt;Hoax&lt;/code&gt;) jauh lebih banyak dari kelas 0 (&lt;code&gt;Bukan Hoax&lt;/code&gt;) atau terdapat &lt;code&gt;imbalance class&lt;/code&gt; pada data. Untuk penanganannya kami menggunakan &lt;code&gt;Stratify&lt;/code&gt; saat melakukan &lt;code&gt;Cross Validation&lt;/code&gt; sehingga proporsi pada subset &lt;code&gt;latih&lt;/code&gt; dan pada subset &lt;code&gt;validasi&lt;/code&gt; sama.&lt;/p&gt;&lt;h3 id=&quot;melihat-sebaran-kelas-per-waktunya&quot;&gt;Melihat Sebaran Kelas per Waktunya&lt;/h3&gt;&lt;p&gt;Melihat sebaran kelas perwaktunya.&lt;/p&gt;&lt;ol&gt;&lt;li&gt;Membuat kolom baru untuk menyimpan tahun &amp;amp; bulan&lt;/li&gt;&lt;li&gt;Mengelompokkan berdasarkan tahun, bulan, dan labelnya&lt;/li&gt;&lt;/ol&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;# Membuat Kolom baru untuk menyimpan nilai tahun dan bulan
train[&amp;#x27;year&amp;#x27;] = [i.year for i in train.tanggal]
train[&amp;#x27;month&amp;#x27;] = [i.month for i in train.tanggal]
# Grouping
data_waktu = (train.groupby([&amp;#x27;year&amp;#x27;, &amp;#x27;month&amp;#x27;, &amp;#x27;label&amp;#x27;]).size().reset_index()
              .pivot_table(columns=&amp;#x27;label&amp;#x27;, index=[&amp;#x27;year&amp;#x27;,&amp;#x27;month&amp;#x27;], values=0))
&lt;/code&gt;&lt;/pre&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;fig_2 = go.Figure(data=[
    go.Bar(name=&amp;#x27;Bukan Hoax&amp;#x27;,
           x=[str(i) for i in data_waktu.index],
           y=data_waktu[0].values,
           marker_color=&amp;#x27;#0db7c5&amp;#x27;),
    go.Bar(name=&amp;#x27;Hoax&amp;#x27;,
           x=[str(i) for i in data_waktu.index],
           y=data_waktu[1].values,
           marker_color=&amp;#x27;#d03850&amp;#x27;)
])

fig_2.update_layout(barmode=&amp;#x27;stack&amp;#x27;,
                  xaxis=dict(dtick=1,
                             showgrid=False,
                             title=&amp;#x27;(Tahun, Bulan)&amp;#x27;),
                  legend=dict(x=0.006,y=0.97,
                              bgcolor=&amp;#x27;rgba(255,255,255,0)&amp;#x27;,
                              bordercolor=&amp;#x27;rgba(0,0,0,0)&amp;#x27;,
                              font_color=&amp;#x27;#090919&amp;#x27;,
                              font_size=14),
                  title=dict(text=&amp;#x27;Sebaran Hoax per Waktu&amp;#x27;,
                             x=0.5,
                             y=0.9,
                             font_size=23),
                  width=1000,height=550
                 )

fig_2.show()
&lt;/code&gt;&lt;/pre&gt;&lt;div style=&quot;margin-bottom:32px&quot;&gt;&lt;canvas style=&quot;min-height:400px&quot; height=&quot;150&quot; width=&quot;300&quot; data-testid=&quot;canvas&quot; role=&quot;img&quot;&gt;&lt;/canvas&gt;&lt;/div&gt;&lt;p&gt;Ditinjau dari grafik diatas bahwa:&lt;/p&gt;&lt;ol&gt;&lt;li&gt;53% dari keseluruhan data berasal dari tahun 2019 - 2020&lt;/li&gt;&lt;li&gt;95% data pada tahun 2019 - 2020 adalah Hoax&lt;/li&gt;&lt;/ol&gt;&lt;h3 id=&quot;mengecek-kata---kata-yang-muncul-perkelasnya&quot;&gt;Mengecek Kata - Kata yang Muncul Perkelasnya&lt;/h3&gt;&lt;p&gt;Mengecek frekuensi kata - kata yang muncul berdasarkan kelasnya.&lt;/p&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;def find_words(texts, label, builder = CountVectorizer(min_df=3, max_df=0.9,
                                                       ngram_range=(1,2))):
    &amp;quot;&amp;quot;&amp;quot;
    Memeriksa kalimat berdasarkan kelasnya untuk membuat vocab
    &amp;quot;&amp;quot;&amp;quot;
    builder.fit(texts)
    n_class, res = len(set(label)), {}
    for i in tqdm(range(len(texts))):
        for vocab in texts[i].split():
            if vocab in builder.vocabulary_:
                if vocab not in res:
                    res[vocab] = [0] * n_class + [0]
                res[vocab][label[i]] += 1
                res[vocab][-1] += 1
    df = pd.DataFrame({&amp;#x27;kata&amp;#x27; : list(res.keys())})
    for i in range(n_class):
        df[f&amp;#x27;Kelas_{i}&amp;#x27;] = [res[x][i] for x in res]
    df[&amp;#x27;Frekuensi&amp;#x27;] = [res[x][-1] for x in res]
    return df.sort_values(by = [&amp;#x27;Frekuensi&amp;#x27;], ascending = False).reset_index(drop=True)
&lt;/code&gt;&lt;/pre&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;# Vocab DF per class
words = find_words(train_text, train.label.values)
words[&amp;#x27;Max_prop&amp;#x27;] = words[[&amp;#x27;Kelas_0&amp;#x27;, &amp;#x27;Kelas_1&amp;#x27;]].max(axis = 1) / words[&amp;#x27;Frekuensi&amp;#x27;]
words.head(10)
&lt;/code&gt;&lt;/pre&gt;&lt;table&gt;&lt;thead&gt;&lt;tr&gt;&lt;th align=&quot;center&quot;&gt;kata&lt;/th&gt;&lt;th align=&quot;center&quot;&gt;Kelas_0&lt;/th&gt;&lt;th align=&quot;center&quot;&gt;Kelas_1&lt;/th&gt;&lt;th align=&quot;center&quot;&gt;Frekuensi&lt;/th&gt;&lt;th align=&quot;center&quot;&gt;Max_prop&lt;/th&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td align=&quot;center&quot;&gt;indonesia&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;160&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;617&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;777&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;0.794080&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;center&quot;&gt;jokowi&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;93&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;522&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;615&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;0.848780&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;center&quot;&gt;foto&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;79&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;446&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;525&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;0.849524&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;center&quot;&gt;orang&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;66&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;458&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;524&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;0.874046&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;center&quot;&gt;video&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;62&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;378&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;440&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;0.859091&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;center&quot;&gt;corona&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;15&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;402&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;417&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;0.964029&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;center&quot;&gt;anak&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;62&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;342&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;404&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;0.846535&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;center&quot;&gt;virus&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;21&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;378&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;399&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;0.947368&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;center&quot;&gt;covid&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;24&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;342&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;366&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;0.934426&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;center&quot;&gt;china&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;33&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;324&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;357&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;0.907563&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;Berikut adalah Barplot untuk kata - kata pada kelas &lt;code&gt;1&lt;/code&gt; yang mempunyai frekuensi kumunculan lebih dari kelas &lt;code&gt;0&lt;/code&gt;.&lt;/p&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;_1 = words[(words[&amp;#x27;Kelas_1&amp;#x27;] &amp;gt; words[&amp;#x27;Kelas_0&amp;#x27;])]

plt.figure(figsize = (12,7))
sns.barplot(x = &amp;#x27;kata&amp;#x27;, y = &amp;#x27;Frekuensi&amp;#x27;, data = _1[:25])
plt.title(&amp;#x27;Frekuensi Kata yang Sering Muncul Pada Label Hoax&amp;#x27;, fontsize = 14)
plt.xticks(rotation = 90)
plt.show()
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;span class=&quot;gatsby-resp-image-wrapper&quot; style=&quot;position:relative;display:block;margin-left:auto;margin-right:auto;max-width:630px&quot;&gt;
      &lt;a class=&quot;gatsby-resp-image-link&quot; href=&quot;/static/96cb2de7a1c7de3184a2aa71d9760734/c54b3/output_96_0.png&quot; style=&quot;display:block&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;
    &lt;span class=&quot;gatsby-resp-image-background-image&quot; style=&quot;padding-bottom:67.72151898734178%;position:relative;bottom:0;left:0;background-image:url(&amp;#x27;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAOCAYAAAAvxDzwAAAACXBIWXMAAAsSAAALEgHS3X78AAADAklEQVQ4y51R32tTZxj+ekpVGPsnduHV7hSUMRjbxXa9ChPBi+qaNW1Cf220DLQaexS05GrYdKY1Zkv6I+akUddf8ZwkjRQrQrWlNLVqbJesWZv0hCRqbL5zvmd8aSLtusHYAw/v+37vy8Pzvh+ZnZ09nMlkTqmqWpvNZmsTicRJWZYbw+FwQygUMvKoKEojzyORyHc88pozFoudyuVytaqqnshkMifj8fhR8swttRTi62AAdE0DYzz7b+Czuq6XyPNsNusi0R6bqfAqzvvvmK7pXJftgMdKvZuVXslDmUUukM/nb5Jor9OcuS+j/MiYrrH/AcoFcrmcgzyzD5lTv/6C7Hyg+GZzpbILV4ZeInt/hn87B2NMKzt0kOUbQ+b00CAS7o7iS7kHr/Mb0LTi/luVuVd8p94j+Mrhbdx0u5CUuosLUgum77ZgSj6HyIITz9Xn+D2X2ueoIsz099T4r5ZXHjanBl1Y91qKi6PfIzjaBI9Uh97xb9EZ7IBBvo6BhRk83Ujij1y+JPYP2HFYeOMgK/0jppS7ItjOQn4T891pYP0BM7sQ6WJGxcYapoaYYeIOa5MfsKsPl9ndRZWtrL1lmT+32dtkkW0nCxpNv4YaS5RuaOIOk15LYXG0nQb9Jir5DdQ+1US7ps9To9JHWxUvbVMmaHNgmjZOPqFX5ASVAmk6I6VobDBNU8617a2ROaz2/TZAVvqH27nDpNeCxdF2BP0m+PwG2Kea0DV9HkbFhmb5NprlcbQEImiV52ENr+NeSMWje1tY9Wwh7VqD6nmC1Z/HhsmMte/4ku269an9h0vhm0ZRunVGHHCcFq3DdWKbr/Vyna/nUv1tu1jvcYv1I37R4Al2X/TNdzul5e5xZ1R81LckRnvnLNEb96/NWQe/JoSQagBkPzdK8SdE+Yywu0cIOfD3+TKqyFdHjtVMWi7WAI8FFF1C9OGP1YBN2IBDGMeYoADC4S8+/eCjT45/CKDqy07xwGffGA+9SKMaKqqxCQEUwtWznQc///joob8AhQl8YbJ5REIAAAAASUVORK5CYII=&amp;#x27;);background-size:cover;display:block&quot;&gt;&lt;/span&gt;
  &lt;img class=&quot;gatsby-resp-image-image&quot; alt=&quot;png&quot; title=&quot;png&quot; src=&quot;/static/96cb2de7a1c7de3184a2aa71d9760734/f058b/output_96_0.png&quot; srcSet=&quot;/static/96cb2de7a1c7de3184a2aa71d9760734/c26ae/output_96_0.png 158w,/static/96cb2de7a1c7de3184a2aa71d9760734/6bdcf/output_96_0.png 315w,/static/96cb2de7a1c7de3184a2aa71d9760734/f058b/output_96_0.png 630w,/static/96cb2de7a1c7de3184a2aa71d9760734/c54b3/output_96_0.png 727w&quot; sizes=&quot;(max-width: 630px) 100vw, 630px&quot; style=&quot;width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0&quot; loading=&quot;lazy&quot; decoding=&quot;async&quot;/&gt;
  &lt;/a&gt;
    &lt;/span&gt;&lt;/p&gt;&lt;p&gt;Plotting WordCloud&lt;/p&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;from wordcloud import WordCloud, ImageColorGenerator

__1 = {}
for i in range(len(_1)):
    __1[_1.kata.values[i]] = _1.Max_prop.values[i]

wordcloud = WordCloud(width = 4000, height = 3000, min_font_size = 5,
                      background_color = &amp;#x27;white&amp;#x27;, colormap = &amp;#x27;inferno&amp;#x27;).fit_words(__1)
plt.figure(figsize = (10, 8))
plt.imshow(wordcloud)
plt.axis(&amp;quot;off&amp;quot;)
plt.tight_layout(pad = 0)
plt.show()
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;span class=&quot;gatsby-resp-image-wrapper&quot; style=&quot;position:relative;display:block;margin-left:auto;margin-right:auto;max-width:630px&quot;&gt;
      &lt;a class=&quot;gatsby-resp-image-link&quot; href=&quot;/static/1876f08eb72ea6309e3d379ab9a70269/c6d67/output_98_0.png&quot; style=&quot;display:block&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;
    &lt;span class=&quot;gatsby-resp-image-background-image&quot; style=&quot;padding-bottom:75.31645569620254%;position:relative;bottom:0;left:0;background-image:url(&amp;#x27;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAPCAYAAADkmO9VAAAACXBIWXMAAAsSAAALEgHS3X78AAAEBklEQVQ4ywXBWXPaVgAGUP3KzvSxT50+tU91J9OmaZPMtI6TNPGkE+I4NN4AF1wwiy1ksECAJQESshBCZhGLwBAM9jheWO7N13OYsTOIZPbSNu/LmrOLrjW/PLe494J15M5YN6OuZSumFViOW6mNtNWuNq1Bp2NxrwMWHxAtet+3ru0Di1woFeLwXQwSbqYi1qzEThLvH/ighDMweQW7yxEEV4JQIgIGZQ3p7RjcS9vgIzIONjOI/hXDzuN9NKQC7vpFOJaKrplBrybzTMvon04GY/hfJRdqKEn3l320oWh0qEp0fSlIJ/UW/XJ7Tn1PQzQZytP1h356ZRn0aJOn3IZI83mNfvzgm+dEFVa9xTK3Q0cvhGX4VyIU6MPzyAtb0/FJL8L1vQfk5hK85wTRtSSGFz24ljy4qhngvRnsv2FxOerAOGZJM5tE3yxzTKOg6U+/coFfP6Z9WUJuL42954cIv4ggsy2ixJl48vU61KMCrp1TcG4e0VeH2HoUxMFWFo1KB3xAIulgARKnc4wWl/WQi0PkHUurJwqmTgvRdwl8XN7HqFGFGY7jaNUL9vkOHFHGjV1F9E0UYoDFqNeCIRSgJvJES5moimccc+ZL6BNNxWLSoROjjEYwgWZWg1EsoxpPYdFRcatmcNe1YEtlOJqJi04dXV2BrdkYVQyM9RzplUo4N7ocU/XxenVXgPinn5pbUVS2w7DZPGrBAk7dPGwuA8ufRiNn4IQ1YEh1aBkL6YiGZEBCNqog7skSMVZEp9TimGq0oFdCEjq5Cm3yRYwrLVyWZEwMBZ9yKcwdGROlhPS/ObRkDU3ZQDlfR11vQOJO0TdbGKsGua/XMb264Zi4X9I3X0QR+YenDdVGNiyjcBjH1dBGLS/hyPsfDFmF520cbVVBgRUwbA/BBwXkORlkPIAaFUncnUS70uWY1w99+srSFlZ/8dJULI/IloAH36whvV+A67Ef3tdxlFUD6y8DePnjJsriGWIbKTz7wY3fv3Xh5EDEyk9bRC/qGDYcjnn/5KPufuaB580xXVv2wuti8fbpLkJuHkO7j7U/gtjdOMDaSgC/fbeOsljHzl8xrP7sBb8noZTUsPrrDgEc0M8Djgl8CGsx7yGywdRcSYnE5wqS3b+jxMxqJBlIk7XHXnJ8kCbC4QlpaBZhfcdkYDaI91mAhFwcaao2CX5IzO5nbXyZX7PMePS5MR2fAbcCMM0BMw2YmpiNFUzHeZCrFObXJ5hfK7gbi/hynQVGeaDHYt5NYuZkQc953HbCIGNFYOh8ukGmPWmxcNKLuSPMFy3hvlcS7mq6MOuUhUm9ItxfnQqTdklwtDNhVK0Kn526cKZJgnVaFAbtqjDt8KnzWqnQa9Ze/g+nmNnkvonmxQAAAABJRU5ErkJggg==&amp;#x27;);background-size:cover;display:block&quot;&gt;&lt;/span&gt;
  &lt;img class=&quot;gatsby-resp-image-image&quot; alt=&quot;png&quot; title=&quot;png&quot; src=&quot;/static/1876f08eb72ea6309e3d379ab9a70269/f058b/output_98_0.png&quot; srcSet=&quot;/static/1876f08eb72ea6309e3d379ab9a70269/c26ae/output_98_0.png 158w,/static/1876f08eb72ea6309e3d379ab9a70269/6bdcf/output_98_0.png 315w,/static/1876f08eb72ea6309e3d379ab9a70269/f058b/output_98_0.png 630w,/static/1876f08eb72ea6309e3d379ab9a70269/c6d67/output_98_0.png 734w&quot; sizes=&quot;(max-width: 630px) 100vw, 630px&quot; style=&quot;width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0&quot; loading=&quot;lazy&quot; decoding=&quot;async&quot;/&gt;
  &lt;/a&gt;
    &lt;/span&gt;&lt;/p&gt;&lt;p&gt;Berikut adalah Barplot untuk kata - kata pada kelas &lt;code&gt;0&lt;/code&gt; yang mempunyai frekuensi kumunculan lebih dari kelas &lt;code&gt;1&lt;/code&gt;.&lt;/p&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;_0 = words[(words[&amp;#x27;Kelas_0&amp;#x27;] &amp;gt; words[&amp;#x27;Kelas_1&amp;#x27;])]

plt.figure(figsize = (12,7))
sns.barplot(x = &amp;#x27;kata&amp;#x27;, y = &amp;#x27;Frekuensi&amp;#x27;, data = _0[:25])
plt.title(&amp;#x27;Frekuensi Kata yang Sering Muncul Pada Label Bukan Hoax&amp;#x27;, fontsize = 14)
plt.xticks(rotation = 90)
plt.show()
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;span class=&quot;gatsby-resp-image-wrapper&quot; style=&quot;position:relative;display:block;margin-left:auto;margin-right:auto;max-width:630px&quot;&gt;
      &lt;a class=&quot;gatsby-resp-image-link&quot; href=&quot;/static/ecc25ab3ac5bb4601c3792db414a3d0d/c54b3/output_100_0.png&quot; style=&quot;display:block&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;
    &lt;span class=&quot;gatsby-resp-image-background-image&quot; style=&quot;padding-bottom:70.25316455696203%;position:relative;bottom:0;left:0;background-image:url(&amp;#x27;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAOCAYAAAAvxDzwAAAACXBIWXMAAAsSAAALEgHS3X78AAACdklEQVQ4y52SXUiTURiA36mVURcFXUR0o21aBJXRReBNJamBlYmlZRlUhE7KdhEaZjb7kZCw1CBXOMHNrakYlBBiyITCvzKiwJ/57ZvfsXC56dr8ZFvfeevsD4IM5gsP5z3n5TycnxfE6Rn4NjZx0D2/ULcoLt5xuVw1PM/XW63WCBzHNUxNTTXwPP+YzS0WSyPLOY6rt9vttaIo3nW73fc9Hs89QDfCpNb4wD87hywopRhtsD1hQHzYDLanrRU+8p3Vliilv6IFEf1hAIkE40+01f5ZBxP6KA2cka40ABt1YNPoKz2DgwHhSq8dDgCA1VZNW5XjhQ5F66egUJKYNfIuUQkZQktX5Q99Mzr7tBFhWBStFLB7GCwaw22nSYczXdU+p/AhJJFQikD/+snQI/8TQONb4J4ZbjlMehQMZd6RjhJp9F2DhIjLQildFkBjL/DPjWqnSY8zpgr82FWKvS+V2D/wCM1j7dhPBnD65xzOiYvoXFpCj8///yu76lpgskmvshtaPba2Mn64s5S86Swiuo5CUtt9gSh7bpIrvVpyo++1UGU2E7V5mGhGbKRn1EE+jy4QYcRFnAPzgmuICK6hCQIVR3LgcsbRTddzT2wvP30g8eq51MSCk/uSzl88tC33UpoiXXk8cWd+RlJCdmZyUl6efHP2Gfn6w4WKlCyVPDNNpSjYf02uTFEqinefSlbtzZKztgEct7NBBgCrvF9qAmvtr4rYEA8AcQCwjtVCxIeIRW+g7TayerhjgkJvcBJgoUkWzPtAWXNMllGeE5NeWRyzYeuWtfLU1Lg9OfmxJab3sq8U2QFgR8KuNeqz6piA54/wN/NnYSTwOgUGAAAAAElFTkSuQmCC&amp;#x27;);background-size:cover;display:block&quot;&gt;&lt;/span&gt;
  &lt;img class=&quot;gatsby-resp-image-image&quot; alt=&quot;png&quot; title=&quot;png&quot; src=&quot;/static/ecc25ab3ac5bb4601c3792db414a3d0d/f058b/output_100_0.png&quot; srcSet=&quot;/static/ecc25ab3ac5bb4601c3792db414a3d0d/c26ae/output_100_0.png 158w,/static/ecc25ab3ac5bb4601c3792db414a3d0d/6bdcf/output_100_0.png 315w,/static/ecc25ab3ac5bb4601c3792db414a3d0d/f058b/output_100_0.png 630w,/static/ecc25ab3ac5bb4601c3792db414a3d0d/c54b3/output_100_0.png 727w&quot; sizes=&quot;(max-width: 630px) 100vw, 630px&quot; style=&quot;width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0&quot; loading=&quot;lazy&quot; decoding=&quot;async&quot;/&gt;
  &lt;/a&gt;
    &lt;/span&gt;&lt;/p&gt;&lt;p&gt;Plotting WordCloud&lt;/p&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;__0 = {}
for i in range(len(_0)):
    __0[_0.kata.values[i]] = _0.Max_prop.values[i]

wordcloud = WordCloud(width = 4000, height = 3000, min_font_size = 5,
                      background_color = &amp;#x27;white&amp;#x27;, colormap = &amp;#x27;Blues&amp;#x27;).fit_words(__0)

plt.figure(figsize = (10, 8))
plt.imshow(wordcloud)
plt.axis(&amp;quot;off&amp;quot;)
plt.tight_layout(pad = 0)
plt.show()
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;span class=&quot;gatsby-resp-image-wrapper&quot; style=&quot;position:relative;display:block;margin-left:auto;margin-right:auto;max-width:630px&quot;&gt;
      &lt;a class=&quot;gatsby-resp-image-link&quot; href=&quot;/static/9433af17f0a44aa445d6f83f2f3d798c/c6d67/output_102_0.png&quot; style=&quot;display:block&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;
    &lt;span class=&quot;gatsby-resp-image-background-image&quot; style=&quot;padding-bottom:75.31645569620254%;position:relative;bottom:0;left:0;background-image:url(&amp;#x27;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAPCAYAAADkmO9VAAAACXBIWXMAAAsSAAALEgHS3X78AAADqklEQVQ4yyVSa28bVRDd/44EfCpfAAkqBGpFaCFtKTRpE6dJk21q5217HcfXa3vf792775d9bw5a82GkeRzNnJkzQlG17/auNMWk5cPSz4mb1uRYssnelUbOJjZZ+gUZmQn5LAdk5uZksAiJQUsSlS2h5ZpERUu8rJm6cWWsN/xXIcwb8sP+FP2Zhx/fT/FCXEHSKA7vDPzSm+FVX8XnqYsX4hI7Zwt8vTvE+1sD7BHIqs3WqjVHs+HY8MdPArGScQe+XoSbp4cz3hvb/MPQ4r8dz/nu+Yr/cbrgpxOH735R+Ou+yv+91Pi7a53Lbs6JnfKpmXCTVmuLVjCi8kB40GPJCHJYtOJJuYYaFBhqMW5VCsXPMbeTDrjN+1kDL21gRBVGWozrVQRJjzHWKLtdRdDDsidcyb7kxBVkL+cGrWHQCmpYQt/6NdysQVi0sJIaC6+AEpbw8xY2LXE59zFSI5hhwbq4bDY9Yerm0shMcWukfOYXIF6BZVhCMhNotISddqxqLP0Cl3qMRVhC6YbGNfSkhpk1mLo5K1uGR6AnDOaB9HFsw4kr3ic+VL/Ah6EFYiXYOV3gRqHojWzcmwlmToaBHOBOpdi7s7DyMogPXmdMnLpw46on/Hwwk56dyPjpwwM/Gln4fn+Kp4cEB0MLT95K0MMC7+9MXMw8nE0cfPtqhJfiCs9PZOx8kvHVyzu87ivsm90h7nXaE568laS/vii4WYSdkvjnSkPXeEB8vDxXtmzeXul4PVDx5kLD89PlltVYj9Hhfz9b4uPEYft3Jk4mTk+4XgRSmLdgHFzxsq2ydljACnI8OBmmdgo1KjE0EhAvh2RnkIMCWlyBVmuotIKTNcxMm87vCUW7kcK8QVyseVFv0Kw5Fn6BExKgbhkUv4AW/t/wRovhJDU6EQcKxfkyQpA3uDUSpoYFgqzuCVMrlWQngx4UnBYt2s0jbvQEexMX56sIx/MAf95YWAYFvixC6EGOjyTA8TzE/r2Hv0cOzleUnS9C1O2mJ7hxOYq7h3bi9eW9zi7GCruYu+yQhGygUHZEAvZJjtjcL9nR1GMTI2Zvhg7rLyM21BN2KofseOa3/WWEes0PhKRsl90Nk2qDIK1hdfcLC0hqCJcW0LwUupducyat0G1j0ho2reElDfKaQVIjLOwEecNEgZbrZ37WXAR5exbkrZjWTIyLVhzLjmj5qTiUHfGzpIuyEYnNmm9rab0RvawVs3oj5jUT82p9mpTtddmw7/4Dlc0yhAnD38YAAAAASUVORK5CYII=&amp;#x27;);background-size:cover;display:block&quot;&gt;&lt;/span&gt;
  &lt;img class=&quot;gatsby-resp-image-image&quot; alt=&quot;png&quot; title=&quot;png&quot; src=&quot;/static/9433af17f0a44aa445d6f83f2f3d798c/f058b/output_102_0.png&quot; srcSet=&quot;/static/9433af17f0a44aa445d6f83f2f3d798c/c26ae/output_102_0.png 158w,/static/9433af17f0a44aa445d6f83f2f3d798c/6bdcf/output_102_0.png 315w,/static/9433af17f0a44aa445d6f83f2f3d798c/f058b/output_102_0.png 630w,/static/9433af17f0a44aa445d6f83f2f3d798c/c6d67/output_102_0.png 734w&quot; sizes=&quot;(max-width: 630px) 100vw, 630px&quot; style=&quot;width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0&quot; loading=&quot;lazy&quot; decoding=&quot;async&quot;/&gt;
  &lt;/a&gt;
    &lt;/span&gt;&lt;/p&gt;&lt;h2 id=&quot;pemodelan&quot;&gt;Pemodelan&lt;/h2&gt;&lt;p&gt;Membuat model untuk memprediksi kelas &lt;code&gt;0&lt;/code&gt; (&lt;code&gt;tidak hoax&lt;/code&gt;) dan kelas &lt;code&gt;1&lt;/code&gt; (&lt;code&gt;hoax&lt;/code&gt;) pada data test.&lt;/p&gt;&lt;h3 id=&quot;accelerator-detection&quot;&gt;Accelerator Detection&lt;/h3&gt;&lt;p&gt;Menggunakan GPU atau TPU dari Kaggle sebagai Accelerator.&lt;/p&gt;&lt;p&gt;Note : Pastikan untuk menyalakan GPU / TPU sebagai Accelator, untuk effisiensi waktu pelatihan model.&lt;/p&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;try:
    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()
    print(&amp;#x27;Running on TPU &amp;#x27;, tpu.master())
except ValueError:
    tpu = None

if tpu:
    tf.config.experimental_connect_to_cluster(tpu)
    tf.tpu.experimental.initialize_tpu_system(tpu)
    strategy = tf.distribute.experimental.TPUStrategy(tpu)
    BATCH_SIZE = 16 * strategy.num_replicas_in_sync
else:
    strategy = tf.distribute.get_strategy()
    BATCH_SIZE = 32 * strategy.num_replicas_in_sync

AUTO = tf.data.experimental.AUTOTUNE
print(&amp;quot;REPLICAS: &amp;quot;, strategy.num_replicas_in_sync)
&lt;/code&gt;&lt;/pre&gt;&lt;pre&gt;&lt;code&gt;Running on TPU  grpc://10.0.0.2:8470
REPLICAS:  8
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&quot;pemodelan-data-gambar-efficientnet&quot;&gt;Pemodelan Data Gambar (EfficientNet)&lt;/h2&gt;&lt;p&gt;Model yang digunakan untuk data gambar yaitu pretrained Deep Learning model yang cukup popular yaitu &lt;code&gt;EfficientNet&lt;/code&gt;. &lt;code&gt;EfficientNet&lt;/code&gt; digunakan karena kemampuannya yang cukup mengesankan dalam mengklasifikasi data gambar pada &lt;code&gt;ImageNet&lt;/code&gt;.&lt;/p&gt;&lt;p&gt;&lt;span class=&quot;gatsby-resp-image-wrapper&quot; style=&quot;position:relative;display:block;margin-left:auto;margin-right:auto;max-width:630px&quot;&gt;
      &lt;a class=&quot;gatsby-resp-image-link&quot; href=&quot;/static/8e6e4c13294e03c69bc808b9d427122c/17d12/params.png&quot; style=&quot;display:block&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;
    &lt;span class=&quot;gatsby-resp-image-background-image&quot; style=&quot;padding-bottom:79.74683544303798%;position:relative;bottom:0;left:0;background-image:url(&amp;#x27;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAQCAYAAAAWGF8bAAAACXBIWXMAABYlAAAWJQFJUiTwAAACRElEQVQ4y4WU627bMAxG8/5vlQHZnxZph66zc7fTzk5cX+OLbMvyGagmQ4o1KwFCMiwdfiQlTcZxpKoqmqahUYp2GGhPJ1RRoHyfneOw3m5ZLJYcjxF936FUQ9uq89iglKKua8uYyCTPcy6mfY9kPqdcr6FpKLOcosjJsoSqPqH1wDCAETew2wWsVmt836coCiYC6foeWVU+PRGt19RdxwCM5yDHY8636Q++z5749fzKqYAsHQkDxd6Prbq+763SyVsUUVQVyOj7fyGCM2Ykz0aKHHwvwHW3JEmKGTXGaJSqWW9WVplY27ZMRGpvDOnjI3VZnmEG1UCayiLQukfrjmHQtobDYJDaX5sx5h3oeR6qrjG+b0GiKsvgeGhtbbuuJQgC5vM5m40oTN71j6N1AV3mFrjxPKowhOjAqYYkHlGNsd3bbrfs93tbcJmnIvkKdplfRgs8vMXo5I3YDVntUlzHIUlibtlnoA/Arb+nCgOSRUzTdVRlabslFoYhq9WKl5cXynN9L+puAl+D3xRRwu6nx2sQ2M3iUqthGOxx0Fp/2HjdkH+AjvPMMTqR5QN1fSIMD7YZ8vNWurdKYIF9L91syTNRoW0TxL9K8SZwHAfSpMZx9njext5XSfVytq6BX7kFSn3iOMF1l9zd3dkzJ00R2P+6+plJve1drqrS0qWjruuyWCzous6+HhJQ7qoEkDLIt7xOsl7Opfy7HicSUciSpkDsi6M1y+WS2Wxmg4jyh4cH7u/v7Y2ZTqc2Ewkoe68D/QGMI9CDW07AUAAAAABJRU5ErkJggg==&amp;#x27;);background-size:cover;display:block&quot;&gt;&lt;/span&gt;
  &lt;img class=&quot;gatsby-resp-image-image&quot; alt=&quot;params&quot; title=&quot;params&quot; src=&quot;/static/8e6e4c13294e03c69bc808b9d427122c/f058b/params.png&quot; srcSet=&quot;/static/8e6e4c13294e03c69bc808b9d427122c/c26ae/params.png 158w,/static/8e6e4c13294e03c69bc808b9d427122c/6bdcf/params.png 315w,/static/8e6e4c13294e03c69bc808b9d427122c/f058b/params.png 630w,/static/8e6e4c13294e03c69bc808b9d427122c/40601/params.png 945w,/static/8e6e4c13294e03c69bc808b9d427122c/78612/params.png 1260w,/static/8e6e4c13294e03c69bc808b9d427122c/17d12/params.png 1666w&quot; sizes=&quot;(max-width: 630px) 100vw, 630px&quot; style=&quot;width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0&quot; loading=&quot;lazy&quot; decoding=&quot;async&quot;/&gt;
  &lt;/a&gt;
    &lt;/span&gt;&lt;/p&gt;&lt;p&gt;Model yang akan digunakan pada notebook ini untuk data gambar adalah &lt;code&gt;EfficientNetB7&lt;/code&gt; dengan weight &lt;code&gt;noise-student&lt;/code&gt;.&lt;/p&gt;&lt;p&gt;Published Paper : &lt;a href=&quot;https://arxiv.org/abs/1905.11946&quot;&gt;EfficientNet&lt;/a&gt;&lt;/p&gt;&lt;h3 id=&quot;load-image-data---tf-dataset&quot;&gt;Load Image Data - TF Dataset&lt;/h3&gt;&lt;p&gt;Load data gambar ke bentuk Tensor lalu alokasikan ke TF Dataset untuk effisiensi memori. Data gambar yang digunakan adalah data hasil upsampling dengan menggunakan augmentasi pada kelas &lt;code&gt;0&lt;/code&gt; sebesar &lt;code&gt;200%&lt;/code&gt;.&lt;/p&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;# Train Dataset
train_prep = pd.read_csv(&amp;#x27;../input/data-bdc/Preprocess and Up Sample/Up-Sample-0-by-200%/Keterangan.csv&amp;#x27;)
# Valid Data
valid_prep = pd.read_csv(&amp;#x27;../input/data-bdc/Validitas/Keterangan.csv&amp;#x27;)
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Spesifikasikan direktori file gambar dengan pada GCS&lt;/p&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;# Spesifikasi PATH data train pada GCS
AUG_PATH = GCS_PATH + &amp;#x27;/Preprocess and Up Sample/&amp;#x27;

# Data Up Sample 200%
TRAIN_X_, TRAIN_y_ = shuffle([AUG_PATH + x for x in train_prep.DIR.values],
                             train_prep.label.values, random_state = SEED)

# Spesifikasi PATH data Valid pada GCS
VAL_X = [GCS_PATH + &amp;#x27;/&amp;#x27; + x for x in valid_prep.DIR.values]
VAL_y = valid_prep.label.values

# Spesifikasi PATH data test pada GCS
TEST_PATH = GCS_PATH + &amp;#x27;/Data BDC - Satria Data 2020/Data Uji/File Gambar Data Uji/&amp;#x27;
TEST_X = [TEST_PATH + x for x in test[&amp;#x27;nama file gambar&amp;#x27;].values]
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;strong&gt;Decoding image&lt;/strong&gt; : mengubah data gambar menjadi tensor.&lt;/p&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;def decode_image(filename, label=None, image_size=(512, 512)):
    &amp;quot;&amp;quot;&amp;quot;
    Decode Image from String Path Tensor
    &amp;quot;&amp;quot;&amp;quot;
    bits = tf.io.read_file(filename)
    image = tf.image.decode_jpeg(bits, channels=3)
    image = tf.cast(image, tf.float32) / 255.0

    if label is None: # if test
        shapes = tf.shape(image)
        h, w = shapes[-3], shapes[-2]
        dim = tf.minimum(h, w)
        image = tf.image.resize_with_crop_or_pad(image, dim, dim)
        image = tf.image.resize(image, image_size)
        return image
    else:
        image = tf.image.resize(image, image_size)
        return image, label
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Inisialisasi &lt;code&gt;TF Dataset&lt;/code&gt;.&lt;/p&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;# TF Train Dataset
train_dataset = (
    tf.data.Dataset
    .from_tensor_slices((TRAIN_X_, TRAIN_y_))
    .map(decode_image, num_parallel_calls=AUTO)
    .cache()
    .repeat()
    .shuffle(1024)
    .batch(BATCH_SIZE)
    .prefetch(AUTO)
)

# TF Valid Dataset
valid_dataset = (
    tf.data.Dataset
    .from_tensor_slices((VAL_X, VAL_y))
    .map(decode_image, num_parallel_calls=AUTO)
    .batch(BATCH_SIZE)
    .cache()
    .prefetch(AUTO)
)

# TF Test Dataset
test_dataset = (
    tf.data.Dataset
    .from_tensor_slices((TEST_X))
    .map(decode_image, num_parallel_calls=AUTO)
    .batch(BATCH_SIZE)
)
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&quot;membuat-model&quot;&gt;Membuat Model&lt;/h3&gt;&lt;p&gt;Membuat model yang akan digunakan. Berikut adalah arsitektur model yang akan digunakan.&lt;/p&gt;&lt;p&gt;&lt;span class=&quot;gatsby-resp-image-wrapper&quot; style=&quot;position:relative;display:block;margin-left:auto;margin-right:auto;max-width:630px&quot;&gt;
      &lt;a class=&quot;gatsby-resp-image-link&quot; href=&quot;/static/2a3851e6da1c28a039a2f955d591a62f/6af66/model_1.png&quot; style=&quot;display:block&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;
    &lt;span class=&quot;gatsby-resp-image-background-image&quot; style=&quot;padding-bottom:98.10126582278481%;position:relative;bottom:0;left:0;background-image:url(&amp;#x27;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAUCAYAAACNiR0NAAAACXBIWXMAAAsTAAALEwEAmpwYAAACJElEQVQ4y42UC5OqMAyF+f9/TscRdRQf+EBABRWLItjufJnJXnCve29nYpLanuakh3rOOXe73VwURe5wOLg0Td3pdBKvORaGoZvP5269XkuMLZdLMfYzrLXOU8DNZuN2u50AH49Hyff7vdtut+IBGwwG4lerlQAFQSBWFEUXsK5rV1WV+Ofz2YnxVPx4PFxZlu5+v0usRt40zR9Afhj8YYyRTXg18slkIlVfr1d3Pp9dnudil8tFDlcwhlT4er1clmVClx7hkyQR2nEcC8Veryf5YrEQ2tPp1Pm+L8AK+E2ZoH06J5NrzAH0jMvhAA6jr8zB4keFmnwagPw22vs9pEElUMboFXNqXAgS0eq0Wm5fW0QrZrOZVO2RsBHJMMECPLR0Eznr+I+5fr8vABTCYciJNRzoteXwLgk1Klc5sZb+0ltVBDGetd57P9AUCxUM0SMbNiFgLg7AT8PT69bGAgI9qEIHKsPh0I1GI2kL8Xg8Fqm15dKRTfumoAZFpcUloUPAiVU2bcBOhe+ALARUPzs8VTJPTkswXf9PQHqGVOgVlVIlFPk6qE5fm/YL8ysgQGyCFn0EhN4hDXoLfT4/1v0XIBS5GCSihqgB0McCg/5fAW1r8H9d17aqqo4FQSC+LEtrjLFFUXyvfx8/KuR2oUj/uAxiPjXkAmViXhqYfKLMc2GsteIZaZqaPM9NlmUmSRIThqHxfd9EUWTiOBbfNE1nn9oXqdb9GF7lj6MAAAAASUVORK5CYII=&amp;#x27;);background-size:cover;display:block&quot;&gt;&lt;/span&gt;
  &lt;img class=&quot;gatsby-resp-image-image&quot; alt=&quot;model&quot; title=&quot;model&quot; src=&quot;/static/2a3851e6da1c28a039a2f955d591a62f/f058b/model_1.png&quot; srcSet=&quot;/static/2a3851e6da1c28a039a2f955d591a62f/c26ae/model_1.png 158w,/static/2a3851e6da1c28a039a2f955d591a62f/6bdcf/model_1.png 315w,/static/2a3851e6da1c28a039a2f955d591a62f/f058b/model_1.png 630w,/static/2a3851e6da1c28a039a2f955d591a62f/6af66/model_1.png 640w&quot; sizes=&quot;(max-width: 630px) 100vw, 630px&quot; style=&quot;width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0&quot; loading=&quot;lazy&quot; decoding=&quot;async&quot;/&gt;
  &lt;/a&gt;
    &lt;/span&gt;&lt;/p&gt;&lt;p&gt;Load EfficientNet&lt;/p&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;# Tensorflow - Keras Layers
import tensorflow.keras.backend as K
import tensorflow.keras.layers as L
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import ModelCheckpoint

# EfficientNet
import efficientnet.tfkeras as efn
&lt;/code&gt;&lt;/pre&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;def Make_EfficientNet_model():
    &amp;quot;&amp;quot;&amp;quot;
    https://www.kaggle.com/xhlulu/flowers-tpu-concise-efficientnet-b7
    &amp;quot;&amp;quot;&amp;quot;
    model = tf.keras.Sequential([
        efn.EfficientNetB7(            # EfficientnetB7
            input_shape=(512, 512, 3),
            weights=&amp;#x27;noisy-student&amp;#x27;,
            include_top=False
        ),
        L.GlobalAveragePooling2D(),
        L.Dense(512, activation= &amp;#x27;relu&amp;#x27;),
        L.Dropout(0.2),
        L.Dense(1, activation=&amp;#x27;sigmoid&amp;#x27;)
    ])
    model.compile(optimizer=&amp;#x27;adam&amp;#x27;, loss = &amp;#x27;binary_crossentropy&amp;#x27;, metrics=[&amp;#x27;accuracy&amp;#x27;])
    return model
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Build &amp;amp; Scope model ke &lt;code&gt;TPU&lt;/code&gt; &amp;amp; Compile model.&lt;/p&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;with strategy.scope():
    model = Make_EfficientNet_model()
&lt;/code&gt;&lt;/pre&gt;&lt;pre&gt;&lt;code&gt;Downloading data from https://github.com/qubvel/efficientnet/releases/download/v0.0.1/efficientnet-b7_noisy-student_notop.h5
258072576/258068648 [==============================] - 4s 0us/step
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&quot;training-model&quot;&gt;Training Model&lt;/h3&gt;&lt;p&gt;Melakukan pelatihan terhadap model dari data train. Pelatihan dilakukan dengan ketentuan:&lt;/p&gt;&lt;pre&gt;&lt;code&gt;EPOCHS = 10
STEPS_PER_EPOCHS = len(TRAIN_X_) // BATCH_SIZE:128 (Jika menggunakan TPU)
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Pelatihan dilakukan dengan memperhatikan nilai dari &lt;code&gt;val_accuracy&lt;/code&gt;, &lt;code&gt;epoch&lt;/code&gt; yang memiliki nilai &lt;code&gt;val_accuracy&lt;/code&gt; terbaik akan disave &lt;code&gt;weights&lt;/code&gt; - nya dan akan di load pada saat melakukan evaluasi dan prediksi nantinya.&lt;/p&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;# Config
EPOCHS = 10
STEPS_PER_EPOCH = len(TRAIN_X_) // BATCH_SIZE
checkpoint = ModelCheckpoint(&amp;#x27;EfficientNetB7_best_model.h5&amp;#x27;, monitor=&amp;#x27;val_accuracy&amp;#x27;,
                             save_best_only=True, save_weights_only=True, mode=&amp;#x27;max&amp;#x27;)

# Fitting Model
print(f&amp;#x27;[INFO] Fitting Model&amp;#x27;)
history = model.fit(train_dataset, epochs = EPOCHS,
                    steps_per_epoch = STEPS_PER_EPOCH,
                    validation_data = valid_dataset,
                    callbacks = [checkpoint])
&lt;/code&gt;&lt;/pre&gt;&lt;pre&gt;&lt;code&gt;[INFO] Fitting Model
Epoch 1/10
38/38 [==============================] - 164s 4s/step - loss: 0.4728 - accuracy: 0.7858 - val_loss: 1.0803 - val_accuracy: 0.7417
Epoch 2/10
38/38 [==============================] - 40s 1s/step - loss: 0.3696 - accuracy: 0.8329 - val_loss: 0.8951 - val_accuracy: 0.8142
Epoch 3/10
38/38 [==============================] - 35s 914ms/step - loss: 0.3544 - accuracy: 0.8454 - val_loss: 0.9107 - val_accuracy: 0.8063
Epoch 4/10
38/38 [==============================] - 40s 1s/step - loss: 0.3058 - accuracy: 0.8647 - val_loss: 0.6083 - val_accuracy: 0.8236
Epoch 5/10
38/38 [==============================] - 35s 915ms/step - loss: 0.3186 - accuracy: 0.8625 - val_loss: 0.5071 - val_accuracy: 0.8047
Epoch 6/10
38/38 [==============================] - 35s 916ms/step - loss: 0.2928 - accuracy: 0.8736 - val_loss: 0.5885 - val_accuracy: 0.8189
Epoch 7/10
38/38 [==============================] - 35s 915ms/step - loss: 0.2837 - accuracy: 0.8734 - val_loss: 1.2540 - val_accuracy: 0.6409
Epoch 8/10
38/38 [==============================] - 35s 915ms/step - loss: 0.2478 - accuracy: 0.8925 - val_loss: 0.7599 - val_accuracy: 0.8142
Epoch 9/10
38/38 [==============================] - 35s 915ms/step - loss: 0.2325 - accuracy: 0.9025 - val_loss: 0.6619 - val_accuracy: 0.7496
Epoch 10/10
38/38 [==============================] - 35s 914ms/step - loss: 0.2350 - accuracy: 0.8966 - val_loss: 0.6473 - val_accuracy: 0.7811
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Meninjau &lt;code&gt;train history&lt;/code&gt; pada model&lt;/p&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;fig, (ax1, ax2) = plt.subplots(2, 1, figsize = (12, 10))
ax1.plot(range(1, EPOCHS + 1), history.history[&amp;#x27;loss&amp;#x27;], label = &amp;#x27;loss&amp;#x27;)
ax1.plot(range(1, EPOCHS + 1), history.history[&amp;#x27;val_loss&amp;#x27;], label = &amp;#x27;val_loss&amp;#x27;)
ax1.set_title(&amp;#x27;Loss at Training&amp;#x27;, fontsize = 14)
ax1.legend()
ax2.plot(range(1, EPOCHS + 1), history.history[&amp;#x27;accuracy&amp;#x27;], label = &amp;#x27;accuracy&amp;#x27;)
ax2.plot(range(1, EPOCHS + 1), history.history[&amp;#x27;val_accuracy&amp;#x27;], label = &amp;#x27;val_accuracy&amp;#x27;)
ax2.set_title(&amp;#x27;Accuracy at Training&amp;#x27;, fontsize = 14)
ax2.legend()
fig.show()
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;span class=&quot;gatsby-resp-image-wrapper&quot; style=&quot;position:relative;display:block;margin-left:auto;margin-right:auto;max-width:630px&quot;&gt;
      &lt;a class=&quot;gatsby-resp-image-link&quot; href=&quot;/static/86b46e6f767204d7be23816991d06fbb/6bbf7/output_123_0.png&quot; style=&quot;display:block&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;
    &lt;span class=&quot;gatsby-resp-image-background-image&quot; style=&quot;padding-bottom:82.91139240506328%;position:relative;bottom:0;left:0;background-image:url(&amp;#x27;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAARCAYAAADdRIy+AAAACXBIWXMAAAsSAAALEgHS3X78AAADFUlEQVQ4y4VUzVIkRRBuL170HTwoYISvYAQgL7AH2NC7B8MX4hnkxGE3NJbDDD3AsoNeZNAwQBgChmG6e7r6t7qyMivTqG5wYDW0Ir7IrKysqqzMLyt4/erVx3GcvFBZ9jJN002VplseaZpuZVm2eXd39/VwOPw2DMPver3e97e3t9/kabyZKtX6KKW2klRt3k6jl3EUvQjeDU+WwDSGnRPHIo65g58/gJnlcTi04oqZOEJxrvMjQsnUXGZRBMHxyc+fIZhUoBBBQ0LWiUPn9z4FM7f3CVRO0DgxxWKdmURYAEAF/X5v2SJm/naG2jHUzKZghsrH1YG5kw67Na/b+pmP349EeRAOBsuItjtQ2le3q2xrYVOKT8XftiYXJtvpzMJWi5hC/CUCpSCYPHh7dLQEYOdAba5QpA2fRISEgARKEig6iU1nfwJmJh+6jxPRpsG74+MVAOsaS1IZbFEaFA0oQCyAToxFMZbauUESaPG8WO2TkVywu7v7SVVVP6KFw1rrgdZ6YIwJK92EscoPVFmHhYZeXjd9Vdb7WdXsp0UVzvNqMJ1nhzNVhpEqw/u0OEpV9iYQkf9FEAQfPMgPvaT/8t3Z2fk0iqK3V1dXvydxfOqIRoQ4staOEOnXsiz/SJJkrLLsz2kUj2eJuii0Oc3rZqTK5nRe6NG80Kfzsvmt0vo4ODw4WLFGg7Ug2oA0yGJIRPucNla0deJLoZHFOmnh8+fh5PkgIgx2dn74wiLN/SFVWSDokowuiZqSxBQkpqSOnxU9kPmhwkwCVVdlEWyLYq0KfjkZLjUGMlNlIlb7LhEhaP07vjlhNMKOPPE7veWpFm4yEc9V27BA1fHw9U9vPkejlTh4JPY/O+QRjhad0uQLG9ROHAkiZsF+v7+MiKpNgg9HFo3xHNxJq1l8Z2Dzvo8/MA/29vZWAKDg936Yf4X3ISuujBbzJz+StbYMtre3P5pMJl8mSbI2Ho+/urm5WY+iaDWO49XLy8uNyWSy7vXZbLZ6cXGxcT+drkWz+9XpdLp2fn6+4e1+/fr6euPs7Gz9L6X62K2rH47sAAAAAElFTkSuQmCC&amp;#x27;);background-size:cover;display:block&quot;&gt;&lt;/span&gt;
  &lt;img class=&quot;gatsby-resp-image-image&quot; alt=&quot;png&quot; title=&quot;png&quot; src=&quot;/static/86b46e6f767204d7be23816991d06fbb/f058b/output_123_0.png&quot; srcSet=&quot;/static/86b46e6f767204d7be23816991d06fbb/c26ae/output_123_0.png 158w,/static/86b46e6f767204d7be23816991d06fbb/6bdcf/output_123_0.png 315w,/static/86b46e6f767204d7be23816991d06fbb/f058b/output_123_0.png 630w,/static/86b46e6f767204d7be23816991d06fbb/6bbf7/output_123_0.png 716w&quot; sizes=&quot;(max-width: 630px) 100vw, 630px&quot; style=&quot;width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0&quot; loading=&quot;lazy&quot; decoding=&quot;async&quot;/&gt;
  &lt;/a&gt;
    &lt;/span&gt;&lt;/p&gt;&lt;h3 id=&quot;load-weights-model&quot;&gt;Load weights model&lt;/h3&gt;&lt;p&gt;Load &lt;code&gt;weights&lt;/code&gt; terbaik yang telah disimpan.&lt;/p&gt;&lt;h4 id=&quot;note-&quot;&gt;Note :&lt;/h4&gt;&lt;ul&gt;&lt;li&gt;Untuk mendapatkan hasil yang sama dengan yang di submit pada website &lt;code&gt;BDC - Satria Data&lt;/code&gt; bisa menggunakan &lt;code&gt;weights&lt;/code&gt; yang telah kami simpan &lt;a href=&quot;https://www.kaggle.com/pencarikebahagiaan/modelku&quot;&gt;disini&lt;/a&gt;.&lt;/li&gt;&lt;li&gt;Untuk melihat proses training menggunakan data dengan kriteria Up-Sampling lainnya untuk perbandingan silahkan kunjugi notebook pada responsitory kami &lt;a href=&quot;https://github.com/Hyuto/BDC-Satria-Data/tree/master/Notebooks&quot;&gt;di sini&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;# model.load_weights(&amp;#x27;EfficientNetB7_best_model.h5&amp;#x27;)
model.load_weights(&amp;#x27;../input/modelku/Image Model/200%_best_model.h5&amp;#x27;) # Jika menggunakan Model kami
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&quot;memprediksi-dan-mengevaluasi-data&quot;&gt;Memprediksi dan Mengevaluasi Data&lt;/h3&gt;&lt;p&gt;Memprediksi data train dan data valid lalu dilakukan pengevaluasian untuk melihat kebaikan model.&lt;/p&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;# Valid
val_pred = model.predict(np.concatenate([x for x, y in valid_dataset], axis=0))
val_pred_classes = np.array(val_pred.flatten() &amp;gt;= .5, dtype = &amp;#x27;int&amp;#x27;)
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Mengevaluasi model dengan &lt;code&gt;Accuracy Score&lt;/code&gt; dan &lt;code&gt;F1 Score&lt;/code&gt;.&lt;/p&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;print(f&amp;#x27;Accuracy Valid Data : {accuracy_score(VAL_y, val_pred_classes)}&amp;#x27;)
print(f&amp;#x27;F1 Score Valid Data : {f1_score(VAL_y, val_pred_classes)}&amp;#x27;)
&lt;/code&gt;&lt;/pre&gt;&lt;pre&gt;&lt;code&gt;Accuracy Valid Data : 0.8299212598425196
F1 Score Valid Data : 0.9040852575488455
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Confussion Matrix&lt;/p&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;plt.figure(figsize = (7, 7))
sns.heatmap(confusion_matrix(VAL_y, val_pred_classes, normalize = &amp;#x27;true&amp;#x27;),
            annot=True, cmap=plt.cm.Blues)
plt.title(&amp;#x27;Normalized Confussion Matrix Valid Data&amp;#x27;)
plt.xlabel(&amp;#x27;Predicted&amp;#x27;)
plt.ylabel(&amp;#x27;Actual&amp;#x27;)
plt.show()
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;span class=&quot;gatsby-resp-image-wrapper&quot; style=&quot;position:relative;display:block;margin-left:auto;margin-right:auto;max-width:426px&quot;&gt;
      &lt;a class=&quot;gatsby-resp-image-link&quot; href=&quot;/static/69dc3cbfb884ef33415b42e135fcf954/531e1/output_131_0.png&quot; style=&quot;display:block&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;
    &lt;span class=&quot;gatsby-resp-image-background-image&quot; style=&quot;padding-bottom:104.43037974683544%;position:relative;bottom:0;left:0;background-image:url(&amp;#x27;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAVCAYAAABG1c6oAAAACXBIWXMAAAsSAAALEgHS3X78AAAEl0lEQVQ4y22TfWwTdRjHnw22u951EF5lokFDFDSOra1usqF/KDGBJQNcu46oQcwGspZhJyu76eyCJINoYkwwgS3tWJCIJvIq/ocEjCHEiBHJeluBIYy1ZWPFtb1r7+0xv197ywj+8clzd7n75Ps8z+/g5ngGrkaiRTEZn/x03xe2D1vbq5s8vjU7d3dW7fJ3VTV722pKVzdWFTy3rqa4rP6VopW11SbchoPP8FtOWJmVW+awZdtLmJeaASJxCYZj6VWRuPRgKJqaFMeSCZPhWGpycHQqYf/oRKKwrjdhdYUSnDOY4JyhSc4ZnOCcIeQ3H/MxDt8etsrfyla2Twvtw7E0RuLSNMMxCW/EJRSjaXT4TmHhhj4saTiCvKvfxKBC98AnzNoDe5m3vuxg3uwBGBpLEqktEpeM4VjaiMQlnTAcS9MqRlO6w3dSL6zr00sa+nXOGaLwrn6FCDlXSOCd/QGuPrjH+v45gH8e6iDem5pOSKTiWBLNezGaQofvJBbW9aG1oZ+mJJVzhlSS1PJ2UHii+cfPFr4TDNxEBEBEuPiH+CpJaAp/F0eRSG/cf1RIBE9tPYaL3j2KlvqckNnYJyz/+OfuZ33n/Mt2HAe4PanSGeZlOEP8WMKSXDIK7+qnwqK6XsGx70J3eeC8v6zjFAA5NvkZmgsxIvH0/wh7aas8keWkVDir9pCw9psrgZr95zs/uIUAl6/fnt5yHrIcHKLXEobHUuhoO42FG4NY4h5Anmw6h8q7BxBqDwuN314LrP/6QpdMZggA7MhExnZrXMY8Bq33JRwZl5GkdbT+gAXrDqJ102HkNh4yUflNhxHe+ErwnBa7t373d/vmQ79QoSWjoy2dNTCtUAxSU1kDJQVxStawsvEAwgvbkXO0ImvbaaJaHK0IT78n7L10t7v9TNi/6/gVgERSBkkx7LKKmMcgVVIMzKiIqYyOle4ehBVNyNk8yJa3UJjyFtVi8yCUNgrBa+OBz89e7SInBlIZDTIa2oiAMFNIanJa2Iyc3UtEFLbCo1rsXoQljcJPNx8Gei+KnWiew6Ss2E2BpBhGKqPR9h8VkoReKrXYqFgl17DYLVwelbrP/BX1f/+bmBOO3B2rMoVTkmKMxiZIxYxGhNq0kLS45PV2XLC6DYtX7cgJFzUIQw+07kvihP9XcRxIuyCraJvRpjGj/ccSzmyZChc2CHEZAxf+HAnQliXFIJAZ6rKKuqQYmomsopbM6Fqlu0eDFU0aZ/NqbIWHwpS3ZKlwgatjSsfA9TsP90hEKKtIhA5zyzNmmU+oPZKQrfBQSEoqnO/qMhD33X0gd94ZTwHIChJelFUMS4oRllUcTGd1MZ01whkNw+msMVjp3j8IK5rDnM0bZspbRKa8JcxWeK5zdm8U5jm3IaJ3YirblMwapGWSEAskxWCzOrL/pjLsbn/HvNujMfbWnXuW4JGj1udf28LDsk2ctTpQPHvpmjlFyzdw7MvtDG/3sDC3fhYizlZ1nJWfIYKk0sXQM0keEk6dPUf+ogIAWAwApeSPKlpaA/NbJbBU7wWmbBvw9p0Ac+vp+7qe++4/XYIdvJr+obYAAAAASUVORK5CYII=&amp;#x27;);background-size:cover;display:block&quot;&gt;&lt;/span&gt;
  &lt;img class=&quot;gatsby-resp-image-image&quot; alt=&quot;png&quot; title=&quot;png&quot; src=&quot;/static/69dc3cbfb884ef33415b42e135fcf954/531e1/output_131_0.png&quot; srcSet=&quot;/static/69dc3cbfb884ef33415b42e135fcf954/c26ae/output_131_0.png 158w,/static/69dc3cbfb884ef33415b42e135fcf954/6bdcf/output_131_0.png 315w,/static/69dc3cbfb884ef33415b42e135fcf954/531e1/output_131_0.png 426w&quot; sizes=&quot;(max-width: 426px) 100vw, 426px&quot; style=&quot;width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0&quot; loading=&quot;lazy&quot; decoding=&quot;async&quot;/&gt;
  &lt;/a&gt;
    &lt;/span&gt;&lt;/p&gt;&lt;h3 id=&quot;memprediksi-data-test&quot;&gt;Memprediksi Data Test&lt;/h3&gt;&lt;p&gt;Menggunakan model untuk memprediksi &lt;code&gt;test dataset&lt;/code&gt;.&lt;/p&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;EfficientNet_pred = model.predict(test_dataset)
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Clear session &amp;amp; free up memory.&lt;/p&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;K.clear_session()
tf.tpu.experimental.initialize_tpu_system(tpu)
&lt;/code&gt;&lt;/pre&gt;&lt;pre&gt;&lt;code&gt;&amp;lt;tensorflow.python.tpu.topology.Topology at 0x7ff9cb254190&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&quot;pemodelan-data-teks-bert&quot;&gt;Pemodelan Data Teks (BERT)&lt;/h2&gt;&lt;p&gt;BERT(Bidirectional Encoder Representations from Transformers) adalah pretrained model karya Jacob Devlin, Ming-Wei Chang, Kenton Lee dan Kristina Toutanova. Model ini berupa transformator dua arah yang telah dilatih sebelumnya.&lt;/p&gt;&lt;p&gt;Published Paper : &lt;a href=&quot;https://arxiv.org/abs/1810.04805&quot;&gt;BERT&lt;/a&gt;&lt;/p&gt;&lt;p&gt;Bert yang digunakan pada notebook ini adalah &lt;code&gt;bert-base-indonesian&lt;/code&gt; buatan &lt;code&gt;Cahya Wirawan&lt;/code&gt; yang berupa pre-trained BERT-base model pada 522MB data wikipedia indonesia dengan vocabulary sebesar 32.000. Keterangan lebih lanjut bisa dibaca &lt;a href=&quot;https://huggingface.co/cahya/bert-base-indonesian-522M&quot;&gt;di sini&lt;/a&gt;&lt;/p&gt;&lt;p&gt;Load Transformers terlebih dahulu&lt;/p&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;from transformers import *
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&quot;load-bert-tokenizer&quot;&gt;Load Bert Tokenizer&lt;/h3&gt;&lt;p&gt;Load Bert Tokenizer dari library &lt;code&gt;transformers&lt;/code&gt;&lt;/p&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;model_name=&amp;#x27;cahya/bert-base-indonesian-522M&amp;#x27;
tokenizer = BertTokenizer.from_pretrained(model_name)
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&quot;encode-teks&quot;&gt;Encode Teks&lt;/h3&gt;&lt;p&gt;Data teks akan di encode menjadi 3 tipe untuk input kedalam model.&lt;/p&gt;&lt;ul&gt;&lt;li&gt;input ids adalah ID / nomor yang merepresentasikan kata&lt;/li&gt;&lt;li&gt;attention mask adalah keterangan dari ID yang harus diperhatikan ditandakan oleh nomor 1.&lt;/li&gt;&lt;li&gt;token type id adalah ID yang berhubungan dengan multiple sequance.&lt;/li&gt;&lt;/ul&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;index = random.randint(0, len(train_text))
encode = tokenizer.encode_plus(train_text[index], return_attention_mask = True,
                               return_token_type_ids=True)
print(&amp;#x27;Actual : &amp;#x27;)
print(train_text[index])
print(&amp;#x27;&amp;#x27;.rjust(80, &amp;#x27;-&amp;#x27;))
print(&amp;#x27;Token IDS : &amp;#x27;)
print(encode[&amp;quot;input_ids&amp;quot;])
print(&amp;#x27;&amp;#x27;.rjust(80, &amp;#x27;-&amp;#x27;))
print(&amp;#x27;Attention Mask : &amp;#x27;)
print(encode[&amp;#x27;attention_mask&amp;#x27;])
print(&amp;#x27;&amp;#x27;.rjust(80, &amp;#x27;-&amp;#x27;))
print(&amp;#x27;Token Type IDS : &amp;#x27;)
print(encode[&amp;#x27;token_type_ids&amp;#x27;])
&lt;/code&gt;&lt;/pre&gt;&lt;pre&gt;&lt;code&gt;Actual :
jokowi ahok bagikan amplop berisi uang beras warga cilincing jokowi ikutan sembako amplop ahok cilincing
--------------------------------------------------------------------------------
Token IDS :
[3, 15071, 29708, 28939, 2688, 9155, 3709, 3714, 2111, 3167, 23190, 1010, 5398, 15071, 3821, 1487, 4461, 9745, 2688, 9155, 29708, 23190, 1010, 5398, 1]
--------------------------------------------------------------------------------
Attention Mask :
[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]
--------------------------------------------------------------------------------
Token Type IDS :
[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&quot;mengecek-sequence-terpanjang&quot;&gt;Mengecek Sequence Terpanjang&lt;/h3&gt;&lt;p&gt;Karena panjang dari data teks tidak sama. Maka perlu di lakukan padding untuk menyamakan panjang dari sequance.&lt;/p&gt;&lt;p&gt;Contoh:&lt;/p&gt;&lt;pre&gt;&lt;code&gt;jokowi ahok bagikan =&amp;gt; Encode IDS =&amp;gt; [3, 15071, 29708] =&amp;gt; Padding to 5 =&amp;gt; [3, 15071, 29708, 0, 0]
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Untuk melakukan padding perlu dicari sequence terpanjang pada data sebagai acuannya.&lt;/p&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;plt.figure(figsize=(7,7))
sns.distplot([len(tokenizer.encode(x)) for x in train_text], label = &amp;#x27;train&amp;#x27;)
plt.title(&amp;#x27;Distplot of Text Lenght&amp;#x27;, fontsize=14)
plt.legend()
plt.show()
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;span class=&quot;gatsby-resp-image-wrapper&quot; style=&quot;position:relative;display:block;margin-left:auto;margin-right:auto;max-width:444px&quot;&gt;
      &lt;a class=&quot;gatsby-resp-image-link&quot; href=&quot;/static/d4694f2bf4b44d4fbbb1e8ec937fba61/9b7bd/output_144_0.png&quot; style=&quot;display:block&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;
    &lt;span class=&quot;gatsby-resp-image-background-image&quot; style=&quot;padding-bottom:97.46835443037975%;position:relative;bottom:0;left:0;background-image:url(&amp;#x27;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAATCAYAAACQjC21AAAACXBIWXMAAAsSAAALEgHS3X78AAAC10lEQVQ4y52Uy08TQRzHfwvVKI9a41Gi5eFBEj2oidF40KN/g3cTTiYae/AgFwwXAmkMAR+xyEsUDBiVmxpaaItAeRWkLe1uH9a2UNjuzmx363bHTCmkYETKL9nMznd/+cz3N7+ZBb1eDzQWFxerVv3+uiAbqPGv+mpjsVi12Wy+0tDQcNtkMt1samq6bjKZbjU2Nt6gWktLy1We540ez0ptiPXXRCLhWrfbfSYHI4RAMpl0ZhSFiFhCCKclRZYlWZaldD4K3yWJfpJzGpbSUjDB43AkkuE41gMMw5wAAANCaI4QQmIpWeOlDCkmNG1rFAQhBAaDgaEuBVGcoaIruKG6IzxN0bJZ7aCRzQM50Ol0OgDQpQTRRUWbN6FaPYncimpW28eVtjNq+UkOyDBMJQDoRRHlgGOehDq6EC1MPkDJBUAag28HYBto9SbUYVeEZAscFAVkGKYCACoR2gJ+mo+qvQ6OKL+zhwOWlpYeAYCjCKFZKo64IqplnCVCOnPgsv8qmZ5DnHc4MhtRuyZYEkiIueTsIUo20HNIgbSp76fD6vMxP3H613eARTscGRrIOaTq4HRYfWULkC8/4ocruaSkpAwAyjFGroyqkXdTIfW1naPgXY3ZD7oXeAwAjssSdm3iDOl1cGqfM0joPsZS6eKB201R0pIrIcikx8Gp/XngcjS1qzH/Au9tykkAOKWkpZl4SiY9dpY61LrtrDY0HdYyW/fvf/c6u8uhuDwGqpKeiwkK6bZzpH8yRN58DxHLBEuWosKB/zpIFEMADEP3sEzgNx1T/rj27OuK2G3z4S6rD3fZfNhi9eJvSz8xF+fRrw0BJ1MIr6cQ3hQxErGE84+IJFlZW1tboQbLAaDCOm6vetLZV28enjA+/ThpvNfcWf+w1XLe/MFpvNs+Wn3tzoPLjzqGzg1aF4xt/Z/r7j9uvmSdWqie9wbPtra/uNDW8fJigOVO/wHNY/So0fNq5QAAAABJRU5ErkJggg==&amp;#x27;);background-size:cover;display:block&quot;&gt;&lt;/span&gt;
  &lt;img class=&quot;gatsby-resp-image-image&quot; alt=&quot;png&quot; title=&quot;png&quot; src=&quot;/static/d4694f2bf4b44d4fbbb1e8ec937fba61/9b7bd/output_144_0.png&quot; srcSet=&quot;/static/d4694f2bf4b44d4fbbb1e8ec937fba61/c26ae/output_144_0.png 158w,/static/d4694f2bf4b44d4fbbb1e8ec937fba61/6bdcf/output_144_0.png 315w,/static/d4694f2bf4b44d4fbbb1e8ec937fba61/9b7bd/output_144_0.png 444w&quot; sizes=&quot;(max-width: 444px) 100vw, 444px&quot; style=&quot;width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0&quot; loading=&quot;lazy&quot; decoding=&quot;async&quot;/&gt;
  &lt;/a&gt;
    &lt;/span&gt;&lt;/p&gt;&lt;p&gt;Dari grafik diatas didapatkan kesimpulan bahwa panjang sequence data tersebar pada rentang 0 - 220 yang memusat pada rentang 0 - 100. Maka dari itu akan digunakan &lt;code&gt;250&lt;/code&gt; sebagai acuan maximum dari panjang sequencenya&lt;/p&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;max_len = 250
&lt;/code&gt;&lt;/pre&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;def regular_encode(texts, tokenizer = tokenizer, maxlen=max_len):
    &amp;quot;&amp;quot;&amp;quot;
    Encoding data teks
    &amp;quot;&amp;quot;&amp;quot;
    enc_di = tokenizer.batch_encode_plus(
        texts,
        return_attention_masks=True,
        return_token_type_ids=True,
        pad_to_max_length=True,
        max_length=maxlen
    )

    return {x : np.asarray(enc_di[x]) for x in enc_di}
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&quot;membuat-model-1&quot;&gt;Membuat Model&lt;/h3&gt;&lt;p&gt;Membuat model yang digunakan untuk memprediksi data teks, berikut adalah arsitekturnya&lt;/p&gt;&lt;p&gt;&lt;span class=&quot;gatsby-resp-image-wrapper&quot; style=&quot;position:relative;display:block;margin-left:auto;margin-right:auto;max-width:630px&quot;&gt;
      &lt;a class=&quot;gatsby-resp-image-link&quot; href=&quot;/static/0a9abd1578520a97bf552ec8263923f6/0d40b/model_2.png&quot; style=&quot;display:block&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;
    &lt;span class=&quot;gatsby-resp-image-background-image&quot; style=&quot;padding-bottom:43.67088607594937%;position:relative;bottom:0;left:0;background-image:url(&amp;#x27;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAJCAIAAAC9o5sfAAAACXBIWXMAAAsTAAALEwEAmpwYAAAA6ElEQVQoz4XR64qFIBQF4N7/CStJMaMsL3nXimqgzcAcOGfO91P2Yqm7EkJIKSmlCKG6rjnn8iGEmKaprmuM8TAMSimYHMcRIdR1XdM0VXx4751z67paa40xcCKltNbqx7qu3vsYYwjB/6ruP5xz0Mk5n+cZY8wYa9uWEDKO43me96uXcCkl51xKSSl577XWOecQAhT+Fz6OwxijHsYYKeXyIIQwxrqu27btY/i6LuhMKZVSYoxKKehMKeWcv1wbaK2VUrACSiljbN/3+503YXg5/Dbs4rqu72EYCiHAg9u27ft+WZb7gx/8+QRq/deDOQAAAABJRU5ErkJggg==&amp;#x27;);background-size:cover;display:block&quot;&gt;&lt;/span&gt;
  &lt;img class=&quot;gatsby-resp-image-image&quot; alt=&quot;model2&quot; title=&quot;model2&quot; src=&quot;/static/0a9abd1578520a97bf552ec8263923f6/f058b/model_2.png&quot; srcSet=&quot;/static/0a9abd1578520a97bf552ec8263923f6/c26ae/model_2.png 158w,/static/0a9abd1578520a97bf552ec8263923f6/6bdcf/model_2.png 315w,/static/0a9abd1578520a97bf552ec8263923f6/f058b/model_2.png 630w,/static/0a9abd1578520a97bf552ec8263923f6/40601/model_2.png 945w,/static/0a9abd1578520a97bf552ec8263923f6/0d40b/model_2.png 1175w&quot; sizes=&quot;(max-width: 630px) 100vw, 630px&quot; style=&quot;width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0&quot; loading=&quot;lazy&quot; decoding=&quot;async&quot;/&gt;
  &lt;/a&gt;
    &lt;/span&gt;&lt;/p&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;def build_model(transformer, loss=&amp;#x27;binary_crossentropy&amp;#x27;, max_len=max_len):
    input_ids = tf.keras.Input(shape=(max_len,), dtype=tf.int32, name=&amp;quot;input_ids&amp;quot;)
    attention_mask = tf.keras.Input(shape=(max_len,), dtype=tf.int32, name=&amp;#x27;attention_mask&amp;#x27;)
    token_type_ids = tf.keras.Input(shape=(max_len,), dtype=tf.int32, name=&amp;#x27;token_type_ids&amp;#x27;)

    sequence_output, pooled_output = transformer([input_ids, attention_mask, token_type_ids])
    cls_token = sequence_output[:, 0, :]
    x = L.Dropout(0.3)(cls_token)
    out = L.Dense(1, activation=&amp;#x27;sigmoid&amp;#x27;)(x)

    model = tf.keras.Model(inputs=[input_ids, attention_mask, token_type_ids], outputs=out)
    model.compile(Adam(lr=3e-5), loss=loss, metrics=[&amp;#x27;accuracy&amp;#x27;])

    return model
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&quot;cross-validation&quot;&gt;Cross Validation&lt;/h3&gt;&lt;p&gt;Metode yang digunakan untuk CV adalah &lt;code&gt;StratifiedKflod&lt;/code&gt;. Metode ini digunakan untuk mengatasi kelas pada data yang imbalance. &lt;code&gt;StratifiedKfold&lt;/code&gt; ini tidak jauh berbeda dengan &lt;code&gt;Kfold&lt;/code&gt; hanya pada saat pembagian menjadi data train dan data valid, proporsi masing masing kelas tetap sama pada kedua data tersebut.&lt;/p&gt;&lt;p&gt;&lt;span class=&quot;gatsby-resp-image-wrapper&quot; style=&quot;position:relative;display:block;margin-left:auto;margin-right:auto;max-width:630px&quot;&gt;
      &lt;a class=&quot;gatsby-resp-image-link&quot; href=&quot;/static/4fe824f6dc276bb0d79b4250ef77d902/8b70b/StratifiedKfold.png&quot; style=&quot;display:block&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;
    &lt;span class=&quot;gatsby-resp-image-background-image&quot; style=&quot;padding-bottom:39.24050632911392%;position:relative;bottom:0;left:0;background-image:url(&amp;#x27;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAICAYAAAD5nd/tAAAACXBIWXMAAAsTAAALEwEAmpwYAAAB+0lEQVQoz0XOS0tUYQCA4flP/YmW0bpVbbMWSoJdsDZdRbBVkBUuMiGQICpMUdQa5+aQHmec41ycZjxzrt+5fucyo76BBW3fxcuT82RCWZfk9RgjiOAM+v0NDravcq5/phHAfBP06Jy2G7HYDND9GM2P+dAKaQuJI1PeNwMObUlOyIQ5NWapf0rPk3AOhU6RV9/uMdRWqRoaT77OEBtlflgwXvDQnIiaEzNREtROBL0w41beptIT5II45Xop5E1nyMBP4Czj+0nC5E7AwAO1u8rrpcvQecmWBVObexiWxYGTMJE3qfdtfocZY1s6u137/3D+eMTAj+E0Y0OTPKrYWG5IxZJMFmwMf0iju87y8iVoz7Dnwf1dh5ou6Ycpd4omu/1/whvlkLfN+K/wNGNNkzzY0XGET8HOGN/WsN2Ikq4zvVnAMTq0zCMWPl1BKo/pxHBzJ6Xcy8j5MuFaIeDjaoWBCOFsyNpJxN2SjemGFK2E2z9NTDcgb2aMFwN6YkTTOOb5+jv67RUGnsHslym69QVy8XDE9IFkY6uKiNKLYd5MmPtl43kBe+6QZxUL3w+pOikvqtaF/Mgf8XQ/48gCzztkcWWMWHlIrtVqoTYaHDYaqKpKmqbYjkNdUZBhiHBd6so+ruPgCHHRPSHwAx+1pmAbAyIpUWotNM3mD3/7N5AuVhFlAAAAAElFTkSuQmCC&amp;#x27;);background-size:cover;display:block&quot;&gt;&lt;/span&gt;
  &lt;img class=&quot;gatsby-resp-image-image&quot; alt=&quot;SKfold&quot; title=&quot;SKfold&quot; src=&quot;/static/4fe824f6dc276bb0d79b4250ef77d902/f058b/StratifiedKfold.png&quot; srcSet=&quot;/static/4fe824f6dc276bb0d79b4250ef77d902/c26ae/StratifiedKfold.png 158w,/static/4fe824f6dc276bb0d79b4250ef77d902/6bdcf/StratifiedKfold.png 315w,/static/4fe824f6dc276bb0d79b4250ef77d902/f058b/StratifiedKfold.png 630w,/static/4fe824f6dc276bb0d79b4250ef77d902/40601/StratifiedKfold.png 945w,/static/4fe824f6dc276bb0d79b4250ef77d902/78612/StratifiedKfold.png 1260w,/static/4fe824f6dc276bb0d79b4250ef77d902/8b70b/StratifiedKfold.png 1266w&quot; sizes=&quot;(max-width: 630px) 100vw, 630px&quot; style=&quot;width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0&quot; loading=&quot;lazy&quot; decoding=&quot;async&quot;/&gt;
  &lt;/a&gt;
    &lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;code&gt;n_split&lt;/code&gt; yang digunakan sebesar 10. Dataset Folding kami bisa di daoat dengan menggunakan:&lt;/p&gt;&lt;pre&gt;&lt;code&gt;cv = StratifiedKFold(n_splits=10)
for fold, (train_ind, val_ind) in enumerate(cv.split(train_text, train.label.values)):
    x_train, y_train = train_text[train_ind], train.label.values[train_ind]
    x_val, y_val = train_text[val_ind], train.label.values[val_ind]
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;load &lt;code&gt;folding.csv&lt;/code&gt; dari responsitory kami.&lt;/p&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;# Load Folding Dataset from responsitory
folding = pd.read_csv(&amp;#x27;https://raw.githubusercontent.com/Hyuto/BDC-Satria-Data/master/Folding.csv&amp;#x27;)
folding.head()
&lt;/code&gt;&lt;/pre&gt;&lt;table&gt;&lt;thead&gt;&lt;tr&gt;&lt;th align=&quot;center&quot;&gt;ID&lt;/th&gt;&lt;th align=&quot;center&quot;&gt;Fold 1&lt;/th&gt;&lt;th align=&quot;center&quot;&gt;Fold 2&lt;/th&gt;&lt;th align=&quot;center&quot;&gt;Fold 3&lt;/th&gt;&lt;th align=&quot;center&quot;&gt;Fold 4&lt;/th&gt;&lt;th align=&quot;center&quot;&gt;Fold 5&lt;/th&gt;&lt;th align=&quot;center&quot;&gt;Fold 6&lt;/th&gt;&lt;th align=&quot;center&quot;&gt;Fold 7&lt;/th&gt;&lt;th align=&quot;center&quot;&gt;Fold 8&lt;/th&gt;&lt;th align=&quot;center&quot;&gt;Fold 9&lt;/th&gt;&lt;th align=&quot;center&quot;&gt;Fold 10&lt;/th&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td align=&quot;center&quot;&gt;71&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;1&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;1&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;1&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;1&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;1&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;1&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;1&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;1&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;1&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;0&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;center&quot;&gt;461&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;1&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;0&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;1&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;1&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;1&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;1&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;1&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;1&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;1&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;1&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;center&quot;&gt;495&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;1&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;1&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;1&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;1&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;1&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;1&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;1&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;1&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;1&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;0&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;center&quot;&gt;550&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;1&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;1&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;1&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;1&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;0&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;1&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;1&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;1&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;1&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;1&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;center&quot;&gt;681&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;1&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;1&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;1&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;0&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;1&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;1&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;1&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;1&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;1&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;1&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;h3 id=&quot;train--evaluate-model&quot;&gt;Train &amp;amp; Evaluate Model&lt;/h3&gt;&lt;p&gt;Melakukan &lt;code&gt;training&lt;/code&gt; pada train data. Train dilakukan pada &lt;code&gt;fold 3&lt;/code&gt; &amp;amp; &lt;code&gt;fold 6&lt;/code&gt; karena kedua fold tersebut memberikan nilai akurasi yang cukup baik dari model pada &lt;code&gt;fold&lt;/code&gt; lainnya. Training memiliki konfihurasi sebagai berikut:&lt;/p&gt;&lt;pre&gt;&lt;code&gt;epochs = 15
steps_per_epoch = len(x_train)//BATCH_SIZE:128 (jika menggunakann TPU)
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Training dilakukan dengan memperhatikan nilai &lt;code&gt;val_accuracy&lt;/code&gt; pada setiap epochnya. Epoch dengan score &lt;code&gt;val_accuracy&lt;/code&gt; tertinggi akan di save &lt;code&gt;weight&lt;/code&gt; - nya untuk diload saat melakukan evaluasi dan pemrediksian terhadap data test.&lt;/p&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;# Init
scores_valid, scores_test, CMS = [], [], []
HISTORY = []

# Test Dataset
test_dataset = (tf.data.Dataset
    .from_tensor_slices((regular_encode(test_text)))
    .batch(BATCH_SIZE)
)

for i in range(1, 11):
    if i in [3, 6]: # Ambil Fold 3 dan 6
        K.clear_session()
        tf.tpu.experimental.initialize_tpu_system(tpu)

        fold = f&amp;#x27;Fold {i}&amp;#x27;
        print(f&amp;#x27;[INFO] {fold}&amp;#x27;)

        # Split Dataset per Fold
        x_train = train_text[folding[fold].values == 1]
        y_train = train.label.values[folding[fold].values == 1]
        x_val = train_text[folding[fold].values == 0]
        y_val = train.label.values[folding[fold].values == 0]

        # Encoding &amp;amp; to TF Dataset
        train_dataset = (tf.data.Dataset
            .from_tensor_slices((regular_encode(x_train), y_train))
            .batch(BATCH_SIZE)
            .cache()
            .repeat()
            .shuffle(1024)
            .prefetch(AUTO)
        )
        valid_dataset = (tf.data.Dataset
            .from_tensor_slices((regular_encode(x_val), y_val))
            .batch(BATCH_SIZE)
            .cache()
            .prefetch(AUTO)
        )

        # Training
        with strategy.scope(): # Build &amp;amp; Scoope Model
            BERT = TFBertModel.from_pretrained(model_name)
            model = build_model(BERT)

        checkpoint = ModelCheckpoint(f&amp;#x27;Fold_{i}_best_model.h5&amp;#x27;, monitor=&amp;#x27;val_accuracy&amp;#x27;,
                                     save_best_only=True, save_weights_only=True,
                                     mode=&amp;#x27;max&amp;#x27;)

        history = model.fit(train_dataset, epochs = 15,
                            steps_per_epoch = len(x_train)//BATCH_SIZE,
                            validation_data = valid_dataset,
                            callbacks = [checkpoint])

        # Load Weights
        # model.load_weights(f&amp;#x27;Fold_{i}_best_model.h5&amp;#x27;) # Trained Best Weights
        model.load_weights(f&amp;#x27;../input/modelku/{fold}_best_model.h5&amp;#x27;) # Catatan Cakrawala Weights

        # Predict
        pred_val  = model.predict(valid_dataset)  # Valid
        pred_test = model.predict(test_dataset)   # Test

        scores_valid.append(pred_val.flatten())
        scores_test.append(pred_test.flatten())
        CMS.append(confusion_matrix(y_val, np.array(pred_val.flatten() &amp;gt;= .5, dtype=&amp;#x27;int&amp;#x27;),
                                    normalize = &amp;#x27;true&amp;#x27;))
        HISTORY.append(history.history)

print(f&amp;#x27;[INFO] Done&amp;#x27;)
&lt;/code&gt;&lt;/pre&gt;&lt;pre&gt;&lt;code&gt;[INFO] Fold 3
Epoch 1/15
29/29 [==============================] - 51s 2s/step - loss: 0.6275 - accuracy: 0.7686 - val_loss: 0.4344 - val_accuracy: 0.8298
Epoch 2/15
29/29 [==============================] - 8s 277ms/step - loss: 0.4023 - accuracy: 0.8413 - val_loss: 0.4263 - val_accuracy: 0.8487
Epoch 3/15
29/29 [==============================] - 6s 199ms/step - loss: 0.3162 - accuracy: 0.8770 - val_loss: 0.4730 - val_accuracy: 0.8440
Epoch 4/15
29/29 [==============================] - 6s 200ms/step - loss: 0.2479 - accuracy: 0.9083 - val_loss: 0.5341 - val_accuracy: 0.7967
Epoch 5/15
29/29 [==============================] - 6s 202ms/step - loss: 0.1471 - accuracy: 0.9448 - val_loss: 0.5429 - val_accuracy: 0.8322
Epoch 6/15
29/29 [==============================] - 6s 201ms/step - loss: 0.0654 - accuracy: 0.9777 - val_loss: 0.7967 - val_accuracy: 0.8227
Epoch 7/15
29/29 [==============================] - 6s 203ms/step - loss: 0.0401 - accuracy: 0.9857 - val_loss: 0.8853 - val_accuracy: 0.8132
Epoch 8/15
29/29 [==============================] - 6s 201ms/step - loss: 0.0480 - accuracy: 0.9855 - val_loss: 0.8994 - val_accuracy: 0.8369
Epoch 9/15
29/29 [==============================] - 6s 200ms/step - loss: 0.0191 - accuracy: 0.9942 - val_loss: 0.9181 - val_accuracy: 0.8463
Epoch 10/15
29/29 [==============================] - 6s 204ms/step - loss: 0.0096 - accuracy: 0.9978 - val_loss: 1.0425 - val_accuracy: 0.8369
Epoch 11/15
29/29 [==============================] - 6s 198ms/step - loss: 0.0125 - accuracy: 0.9958 - val_loss: 1.0749 - val_accuracy: 0.8180
Epoch 12/15
29/29 [==============================] - 6s 200ms/step - loss: 0.0113 - accuracy: 0.9968 - val_loss: 1.2689 - val_accuracy: 0.8369
Epoch 13/15
29/29 [==============================] - 6s 201ms/step - loss: 0.0201 - accuracy: 0.9938 - val_loss: 1.1791 - val_accuracy: 0.8392
Epoch 14/15
29/29 [==============================] - 6s 198ms/step - loss: 0.0131 - accuracy: 0.9956 - val_loss: 1.2603 - val_accuracy: 0.8392
Epoch 15/15
29/29 [==============================] - 6s 202ms/step - loss: 0.0153 - accuracy: 0.9951 - val_loss: 1.2632 - val_accuracy: 0.8203
[INFO] Fold 6
Epoch 1/15
29/29 [==============================] - 26s 912ms/step - loss: 0.5667 - accuracy: 0.7853 - val_loss: 0.4016 - val_accuracy: 0.8463
Epoch 2/15
29/29 [==============================] - 6s 205ms/step - loss: 0.3648 - accuracy: 0.8605 - val_loss: 0.4023 - val_accuracy: 0.8369
Epoch 3/15
29/29 [==============================] - 27s 935ms/step - loss: 0.3321 - accuracy: 0.8733 - val_loss: 0.3992 - val_accuracy: 0.8463
Epoch 4/15
29/29 [==============================] - 6s 203ms/step - loss: 0.2438 - accuracy: 0.9075 - val_loss: 0.4846 - val_accuracy: 0.8203
Epoch 5/15
29/29 [==============================] - 6s 203ms/step - loss: 0.1393 - accuracy: 0.9533 - val_loss: 0.5838 - val_accuracy: 0.8227
Epoch 6/15
29/29 [==============================] - 6s 204ms/step - loss: 0.1016 - accuracy: 0.9617 - val_loss: 0.6281 - val_accuracy: 0.8109
Epoch 7/15
29/29 [==============================] - 6s 200ms/step - loss: 0.0481 - accuracy: 0.9834 - val_loss: 0.7427 - val_accuracy: 0.8180
Epoch 8/15
29/29 [==============================] - 6s 201ms/step - loss: 0.0360 - accuracy: 0.9875 - val_loss: 0.7745 - val_accuracy: 0.8227
Epoch 9/15
29/29 [==============================] - 6s 202ms/step - loss: 0.0177 - accuracy: 0.9957 - val_loss: 0.8515 - val_accuracy: 0.8014
Epoch 10/15
29/29 [==============================] - 6s 203ms/step - loss: 0.0098 - accuracy: 0.9959 - val_loss: 0.9747 - val_accuracy: 0.8156
Epoch 11/15
29/29 [==============================] - 6s 205ms/step - loss: 0.0054 - accuracy: 0.9981 - val_loss: 1.0571 - val_accuracy: 0.8274
Epoch 12/15
29/29 [==============================] - 6s 202ms/step - loss: 0.0064 - accuracy: 0.9986 - val_loss: 1.1008 - val_accuracy: 0.8440
Epoch 13/15
29/29 [==============================] - 6s 205ms/step - loss: 0.0073 - accuracy: 0.9975 - val_loss: 1.0765 - val_accuracy: 0.8274
Epoch 14/15
29/29 [==============================] - 6s 205ms/step - loss: 0.0099 - accuracy: 0.9970 - val_loss: 1.0959 - val_accuracy: 0.8392
Epoch 15/15
29/29 [==============================] - 6s 204ms/step - loss: 0.0075 - accuracy: 0.9976 - val_loss: 1.0887 - val_accuracy: 0.8251
[INFO] Done
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Meninjau &lt;code&gt;train history&lt;/code&gt; pada model setiap &lt;code&gt;fold&lt;/code&gt; - nya&lt;/p&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;fig, (ax1, ax2) = plt.subplots(2, 1, figsize = (12, 10))
for i, fold in enumerate([3, 6]):
    ax1.plot(range(1, 16), HISTORY[i][&amp;#x27;loss&amp;#x27;], label = f&amp;#x27;Fold {fold} loss&amp;#x27;)
    ax1.plot(range(1, 16), HISTORY[i][&amp;#x27;val_loss&amp;#x27;], label = f&amp;#x27;Fold {fold} val_loss&amp;#x27;)
    ax2.plot(range(1, 16), HISTORY[i][&amp;#x27;accuracy&amp;#x27;], label = f&amp;#x27;Fold {fold} accuracy&amp;#x27;)
    ax2.plot(range(1, 16), HISTORY[i][&amp;#x27;val_accuracy&amp;#x27;], label = f&amp;#x27;Fold {fold} val_accuracy&amp;#x27;)
ax1.set_title(&amp;#x27;Loss at Training&amp;#x27;, fontsize = 14)
ax2.set_title(&amp;#x27;Accuracy at Training&amp;#x27;, fontsize = 14)
ax1.legend()
ax2.legend()
fig.show()
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;span class=&quot;gatsby-resp-image-wrapper&quot; style=&quot;position:relative;display:block;margin-left:auto;margin-right:auto;max-width:630px&quot;&gt;
      &lt;a class=&quot;gatsby-resp-image-link&quot; href=&quot;/static/599ae3a41fde39efa22e75626a02c479/6bbf7/output_156_0.png&quot; style=&quot;display:block&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;
    &lt;span class=&quot;gatsby-resp-image-background-image&quot; style=&quot;padding-bottom:82.91139240506328%;position:relative;bottom:0;left:0;background-image:url(&amp;#x27;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAARCAYAAADdRIy+AAAACXBIWXMAAAsSAAALEgHS3X78AAADL0lEQVQ4y4VUS2/bRhBm0UMP/REFepDzHwLUqG30nIMdpPceiv43H3xq4UbWI1biOKdCQlxHlixRosTlksul+NzHzBRklcpJA3iAD7skd4Yz832zzuur62+XfvhsvV4/j6LoWEp5IoRoIKU8Xnnei+vrt7/0+71fLy7avy2X3s9JXh7Hm/Qk5t5JHK5qHMto/Zxz9szp9Tp7AfOqyWRCy+WSrLUEADsgEtLOACxBVZBNBZloTTYJycQBWcFIFblyuhftVpYIwTknzrkFa6H2I8TtaoGsAlQpQCoABAOIA4BsU/8LUGvAOgskMsbETrvd3jMW4o8JIBHWoHotNwgJRxuHaNMNQpYiGtN8e4jt+Tpg4lwPrlppnsk4S4jAAOmSaCPIRj5ZGRJUihCQEHeF1/vPsAvYuRy0dJlHsT+n9WJshD+3ZZrYpllEnwLR4pdhtgGF0+12n2ht4GO9hTGUmpJEsaG0KiitSspVSYVWBPCQnv+bMQad09PT77Is+70sy9dlUfS1Uv2yKAZlVfVXEb9cRLwfZbLjCf7K34QdPwn6bugN3MjvT9lyMAlW/SnzLmfcexNvkj8cInoUjuN8tcXXj549Ozv7PuD8zWKxeD+dTodCiJExZgjWjpQ2w1UY3k49dzb3vbub+WR27y/GfhIN5xEbuSIYzkI2mkfB0BXBe5lurpzLV709KZiqRT2dTkkI0fQj0xWxVFJS5VSBocoaesystcZpv2y3VFlGWivSShkwxsoysyxNLOJ/DNek1WzCIyzHzvn5+Z6xVtK/XhAUWZPVJ5r7TMlfsJ0OX/a6LatUnIs1eYs7KERQR2gmAsE2g4PWIqocyRRIukA0ClFVCDJCLBKENAasCjJGS+f8z86TqsykH67IVhVQnqANVwjCRxAMQXKEOECQIUJaO8v6mSBmiGVGzbsiR0xjMkWWOH+9e9eSWbZJldkNs1HNLVMLGbQmQGhqskoTaNPsYVsnPGiD0jpz1r73jb9mT5eue+jO5z8y398fT+4PQ873Z7P7A9fzDnjA9z/c3h4FUbQ/c92Dv29ujkQY/jAaDn8KGNu/G48P78YfjhhjT/8B42rNSby+gN4AAAAASUVORK5CYII=&amp;#x27;);background-size:cover;display:block&quot;&gt;&lt;/span&gt;
  &lt;img class=&quot;gatsby-resp-image-image&quot; alt=&quot;png&quot; title=&quot;png&quot; src=&quot;/static/599ae3a41fde39efa22e75626a02c479/f058b/output_156_0.png&quot; srcSet=&quot;/static/599ae3a41fde39efa22e75626a02c479/c26ae/output_156_0.png 158w,/static/599ae3a41fde39efa22e75626a02c479/6bdcf/output_156_0.png 315w,/static/599ae3a41fde39efa22e75626a02c479/f058b/output_156_0.png 630w,/static/599ae3a41fde39efa22e75626a02c479/6bbf7/output_156_0.png 716w&quot; sizes=&quot;(max-width: 630px) 100vw, 630px&quot; style=&quot;width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0&quot; loading=&quot;lazy&quot; decoding=&quot;async&quot;/&gt;
  &lt;/a&gt;
    &lt;/span&gt;&lt;/p&gt;&lt;p&gt;Confussion Matrix prediksian model terhadap data valid per - &lt;code&gt;foldnya&lt;/code&gt;&lt;/p&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;# Plotting Confussion Matrix
fig, ax = plt.subplots(1, 2, figsize = (14, 6))
i = 0
for j in range(10):
    if j in [2, 5]:
        sns.heatmap(CMS[i], annot=True, cmap = plt.cm.Blues, ax = ax[i])
        ax[i].set_title(f&amp;#x27;Normalized Confussion Matrix Valid Data Fold {j + 1}&amp;#x27;)
        ax[i].set_xlabel(&amp;#x27;Predicted&amp;#x27;)
        ax[i].set_ylabel(&amp;#x27;Actual&amp;#x27;)
        i += 1
fig.show()
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;span class=&quot;gatsby-resp-image-wrapper&quot; style=&quot;position:relative;display:block;margin-left:auto;margin-right:auto;max-width:630px&quot;&gt;
      &lt;a class=&quot;gatsby-resp-image-link&quot; href=&quot;/static/347db183fa9c90059caa30a5a3489834/97655/output_158_0.png&quot; style=&quot;display:block&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;
    &lt;span class=&quot;gatsby-resp-image-background-image&quot; style=&quot;padding-bottom:47.46835443037975%;position:relative;bottom:0;left:0;background-image:url(&amp;#x27;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAJCAYAAAAywQxIAAAACXBIWXMAAAsSAAALEgHS3X78AAACrUlEQVQoz02PW0jTARjFv8xpYpb2Uk8RiYROzUwzMLDU1KCHAt8y0tJkpjN1EdpD0UPhhYK5oXPpnGmGmF3AEkzTdG5zKkoTUue8LAunWabM7X878V8vPRw+OPw43znUOrW6v3nMsa2zONA+uYLCTisSnxiQrDQiuda0EnxZXZ6Qlu1bPzRLjRZHw7NRB/Tj31AzYEea2oyUWhPS6yyIvPNWT+GlUqp+MxyiMy86tQYb2saX2FsdU1xizWcmVWnA2afDc0eK35UlXMyWdNpcpDXa1VrjAppMdk91/xyXqjRw50XVjrAnH/ZpAjMbIkjZPx2sGVlYqxueh868wBd0WoUk1SiXXj+G1DqL7Uz1oOJcZo6fxrhIaoNdrTbYUT9iZx/3zQkXGsaFDM2YkKEZ55NrTdrY8ldSululOvTJ9tPZO+OEyfGbl+sMCM3X89G3XyCyqG3+SrOlTP++2xcAfZxbU/XOrGFwYYPVj9gRW9ouRBeLXCuf8qhHW9o9E05fpr8eYHms7TACAPCKqk7Q4av8nmgZdkfk2R50jSmGTB/8xECGh8rNihhYi3UZgTEFgq/0JijsupAkq9P2/NiW0vJ3Z7CLEVbXN10iyChqulgKu+EJOV0CSbRstm1gphTweBtue3jVposBwwtus9XBBsXL2X3xxayPNJ+5VNak2QLCaeOPSwz0bLr+vVbUdIFCc7A3Tg5JlGy1b2Lp3sai0dvQxQiN227ey5mtDgTFyRFwohAUnoesipZWAFHk5hCww0K56WKfA9CVVHa00NHspsDYwlZJlKxy0raaDmCXGLjDImvLzb0E0GiYtOuD4oqa/Y8X6OhYblPu/bZsAAfJO8XNea8oIvKhmEoKPKUgSWQ+TdjWRY9YjqNfWzv/cxT1GuQfU0QkLaRrFc1e/y8zkMkwXho3pQAAAABJRU5ErkJggg==&amp;#x27;);background-size:cover;display:block&quot;&gt;&lt;/span&gt;
  &lt;img class=&quot;gatsby-resp-image-image&quot; alt=&quot;png&quot; title=&quot;png&quot; src=&quot;/static/347db183fa9c90059caa30a5a3489834/f058b/output_158_0.png&quot; srcSet=&quot;/static/347db183fa9c90059caa30a5a3489834/c26ae/output_158_0.png 158w,/static/347db183fa9c90059caa30a5a3489834/6bdcf/output_158_0.png 315w,/static/347db183fa9c90059caa30a5a3489834/f058b/output_158_0.png 630w,/static/347db183fa9c90059caa30a5a3489834/97655/output_158_0.png 819w&quot; sizes=&quot;(max-width: 630px) 100vw, 630px&quot; style=&quot;width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0&quot; loading=&quot;lazy&quot; decoding=&quot;async&quot;/&gt;
  &lt;/a&gt;
    &lt;/span&gt;&lt;/p&gt;&lt;h2 id=&quot;ensemble&quot;&gt;Ensemble&lt;/h2&gt;&lt;blockquote&gt;&lt;p&gt;Ensemble is methods use multiple learning algorithms to obtain better predictive performance than could be obtained from any of the constituent learning algorithms alone.&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;Untuk mendapatkan hasil yang lebih baik kami melakukan &lt;code&gt;ensembling&lt;/code&gt; pada hasil prediksi model kami. Hasil model yang di &lt;code&gt;ensembling&lt;/code&gt; adalah hasil peramalan data test pada model &lt;code&gt;EfficientNetB7&lt;/code&gt; dan model &lt;code&gt;Bert&lt;/code&gt; pada fold &lt;code&gt;3&lt;/code&gt; dan &lt;code&gt;6&lt;/code&gt;. Metode ensemble yang digunakan adalah dengan mengambil rata - rata probabilitas dari hasil prediksian ketiga model tersebut.&lt;/p&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;ensemble = (EfficientNet_pred.flatten() + scores_test[0] + scores_test[1]) / 3
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&quot;mencari-threshold&quot;&gt;Mencari Threshold&lt;/h2&gt;&lt;blockquote&gt;&lt;p&gt;The decision for converting a predicted probability or scoring into a class label is governed by a parameter referred to as the ‚Äúthreshold‚Äù The default value for the threshold is 0.5 for normalized predicted probabilities or scores in the range between 0 or 1.&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;Kami mendapatkan beberapa indikasi bahwa untuk mendapatkan hasil yang lebih baik perlu dilakukan penggeseran terhadap tresholdnya:&lt;/p&gt;&lt;ol&gt;&lt;li&gt;Kelas pada label yang tidak berimbang&lt;br/&gt;
Kelas &lt;code&gt;0&lt;/code&gt; pada label memiliki frekuensi yang sangat kecil, berbeda jauh jika dibandingkan dengan pada kelas &lt;code&gt;1&lt;/code&gt;. Kami menduga hal ini berpengaruh terhadap berat masing - masing kelas pada saat dilakukan pemodelan &lt;code&gt;tidak sama&lt;/code&gt;.&lt;/li&gt;&lt;li&gt;Nilai loss dan akurasi yang berbanding lurus pada saat training&lt;br/&gt;
Pada saat training kami mendapatkan ketika &lt;code&gt;val_loss&lt;/code&gt; (validation loss) menaik &lt;code&gt;val_accuracy&lt;/code&gt; juga tetap menaik untuk beberapa saat. Hal ini cukup aneh karena seharusnya ketika nilai loss menaik maka nilai akurasi menurun. Hal ini membawa kami pada kesimpulan bahwa &lt;code&gt;threshold&lt;/code&gt;-nya perlu untuk dirubah.&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;Mengecek distribusi peluang &lt;code&gt;ensemble&lt;/code&gt;&lt;/p&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;plt.figure(figsize = (8,6))
sns.distplot(ensemble)
plt.title(&amp;#x27;Distribusi Sebaran Peluang Prediksi&amp;#x27;, fontsize = 14)
plt.show()
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;span class=&quot;gatsby-resp-image-wrapper&quot; style=&quot;position:relative;display:block;margin-left:auto;margin-right:auto;max-width:484px&quot;&gt;
      &lt;a class=&quot;gatsby-resp-image-link&quot; href=&quot;/static/2023d3371a898bd7709f417b7372fcb3/ff42b/output_163_0.png&quot; style=&quot;display:block&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;
    &lt;span class=&quot;gatsby-resp-image-background-image&quot; style=&quot;padding-bottom:77.84810126582278%;position:relative;bottom:0;left:0;background-image:url(&amp;#x27;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAQCAYAAAAWGF8bAAAACXBIWXMAAAsSAAALEgHS3X78AAACK0lEQVQ4y62Tz2sTQRTHB8zGHxCldtNsfhjaWtFLoR7qIbWoFy968m8wED3kFOJJCISYXBJ/QCKW6EEUPUVB9KheLB4UJeDBtJZNZm1KdrM7u9ndZHcyu7JNo+KPQ0kGHt8vM48P7w3vgUgkAorFIlUqldzlcpnK5/PuYDA47fP5jjnq9/tnHR8KhcKOMgwzFwgEZrxe74lwOHzkxctXrscP7lFvnz9yF27foUClUvHIsvyu2+3WNU2r6bq+JssyK0lSfRjtdhsOvSAInKOiKDYQQqyT31HVGuqodUWR34BoNBrCGPP2CEc3+7agmrZF+hzIZrO0aZrQebAsC1uWRXYTtm0TrYdxS+nZpI/XQTwe95um+X0HSHZTmWVZ26oZmOwAN0Amk/GOFRiLxQLjAPIdYwBMp9MjV6j2MIGiPgAmk0nfqECla5IqhwbAXC5HjxWYSCSYUYFIN8mnhmRbDjCVSv31h8NER3/3f8J+AjWDfGTFX0DDMLh/DTb5zzAP7wkZDLakGfgzlJyWv4FC4SaNMW6NsnqsoNlVTnZWD4JrySQtIfRBUtQtHin1TUGGHI+go1W2BSGPoIA6sNmWt4PdEuGXBg83miJc32zDr5wAn6yu1d/Xmi1BlFYBfXhiT2nl/r7jC6eYi1eu719OPXN7ZhemAkuXPIC6QXnmFun50+cnLr/uu5j5pUkQOjkJ6LsuMHP2IDh0lAGLt6jpC1cPLJ85N7Xy8OneH2fdSSvhaB+UAAAAAElFTkSuQmCC&amp;#x27;);background-size:cover;display:block&quot;&gt;&lt;/span&gt;
  &lt;img class=&quot;gatsby-resp-image-image&quot; alt=&quot;png&quot; title=&quot;png&quot; src=&quot;/static/2023d3371a898bd7709f417b7372fcb3/ff42b/output_163_0.png&quot; srcSet=&quot;/static/2023d3371a898bd7709f417b7372fcb3/c26ae/output_163_0.png 158w,/static/2023d3371a898bd7709f417b7372fcb3/6bdcf/output_163_0.png 315w,/static/2023d3371a898bd7709f417b7372fcb3/ff42b/output_163_0.png 484w&quot; sizes=&quot;(max-width: 484px) 100vw, 484px&quot; style=&quot;width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0&quot; loading=&quot;lazy&quot; decoding=&quot;async&quot;/&gt;
  &lt;/a&gt;
    &lt;/span&gt;&lt;/p&gt;&lt;p&gt;Dapat dilihat dari plot diatas bahwa sebaran peluang mulai memusat pada selang &lt;code&gt;0.65&lt;/code&gt; - &lt;code&gt;1&lt;/code&gt;.&lt;/p&gt;&lt;p&gt;Maka dari itu kami putuskan untuk mengambil &lt;code&gt;0.65&lt;/code&gt; sebagai &lt;code&gt;threshold&lt;/code&gt;-nya.&lt;/p&gt;&lt;h2 id=&quot;membuat-submission&quot;&gt;Membuat Submission&lt;/h2&gt;&lt;p&gt;Membuat Submission File.&lt;/p&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;test[&amp;#x27;prediksi&amp;#x27;] = np.array(ensemble &amp;gt; .65, dtype = &amp;#x27;int&amp;#x27;)
submission_temp = pd.read_csv(&amp;#x27;../input/data-bdc/Submission Template.csv&amp;#x27;)[[&amp;#x27;ID&amp;#x27;]]
hasil = submission_temp.merge(test[[&amp;#x27;ID&amp;#x27;, &amp;#x27;prediksi&amp;#x27;]], on = &amp;#x27;ID&amp;#x27;)
hasil.head()
&lt;/code&gt;&lt;/pre&gt;&lt;table&gt;&lt;thead&gt;&lt;tr&gt;&lt;th align=&quot;center&quot;&gt;ID&lt;/th&gt;&lt;th align=&quot;center&quot;&gt;prediksi&lt;/th&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td align=&quot;center&quot;&gt;56&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;1&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;center&quot;&gt;1129&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;1&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;center&quot;&gt;8468&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;1&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;center&quot;&gt;9527&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;1&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;center&quot;&gt;11152&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;1&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;hasil.to_csv(&amp;#x27;Catatan Cakrawala BDC - Ensemble.csv&amp;#x27;, index = False)
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&quot;kesimpulan--saran&quot;&gt;Kesimpulan &amp;amp; Saran&lt;/h2&gt;&lt;h3 id=&quot;kesimpulan&quot;&gt;Kesimpulan&lt;/h3&gt;&lt;ol&gt;&lt;li&gt;Data memiliki frekuensi kelas yang &lt;code&gt;tidak berimbang&lt;/code&gt; sehingga perlu di perhatikan saat pembuatan model&lt;/li&gt;&lt;li&gt;Dengan menggukan model yang kami buat didapatkan hasil yang cukup baik dengan f1 score 94 % namun memerlukan komputasi yang cukup berat dan biaya yang mahal untuk ukuran data yang tidak terlalu besar.&lt;/li&gt;&lt;/ol&gt;&lt;h3 id=&quot;saran&quot;&gt;Saran&lt;/h3&gt;&lt;ol&gt;&lt;li&gt;Jika memungkinkan diperlukan &lt;code&gt;penambahan data&lt;/code&gt; pada label bukan hoax agar frekuensi setiap kelas pada data tidak berbanding cukup jauh&lt;/li&gt;&lt;li&gt;Menggunakan model &lt;code&gt;multimodal&lt;/code&gt; karena hasil pada ensembel model gambar dan model teks menunjukkan hasil yang lebih baik.&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;¬© Catatan Cakrawala 2020&lt;/p&gt;</content:encoded></item><item><title><![CDATA[MNIST Digit Classifier Using Keras, Tensorflow, and TPU]]></title><description><![CDATA[This notebook is basically my notebook run on  kaggle  so if you want to try
and run the code with same environment as mine go to link‚Ä¶]]></description><link>https://Hyuto.github.io/cnn-keras-cv-0-996-tpu/</link><guid isPermaLink="false">https://Hyuto.github.io/cnn-keras-cv-0-996-tpu/</guid><pubDate>Tue, 25 Aug 2020 15:56:36 GMT</pubDate><content:encoded>&lt;p&gt;This notebook is basically my notebook run on &lt;a href=&quot;https://www.kaggle.com/&quot;&gt;kaggle&lt;/a&gt; so if you want to try
and run the code with same environment as mine go to link bellow.&lt;/p&gt;&lt;p&gt;Kaggle Notebook : &lt;a href=&quot;https://www.kaggle.com/wahyusetianto/cnn-keras-cv-0-996-tpu&quot;&gt;CNN Keras CV - 0.996 [TPU]&lt;/a&gt;&lt;/p&gt;&lt;p&gt;P.S. Don&amp;#x27;t forget to &lt;em&gt;upvote&lt;/em&gt; if you like it üòä.&lt;/p&gt;&lt;h2 id=&quot;overview&quot;&gt;Overview&lt;/h2&gt;&lt;p&gt;This kernel is on purpose to build model for MNIST digits dataset. In this kernel we&amp;#x27;re gonna do some preprocessing then make augmentation datagen so our model didn&amp;#x27;t train on the same image data, then at each of our model(here we use 15 folds so there will be 15 models) to make prediction for test dataset and by the end we&amp;#x27;re gonna do ensembles for the prediction.&lt;/p&gt;&lt;h2 id=&quot;web-app&quot;&gt;Web App&lt;/h2&gt;&lt;p&gt;You can visit my web app for the live prediction by the best model trained on this kernel run on Tensorflow.js &lt;a href=&quot;https://hyuto.github.io/showcase/digit-recognizer/&quot;&gt;Digit Recognizer&lt;/a&gt;.&lt;/p&gt;&lt;h2 id=&quot;train-on-tpu&quot;&gt;Train on TPU!!&lt;/h2&gt;&lt;p&gt;why? Because it&amp;#x27;s &lt;strong&gt;faster&lt;/strong&gt;. While people usually train on GPU for image related things, in this notebook we try to do things on TPU and see how it affect the Accuracy.&lt;/p&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;import random
import numpy as np
import pandas as pd
from timeit import default_timer
from sklearn.model_selection import KFold

# Plot
import matplotlib.pyplot as plt
import seaborn as sns

# Tensorflow and Keras
import tensorflow as tf
from tensorflow import keras
import tensorflow.keras.backend as K
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.models import Sequential, load_model
from tensorflow.keras.layers import Dense, Dropout, Flatten, Conv2D
from tensorflow.keras.layers import MaxPool2D, BatchNormalization
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.callbacks import LearningRateScheduler, ModelCheckpoint

print(f&amp;#x27;Using Tensorflow Version : {tf.__version__}&amp;#x27;)
print(f&amp;#x27;Using Keras Version      : {keras.__version__}&amp;#x27;)
&lt;/code&gt;&lt;/pre&gt;&lt;pre&gt;&lt;code&gt;Using Tensorflow Version : 2.2.0
Using Keras Version      : 2.3.0-tf
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&quot;load-the-data&quot;&gt;Load the Data&lt;/h2&gt;&lt;p&gt;Load the MNIST data&lt;/p&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;train = pd.read_csv(&amp;#x27;../input/digit-recognizer/train.csv&amp;#x27;)
test = pd.read_csv(&amp;#x27;../input/digit-recognizer/test.csv&amp;#x27;)
train.head()
&lt;/code&gt;&lt;/pre&gt;&lt;pre&gt;&lt;code&gt;5 rows √ó 785 columns
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&quot;preprocess&quot;&gt;Preprocess&lt;/h2&gt;&lt;h3 id=&quot;specifying-x-and-y&quot;&gt;Specifying X and y&lt;/h3&gt;&lt;p&gt;for y we need to encode to one hot vector. In keras we have function &lt;code&gt;to_categorical&lt;/code&gt; for this.&lt;/p&gt;&lt;p&gt;Example :&lt;/p&gt;&lt;pre&gt;&lt;code&gt;y = [1, 0, 4]
to_categorical(y)

# Output:
[
 [0,1,0,0,0], # 1
 [1,0,0,0,0], # 0
 [0,0,0,0,1], # 4
]
&lt;/code&gt;&lt;/pre&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;# Specifying X and y
X = train.drop([&amp;#x27;label&amp;#x27;], axis = 1)
y = to_categorical(train[&amp;#x27;label&amp;#x27;].values) # To Categorical y
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&quot;normalizing-images&quot;&gt;Normalizing Images&lt;/h3&gt;&lt;p&gt;Data normalization is an important step to ensures that each input parameter (pixel, in this case) has a similar data distribution. This makes convergence faster while training the network.&lt;/p&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;# Normalize
X = X / 255.0
X_test = test / 255.0
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&quot;reshape&quot;&gt;Reshape&lt;/h3&gt;&lt;p&gt;Train and test images (28px x 28px) has been stock into pandas. Dataframe as 1D vectors of 784 values. We reshape all data to 28x28x1 3D matrices.&lt;/p&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;# Reshape Array
X = X.values.reshape(-1, 28, 28, 1)
X_test = X_test.values.reshape(-1, 28, 28, 1)
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&quot;lets-take-a-look-at-our-data&quot;&gt;Let&amp;#x27;s take a look at our data&lt;/h3&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;fig, axes = plt.subplots(ncols=10, nrows=5, figsize = (13, 7))
init = 0
for i in range(5):
    j = 0
    for k in range(2):
        ind = random.choices(train.label[train.label == init].index, k = 5)
        init += 1
        while j &amp;lt; len(ind):
            axes[i, k*5 + j].imshow(X[ind[j]][:,:,0], cmap=plt.cm.binary)
            axes[i, k*5 + j].axis(&amp;#x27;off&amp;#x27;)
            j += 1
        j = 0
fig.tight_layout()
plt.show()
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;span class=&quot;gatsby-resp-image-wrapper&quot; style=&quot;position:relative;display:block;margin-left:auto;margin-right:auto;max-width:630px&quot;&gt;
      &lt;a class=&quot;gatsby-resp-image-link&quot; href=&quot;/static/0995ecc1e7de346086519e39d71607ea/cc488/output_12_0.png&quot; style=&quot;display:block&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;
    &lt;span class=&quot;gatsby-resp-image-background-image&quot; style=&quot;padding-bottom:52.53164556962025%;position:relative;bottom:0;left:0;background-image:url(&amp;#x27;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAALCAYAAAB/Ca1DAAAACXBIWXMAAAsSAAALEgHS3X78AAADBElEQVQozxWS3VPiVgDF869Xtzq7XTu7+1B37IMz4kpgpUZnG7aZKhgRhQQDCIR8EGgwueTrkhByEyaB28n775w558wh2u32axiGZLPZHGRZVqEoqj+fz8nRaCQlSfK93W4rcRwXJ5OJGEVRxfO81+VyWXIcp79er/+CEA4AAH8jhB4URekTz8/PQJblnzzPW/1+/+7k5MS0bfsnx3EQQvhvo9HwN5tNVZZlZ7Va3em6DrbbLaPrupll2Z2iKAuEUFNRlKHruoAQBKEEADjgeb4MAPggSRIZx/GBqqoVjPH7l5eXq81mcyAIwnfLsj64rktijA91XSdN0/wtDMOSqqqfkiT5M9cSxWIxGo/HD6enp8iyrNbV1VVo2/YDTdOp4zhNkiS3q9WKvbi42Hie15pOp+s4jhvD4TDMeZ7nc32/0+noPM+viSRJClEU7bmue75arQ4AAGfb7XYPQlhcLBbvDMMoxXH8i67rFxjjg5xHCO0HQXCWN/F9v4Ax/mhZ1h+maZ4RNE3jKIo6giDgbrf7enx8jCVJ6pTLZRwEQY9hGAwh5HNusVi8DodD7HmeMBqNsGmag9lshgEAqiiKHsdxmJhOp+e73W7fNM1v+Tbdbrdg2/a+ruulLMt+dV23nKbp3mQyKZqmeZg32e1278IwLKRpehgEwTfP845Go9GxZVnnBM/zvizLNZIkA9/3H2mahgihWqFQQMvlkm21WglC6I5l2bWmaQ2WZZeaptXH4zFUVfUxf8FgMOh0Op1JvV5fEpqmXWuadsRx3A2E8HOv16PSND2iKIre7XafFEWpRlH0UVXVH0mSfJZl+TqKot8dx6Ewxl8Mw7gxDOPrfD4vvL29XROz2UzDGOem0/V6Td/f36tJklD1ev0tDMMf1WrVsCyLEkXxvyiK6DwJhPCmUqmoDMPQiqJos9ks/yPPsuwkT2j6vl+VJAkEQXBbq9WMNE2rDMO4nucxl5eXMMsymuM4GyF022q1TNd1/6Eoynh6eroVRRE4jvPY6/UGsiwb/wOjpOMVzbCcsQAAAABJRU5ErkJggg==&amp;#x27;);background-size:cover;display:block&quot;&gt;&lt;/span&gt;
  &lt;img class=&quot;gatsby-resp-image-image&quot; alt=&quot;png&quot; title=&quot;png&quot; src=&quot;/static/0995ecc1e7de346086519e39d71607ea/f058b/output_12_0.png&quot; srcSet=&quot;/static/0995ecc1e7de346086519e39d71607ea/c26ae/output_12_0.png 158w,/static/0995ecc1e7de346086519e39d71607ea/6bdcf/output_12_0.png 315w,/static/0995ecc1e7de346086519e39d71607ea/f058b/output_12_0.png 630w,/static/0995ecc1e7de346086519e39d71607ea/cc488/output_12_0.png 928w&quot; sizes=&quot;(max-width: 630px) 100vw, 630px&quot; style=&quot;width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0&quot; loading=&quot;lazy&quot; decoding=&quot;async&quot;/&gt;
  &lt;/a&gt;
    &lt;/span&gt;&lt;/p&gt;&lt;p&gt;as you can see there some nice &amp;amp; bad hand written digits number at our dataset.&lt;/p&gt;&lt;h2 id=&quot;data-augmentation&quot;&gt;Data Augmentation&lt;/h2&gt;&lt;p&gt;we currently have about 42000 image data, let&amp;#x27;s multiply that value by doing some soft augmentation.&lt;/p&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;datagen = ImageDataGenerator(rotation_range=10,
                             zoom_range = 0.10,
                             width_shift_range=0.1,
                             height_shift_range=0.1)
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;let&amp;#x27;s take a look on our Augmentation datagen&lt;/p&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;def AUG_test(X, y):
    fig, axes = plt.subplots(1, 5, figsize = (10,5))
    axes[0].imshow(X[:,:,0], cmap=plt.cm.binary)
    axes[0].set_title(&amp;#x27;Actual&amp;#x27;)
    axes[0].axis(&amp;#x27;off&amp;#x27;)
    for i in range(1, 5):
        aug, _ = datagen.flow(X.reshape(1,28,28,1), y.reshape(1,10)).next()
        axes[i].imshow(aug.reshape(28,28),cmap=plt.cm.binary)
        axes[i].set_title(&amp;#x27;Augmented&amp;#x27;)
        axes[i].axis(&amp;#x27;off&amp;#x27;)
    fig.tight_layout()
    return plt.show()
&lt;/code&gt;&lt;/pre&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;AUG_test(X[0], y[0])
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;span class=&quot;gatsby-resp-image-wrapper&quot; style=&quot;position:relative;display:block;margin-left:auto;margin-right:auto;max-width:630px&quot;&gt;
      &lt;a class=&quot;gatsby-resp-image-link&quot; href=&quot;/static/710183c6067410e8862ad6a60e749cd2/3d4b6/output_18_0.png&quot; style=&quot;display:block&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;
    &lt;span class=&quot;gatsby-resp-image-background-image&quot; style=&quot;padding-bottom:22.78481012658228%;position:relative;bottom:0;left:0;background-image:url(&amp;#x27;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAFCAYAAABFA8wzAAAACXBIWXMAAAsSAAALEgHS3X78AAABPklEQVQY0zWPwUsCURDG908KokuXDh27RYfskuXfE54CxYuXlA6BmBIqhRt4adNyfT1SWFAQBDVCAtd1ffOLF3aZ+Wa+b+abcbTWifF4fAYcAntKqYTruqnFYnEK7M7n86Nms5kKgiAZhuEBsO95XlIpddHv94+BneFweNJqtS611ucOQLvdRin1DTRtHccx3W7XwkcR+bHA933W67UG3m1dr9f/Nfc2hGEo1WoVJwgC0uk0xpgJ0LBkNpuNPc+z8AH40lpLoVBgu+ylXC5TLBZjIDLG3AGSy+WiTqeDk8lkmM1mVrwAnhuNBnZge+kTsMzn80wm1o/P6XTq12q1P15EbKoMBgNKpdJfzxmNRr513mw2j0Cm1+t1V6vV6/aa6+Vy6VYqlQ7wYYy5BW4sFpFXEbFvXEVR9CYiL9bwF7WcWuUXOXHlAAAAAElFTkSuQmCC&amp;#x27;);background-size:cover;display:block&quot;&gt;&lt;/span&gt;
  &lt;img class=&quot;gatsby-resp-image-image&quot; alt=&quot;png&quot; title=&quot;png&quot; src=&quot;/static/710183c6067410e8862ad6a60e749cd2/f058b/output_18_0.png&quot; srcSet=&quot;/static/710183c6067410e8862ad6a60e749cd2/c26ae/output_18_0.png 158w,/static/710183c6067410e8862ad6a60e749cd2/6bdcf/output_18_0.png 315w,/static/710183c6067410e8862ad6a60e749cd2/f058b/output_18_0.png 630w,/static/710183c6067410e8862ad6a60e749cd2/3d4b6/output_18_0.png 712w&quot; sizes=&quot;(max-width: 630px) 100vw, 630px&quot; style=&quot;width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0&quot; loading=&quot;lazy&quot; decoding=&quot;async&quot;/&gt;
  &lt;/a&gt;
    &lt;/span&gt;&lt;/p&gt;&lt;h2 id=&quot;build-cnn-model&quot;&gt;Build CNN Model&lt;/h2&gt;&lt;p&gt;The CNN&amp;#x27;s in this kernel follow LeNet5&amp;#x27;s design on &lt;a href=&quot;https://www.kaggle.com/cdeotte&quot;&gt;Chris Deotte&lt;/a&gt; kernel &lt;a href=&quot;https://www.kaggle.com/cdeotte/25-million-images-0-99757-mnist&quot;&gt;here&lt;/a&gt;&lt;/p&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;def build_model():
    model = Sequential()
    model.add(Conv2D(32, kernel_size = 3, activation=&amp;#x27;relu&amp;#x27;,
                     input_shape = (28, 28, 1)))
    model.add(BatchNormalization())
    model.add(Conv2D(32, kernel_size = 3, activation=&amp;#x27;relu&amp;#x27;))
    model.add(BatchNormalization())
    model.add(Conv2D(32, kernel_size = 5, strides=2, padding=&amp;#x27;same&amp;#x27;,
                     activation=&amp;#x27;relu&amp;#x27;))
    model.add(BatchNormalization())
    model.add(Dropout(0.4))

    model.add(Conv2D(64, kernel_size = 3, activation=&amp;#x27;relu&amp;#x27;))
    model.add(BatchNormalization())
    model.add(Conv2D(64, kernel_size = 3, activation=&amp;#x27;relu&amp;#x27;))
    model.add(BatchNormalization())
    model.add(Conv2D(64, kernel_size = 5, strides=2, padding=&amp;#x27;same&amp;#x27;,
                     activation=&amp;#x27;relu&amp;#x27;))
    model.add(BatchNormalization())
    model.add(Dropout(0.4))

    model.add(Conv2D(128, kernel_size = 4, activation=&amp;#x27;relu&amp;#x27;))
    model.add(BatchNormalization())
    model.add(Flatten())
    model.add(Dropout(0.4))
    model.add(Dense(10, activation=&amp;#x27;softmax&amp;#x27;))

    model.compile(optimizer=&amp;quot;adam&amp;quot;, loss=&amp;quot;categorical_crossentropy&amp;quot;,
                  metrics=[&amp;quot;accuracy&amp;quot;])

    return model
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&quot;detect-and-instantiate-tpu-distribution-strategy&quot;&gt;Detect and Instantiate TPU Distribution Strategy&lt;/h2&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;# detect and init the TPU
tpu = tf.distribute.cluster_resolver.TPUClusterResolver()
tf.config.experimental_connect_to_cluster(tpu)
tf.tpu.experimental.initialize_tpu_system(tpu)

# instantiate a distribution strategy
tpu_strategy = tf.distribute.experimental.TPUStrategy(tpu)
&lt;/code&gt;&lt;/pre&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;# Config
EPOCHS = 45
BATCH_SIZE = 16 * tpu_strategy.num_replicas_in_sync
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&quot;training&quot;&gt;Training&lt;/h2&gt;&lt;p&gt;Here we use Kfold CV to split our data by 15 and build model at each fold. At the callbacks we use &lt;span class=&quot;math math-inline&quot;&gt;f(x) = 0.001 \times 0.95^x&lt;/span&gt; for our LR Scheduler and do Checkpoint at &lt;strong&gt;best &lt;em&gt;val_acc&lt;/em&gt;&lt;/strong&gt; score.&lt;/p&gt;&lt;p&gt;Note that Tensorflow distribution strategy haven&amp;#x27;t supported &lt;code&gt;ImageDataGenerator&lt;/code&gt; by this &lt;a href=&quot;https://github.com/tensorflow/tensorflow/issues/34346&quot;&gt;issue&lt;/a&gt; so instead use that at &lt;code&gt;fit_generator&lt;/code&gt; we just have to extract Augmentation data by looping through and done training by &lt;code&gt;fit&lt;/code&gt;.&lt;/p&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;# Init
scores, History = [], []
pred = np.zeros(shape = (len(test), 10))

# CV
cv = KFold(n_splits=15, shuffle = True, random_state = 42)
for fold, (train_index, val_index) in enumerate(cv.split(X)):
    start = default_timer()
    # Clear Session
    K.clear_session()
    tf.tpu.experimental.initialize_tpu_system(tpu)

    # Splitting
    X_train , y_train = X[train_index], y[train_index]
    X_val, y_val = X[val_index], y[val_index]

    # Augmentation
    Train_x, Train_y = None, None
    batch = 0
    for x_batch, y_batch in datagen.flow(X_train, y_train,
                                         batch_size=BATCH_SIZE):
        if batch == 0:
            Train_x, Train_y = x_batch, y_batch
        elif batch &amp;gt;= X.shape[0] // BATCH_SIZE:
            break
        else:
            Train_x = np.concatenate((Train_x, x_batch))
            Train_y = np.concatenate((Train_y, y_batch))
        batch += 1

    # Model
    with tpu_strategy.scope():
        model = build_model()

    # Callbacks
    annealer = LearningRateScheduler(lambda x: 1e-3 * 0.95 ** x) # LR
    sv = ModelCheckpoint(f&amp;#x27;Model Fold {fold}.h5&amp;#x27;, monitor=&amp;#x27;val_accuracy&amp;#x27;,
                         save_best_only=True, mode=&amp;#x27;max&amp;#x27;)

    # Training
    history = model.fit(Train_x, Train_y, batch_size = BATCH_SIZE,
                        epochs = EPOCHS, verbose = 0, callbacks=[annealer, sv],
                        steps_per_epoch = X_train.shape[0]//BATCH_SIZE,
                        validation_data = (X_val, y_val))
    History.append(history.history)

    # Load best model
    model = load_model(f&amp;#x27;Model Fold {fold}.h5&amp;#x27;)

    # Evaluate
    score = model.evaluate(X_val, y_val, verbose = 0)[1]
    scores.append(score)

    # Making Prediction
    pred += model.predict(X_test)

    time = round(default_timer() - start, 4)
    print(f&amp;#x27;[INFO] Fold {fold + 1} val_accuracy : {round(score, 4)} - Time : {time} s&amp;#x27;)

print()
print(f&amp;#x27;[INFO] Mean CV scores : {round(sum(scores)/len(scores), 4)}&amp;#x27;)
&lt;/code&gt;&lt;/pre&gt;&lt;pre&gt;&lt;code&gt;[INFO] Fold 1 val_accuracy : 0.9961 - Time : 168.7797 s
[INFO] Fold 2 val_accuracy : 0.9957 - Time : 173.7161 s
[INFO] Fold 3 val_accuracy : 0.9986 - Time : 178.845 s
[INFO] Fold 4 val_accuracy : 0.9957 - Time : 174.5829 s
[INFO] Fold 5 val_accuracy : 0.9946 - Time : 173.8493 s
[INFO] Fold 6 val_accuracy : 0.9979 - Time : 175.585 s
[INFO] Fold 7 val_accuracy : 0.9964 - Time : 176.5861 s
[INFO] Fold 8 val_accuracy : 0.9968 - Time : 177.6894 s
[INFO] Fold 9 val_accuracy : 0.995 - Time : 179.8059 s
[INFO] Fold 10 val_accuracy : 0.9943 - Time : 176.4369 s
[INFO] Fold 11 val_accuracy : 0.9939 - Time : 182.6052 s
[INFO] Fold 12 val_accuracy : 0.9961 - Time : 177.0005 s
[INFO] Fold 13 val_accuracy : 0.995 - Time : 180.026 s
[INFO] Fold 14 val_accuracy : 0.9954 - Time : 179.1445 s
[INFO] Fold 15 val_accuracy : 0.9975 - Time : 175.2097 s

[INFO] Mean CV scores : 0.9959
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;And so we&amp;#x27;ve got really great CV score there. Let&amp;#x27;s check the Training History.&lt;/p&gt;&lt;h2 id=&quot;history&quot;&gt;History&lt;/h2&gt;&lt;div class=&quot;tabbed&quot;&gt;
  &lt;span class=&quot;gatsby-resp-image-wrapper&quot; style=&quot;position:relative;display:block;margin-left:auto;margin-right:auto;max-width:630px&quot;&gt;
      &lt;a class=&quot;gatsby-resp-image-link&quot; href=&quot;/static/7422a0fae641ba551fdb715a86950450/302a4/val_loss.png&quot; style=&quot;display:block&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;
    &lt;span class=&quot;gatsby-resp-image-background-image&quot; style=&quot;padding-bottom:66.45569620253164%;position:relative;bottom:0;left:0;background-image:url(&amp;#x27;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAANCAYAAACpUE5eAAAACXBIWXMAAAsSAAALEgHS3X78AAAB6klEQVQ4y6WT6XLbMAyE+/7Pl2mnmTpObEuWdVKUeJP6OmTiK/1ZaDArQOAKJBc/tm0jW0qJGOMThhCoqoqmaajruriUkrym1MSIDwHnPVeeH1cyrTXeW7TVxPhF+oXZM3n2EofPXCZR68K5rgkh3gnzR2sdwa/088DV4hZv796GglvaCO6ed9rStS0hhscOI84HZnGhOb5gQsQ6y+rXUpjdmETIxxEiRodCTAS1Kuq6Kt3fCLeUMNYyjzXt20/25z3VNDAukk4PaK24TAOHYUKsK51U9Iumyy4Fp9PpmTCmiDWWufmg3/3i+L6jb2uOh4pDfeHSCY6XE+00Fx+1RAjJOE5oZVm1IT5uOZ9h7nDpBGL3G3Vq8E2F37/Svu2o397xQ4MbWmxfE9oTrs1YMR//YLUipvSw5W3D5i3Pgu5jz9icmcaJrukZ63e61xfmpkUeT4yHM2JYmMeJZRRoKQnelKaeZGONwbgVoUbMuuD1gl0EVgqMNqyjQA0dS98hhwEjJ8wqUKIlOkP4TqiNIeVnSyxWYkKOIyZa/OaxUWOdLj91waDtijES7RTO2387zMIO/lO41ltccEU6zjuMM0Xsyip89CUXiuBTWet9uN/y4+j54G9TkW6YbhPyHb2/1z+N3jX4X8s8fwHNYfM7HIlj8gAAAABJRU5ErkJggg==&amp;#x27;);background-size:cover;display:block&quot;&gt;&lt;/span&gt;
  &lt;img class=&quot;gatsby-resp-image-image&quot; alt=&quot;validation loss&quot; title=&quot;validation loss&quot; src=&quot;/static/7422a0fae641ba551fdb715a86950450/f058b/val_loss.png&quot; srcSet=&quot;/static/7422a0fae641ba551fdb715a86950450/c26ae/val_loss.png 158w,/static/7422a0fae641ba551fdb715a86950450/6bdcf/val_loss.png 315w,/static/7422a0fae641ba551fdb715a86950450/f058b/val_loss.png 630w,/static/7422a0fae641ba551fdb715a86950450/40601/val_loss.png 945w,/static/7422a0fae641ba551fdb715a86950450/302a4/val_loss.png 1080w&quot; sizes=&quot;(max-width: 630px) 100vw, 630px&quot; style=&quot;width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0&quot; loading=&quot;lazy&quot; decoding=&quot;async&quot;/&gt;
  &lt;/a&gt;
    &lt;/span&gt;
  &lt;span class=&quot;gatsby-resp-image-wrapper&quot; style=&quot;position:relative;display:block;margin-left:auto;margin-right:auto;max-width:630px&quot;&gt;
      &lt;a class=&quot;gatsby-resp-image-link&quot; href=&quot;/static/db083aad0563651af90980c4e8ae20ae/302a4/val_accuracy.png&quot; style=&quot;display:block&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;
    &lt;span class=&quot;gatsby-resp-image-background-image&quot; style=&quot;padding-bottom:66.45569620253164%;position:relative;bottom:0;left:0;background-image:url(&amp;#x27;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAANCAYAAACpUE5eAAAACXBIWXMAAAsSAAALEgHS3X78AAABvklEQVQ4y6WTfW/bIBCH+/2/1/6vlqzNqrROHWNTNzZ2/AIY8DPZmb1kq6pNOwkh7uDh7vhxN44jk4UQ8N6vYzKlFEmSkOc5QghkJpFSrnuC/3Vm4dwtsL7v1+DgB6yzs18Pmt52OO/Qg6E1DdYNc8z6YU3EOXcL1EYTxoANlqavZ6AJlt5pjLOoqqY4lVRlRVXX1FWJqgvObYWzw1rVCrTGoE1LISV921PKE+9RTHmUiOeEUki65IU2OdIcj9THhDKNOb1G6LPCh6nk8QpoLUWakWy3iO1X5GaD2n0jftzwdP+FU/ZEkew57B+QMqJMXxCHHad4h+kafAhXGY4jRhvqvCJ9vCc/PCOi7+yTPbGMyfN3RPKCUA2yUhSlQrWat6Ihywt64wh/lGwH+ipHRA9kqiBTJXlV4kbQNnCuW856YKrMmECYE4H6rGm7Du9/exRjLZV6pcgjpm5YH+CihLk3zgbGn326uC6z7luyNL19lElDxhicNzRdO9+8+D8bk02tepPyVjYTXWs9X9oYz7+YtoY0yz7QodZzH7Ud5uAi1s/mOZGuJRWCYQFef71pY/gL0Efgm6+3LP7XJs4PIm3zZreLK4cAAAAASUVORK5CYII=&amp;#x27;);background-size:cover;display:block&quot;&gt;&lt;/span&gt;
  &lt;img class=&quot;gatsby-resp-image-image&quot; alt=&quot;validation accuracy&quot; title=&quot;validation accuracy&quot; src=&quot;/static/db083aad0563651af90980c4e8ae20ae/f058b/val_accuracy.png&quot; srcSet=&quot;/static/db083aad0563651af90980c4e8ae20ae/c26ae/val_accuracy.png 158w,/static/db083aad0563651af90980c4e8ae20ae/6bdcf/val_accuracy.png 315w,/static/db083aad0563651af90980c4e8ae20ae/f058b/val_accuracy.png 630w,/static/db083aad0563651af90980c4e8ae20ae/40601/val_accuracy.png 945w,/static/db083aad0563651af90980c4e8ae20ae/302a4/val_accuracy.png 1080w&quot; sizes=&quot;(max-width: 630px) 100vw, 630px&quot; style=&quot;width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0&quot; loading=&quot;lazy&quot; decoding=&quot;async&quot;/&gt;
  &lt;/a&gt;
    &lt;/span&gt;&lt;/div&gt;&lt;h2 id=&quot;ensembleing-predictions&quot;&gt;Ensembleing Predictions&lt;/h2&gt;&lt;p&gt;&lt;code&gt;np.argmax&lt;/code&gt; through the prediction to get the number of class&lt;/p&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;pred = np.array([np.argmax(x) for x in pred])

# Countplot Prediction
plt.figure(figsize = (7,7))
sns.countplot(pred)
plt.title(&amp;#x27;Countplot of Predictions&amp;#x27;)
plt.show()
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;span class=&quot;gatsby-resp-image-wrapper&quot; style=&quot;position:relative;display:block;margin-left:auto;margin-right:auto;max-width:451px&quot;&gt;
      &lt;a class=&quot;gatsby-resp-image-link&quot; href=&quot;/static/331531bb65cc6908f464a6e2e9da132e/38070/output_30_0.png&quot; style=&quot;display:block&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;
    &lt;span class=&quot;gatsby-resp-image-background-image&quot; style=&quot;padding-bottom:94.9367088607595%;position:relative;bottom:0;left:0;background-image:url(&amp;#x27;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAATCAYAAACQjC21AAAACXBIWXMAAAsSAAALEgHS3X78AAAD10lEQVQ4y62Ta0yTZxiG3wnq4lTAP8KSRV1iMv4s2yKJLNkhTA5CoQUMjE4UdbGwEEB0HLYsYGlhC4eICqKlFCi0wEI4CAj0a3WTmhFhBzqzgG1tRmCljQu0/Vpo+3338nVucT8Wd+BJnvfnlevO/bzEYrEQAOSHOUOI6eH8i78A4Uvc/mzZq9FQLxUUFLyek5NzuLCw8LX8/Pw3RCLRIZFIFNXf379veXl578LCQoTRaAw3m80RBoNhDwcL4oB2u73Vs+HF6sNv3GumGS+9vu6ladrndDgYh8PBuly0n6Zpv8vlYpxOJ+um3T6Px+N1u93crnvW12G32yfJ0ODANkLIdufqr0oAsMqFfnt3Lv7lMNzj9bhvE9uaO5gzNC8uK2xuwNqW7V3q/JCdtc+zi4vz7Nr9Gdb1eJWlnRssyzKs3+9n4QVrMT1iFe3t7OqqjfX5nL5l2gOjza4lGY2TwedtIMekqnZBwzhW2k76jK1ZeLcrFS03Psb3MbEYvzwMdeO9gIrX4cHijVno+yh8UiHGV3cqob+b5+fduovkIY2ORGWXc5G38S92dvJqx2ANAIWIV78Pmbwcc0d5GKkbgOrSNAavNUIjV2Cp+TvoVZOQfFEPSnMBWk2mnz+hh2CY0hHRiDUoukxJ0qTqtuTaMc7QyxnGqTN/ByYkYaR+AH1Ns7hSlIuuigqsyB9gSjUBcU0ttFQpbms/8Kf8AXynoGErISRYIFZ2JP9pmIU41V+BvVdncb2sCL3SalhlhqeAJdBRQn/K+BT4HDAy4cTzPwEktaqrnfcMYEtJIXokUlhlP/49cFf4voj0obVAZN7Tkf8r8EB04g5CyA7BRWVnct2t/2+oBrZEnZORNMkmGR4+XcGdTVCquOuZpfwjYEzx1a0IlNK9OaXkjq4EHbk0RdIkKsWmRE5v1ARzd8iv7FA+KWXD2JrFxKoyGJm8jJlLSGJG6gaY3iszTEtJAdMjkTBWmYGZ6p5gxDW1DEWVMDpK6A0AhyiKZHfOBb1ddZOkV/cEWrYpcmCSCwM/pbWtHIajSRitH0Rf07e4XlqIXqkUttYH0KsnUfV5HXTaUtzRCcGfvAfBTe3X5Pw0tpydB+F9KmtIrB54bGk5Pm9ozjC9155ubr52wTgdm2Dur+kxKxv05svFH5k6KipNj5rum7WKYdNnYql5bLTYNDaaYUwaoqy8L0e7yZN5bv+hmJ0HjwjDGk69ueuc4NXQ/SmvhIlyBTtPHXg57ExyTsixxLOh/LeiQ7Li4kPyE86EFZ3I2x0TGx+alxezOzMjMuxgZvaeyOOnX/gNDjTpSlOVscEAAAAASUVORK5CYII=&amp;#x27;);background-size:cover;display:block&quot;&gt;&lt;/span&gt;
  &lt;img class=&quot;gatsby-resp-image-image&quot; alt=&quot;png&quot; title=&quot;png&quot; src=&quot;/static/331531bb65cc6908f464a6e2e9da132e/38070/output_30_0.png&quot; srcSet=&quot;/static/331531bb65cc6908f464a6e2e9da132e/c26ae/output_30_0.png 158w,/static/331531bb65cc6908f464a6e2e9da132e/6bdcf/output_30_0.png 315w,/static/331531bb65cc6908f464a6e2e9da132e/38070/output_30_0.png 451w&quot; sizes=&quot;(max-width: 451px) 100vw, 451px&quot; style=&quot;width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0&quot; loading=&quot;lazy&quot; decoding=&quot;async&quot;/&gt;
  &lt;/a&gt;
    &lt;/span&gt;&lt;/p&gt;&lt;h2 id=&quot;lets-check-some-of-our-prediction&quot;&gt;Let&amp;#x27;s check some of our prediction&lt;/h2&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;fig, axes = plt.subplots(ncols=10, nrows=5, figsize = (13, 7))
init = 0
for i in range(5):
    j = 0
    for k in range(2):
        ind = random.choices(test.index, k = 5)
        init += 1
        while j &amp;lt; len(ind):
            axes[i, k*5 + j].imshow(X_test[ind[j]][:,:,0], cmap=plt.cm.binary)
            axes[i, k*5 + j].set_title(f&amp;#x27;Prediction : {pred[ind][j]}&amp;#x27;, fontsize = 11)
            axes[i, k*5 + j].axis(&amp;#x27;off&amp;#x27;)
            j += 1
        j = 0
fig.tight_layout()
plt.show()
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;span class=&quot;gatsby-resp-image-wrapper&quot; style=&quot;position:relative;display:block;margin-left:auto;margin-right:auto;max-width:630px&quot;&gt;
      &lt;a class=&quot;gatsby-resp-image-link&quot; href=&quot;/static/93510023de4a8aa0b9df3c023f51c209/6295b/output_32_0.png&quot; style=&quot;display:block&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;
    &lt;span class=&quot;gatsby-resp-image-background-image&quot; style=&quot;padding-bottom:53.16455696202532%;position:relative;bottom:0;left:0;background-image:url(&amp;#x27;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAALCAYAAAB/Ca1DAAAACXBIWXMAAAsSAAALEgHS3X78AAADS0lEQVQozxXQa2/bVACA4fwy2KiGJsbaISaE6MQqxIBxGWnG2CQkVIkx2iplg03a1jGE1rJVCNJQGtpG6mLnNDSJ6zRu6pwcH9ttbMeXOL4kduK0/mDUH/BKj94YwzAJlmUv5fP5b3mef4ckye9ardZ5lmV/UBTlAsMwdziOGy0Wi3ckSRqjKGpa1/VzEMIZy7LOAQC+z+fzF3men+I4bjy2u7vLHh4ePqcoymi320/X19d7R0dHSUEQwn6//3Oj0QiiKLqLEAp833+wvb09HA6HSdM0Q9/3kyRJ9jDGT3Vd1x3H+TMmSdJlURTP7+zsXBUE4Wy9Xv/Cdd1TLMtO1mq1EYRQ3DCM0yzLxm3bHuE4LgEhfJXn+QTG+BWapr90HOd1jPFnhmG8GctkMnvtdnthdXVV9X3/CYTQ0XV9liTJwPO8e8vLy76u63MEQXi2bf+UyWT6ruvOEAQxlGV5dmtry2k2m0+y2WyrXC7/EatWqx9gjMd4nv9c1/U3RFGMF4vF1xqNxldBEJypVCrXIYQjsixfxxifoSjqRq/XO8UwzNee550+EVuWdZam6WuCIIzFarVaNQzDhVarpRiGMV+tVi1N02YAAH3Hce6urKz0bNtOptPpbqfTucdxnBcEwXQulxuYpjmzublpMQzzGCEkQwiXTh7eNE3zsqIot13Xfc+yrGlN0y7QND3H8/zbGxsbSUmS3qJpOuk4zkVBEOb6/f4oRVE/mqY5CgCYhRC+KwjC7U6nMxEDAPyl6/o0wzAvOY6bKhQKBcMwEgCAncFgcHN/f7/s+/4NAEA5CIJbEELKdd1JhBCtKEqiVCr95zjO1OLi4st0Op2MEQTxSFXVW6IovlAUZbJSqaR6vd6VQqGwqmna1Xq9vhIEwccAgL/DMPwUY/yPqqofIoQyURRdIUkyJctyfGlp6QVJkt/EcrncrKIo1/L5/CNN0z4hSfLXg4ODSzzPL6iqOpFOp581m833CYJ45nneBEJosdvtjgMAfk+lUuMIod8ajcZHe3t78xjjeCyVSjW73e5yqVQaCILwvFwuR2EY3hdFMTo+Pp7PZrORZVkPGYaJTNP8RZKkyLbt+2traxHG+AFFUVG73T7p+vV6/d//Ac59rqkeFQn6AAAAAElFTkSuQmCC&amp;#x27;);background-size:cover;display:block&quot;&gt;&lt;/span&gt;
  &lt;img class=&quot;gatsby-resp-image-image&quot; alt=&quot;png&quot; title=&quot;png&quot; src=&quot;/static/93510023de4a8aa0b9df3c023f51c209/f058b/output_32_0.png&quot; srcSet=&quot;/static/93510023de4a8aa0b9df3c023f51c209/c26ae/output_32_0.png 158w,/static/93510023de4a8aa0b9df3c023f51c209/6bdcf/output_32_0.png 315w,/static/93510023de4a8aa0b9df3c023f51c209/f058b/output_32_0.png 630w,/static/93510023de4a8aa0b9df3c023f51c209/6295b/output_32_0.png 919w&quot; sizes=&quot;(max-width: 630px) 100vw, 630px&quot; style=&quot;width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0&quot; loading=&quot;lazy&quot; decoding=&quot;async&quot;/&gt;
  &lt;/a&gt;
    &lt;/span&gt;&lt;/p&gt;&lt;p&gt;looks like our models really doing good for predicting test data&lt;/p&gt;&lt;h2 id=&quot;conclusions&quot;&gt;Conclusions&lt;/h2&gt;&lt;p&gt;Based on our CV scores it&amp;#x27;s lower than &lt;a href=&quot;https://www.kaggle.com/cdeotte&quot;&gt;Chris Deotte&lt;/a&gt; kernel &lt;a href=&quot;https://www.kaggle.com/cdeotte/25-million-images-0-99757-mnist&quot;&gt;here&lt;/a&gt; with 0.99757 on validation accuracy on the same model architecture. So here&amp;#x27;s the point:&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;GPU do better job in this task [Expected] while TPU give you lower accuracy but faster since we&amp;#x27;d train 15 CNNs model on 45 epochs in less than one hour.&lt;/p&gt;&lt;/blockquote&gt;&lt;h2 id=&quot;references&quot;&gt;References&lt;/h2&gt;&lt;ol&gt;&lt;li&gt;25 Million Images! [0.99757] MNIST &lt;a href=&quot;https://www.kaggle.com/cdeotte/25-million-images-0-99757-mnist&quot;&gt;Link&lt;/a&gt;&lt;/li&gt;&lt;li&gt;Introduction to CNN Keras - 0.997 (top 6%) &lt;a href=&quot;https://www.kaggle.com/yassineghouzam/introduction-to-cnn-keras-0-997-top-6&quot;&gt;Link&lt;/a&gt;&lt;/li&gt;&lt;/ol&gt;</content:encoded></item><item><title><![CDATA[Machine Translation English to Japanese with Seq2Seq & Tensorflow]]></title><description><![CDATA[Hello guys, lately i've been studying about machine translation and give it a try. Most of code in this notebook is based on tensorflow‚Ä¶]]></description><link>https://Hyuto.github.io/machine-translation-en-jp-seq2seq-tf/</link><guid isPermaLink="false">https://Hyuto.github.io/machine-translation-en-jp-seq2seq-tf/</guid><pubDate>Wed, 19 Aug 2020 01:51:25 GMT</pubDate><content:encoded>&lt;p&gt;Hello guys, lately i&amp;#x27;ve been studying about machine translation and give it a try.&lt;/p&gt;&lt;p&gt;Most of code in this notebook is based on tensorflow tutorial on their website
&lt;a href=&quot;https://www.tensorflow.org/addons/tutorials/networks_seq2seq_nmt&quot;&gt;TensorFlow Addons Networks : Sequence-to-Sequence NMT with Attention Mechanism&lt;/a&gt;.&lt;/p&gt;&lt;p&gt;This notebook is basically my notebook run on &lt;a href=&quot;https://www.kaggle.com/&quot;&gt;kaggle&lt;/a&gt; so if you want to try
and run the code with same environment as mine go to link bellow.&lt;/p&gt;&lt;p&gt;Kaggle Notebook : &lt;a href=&quot;https://www.kaggle.com/wahyusetianto/machine-translation-en-jp-seq2seq-tf&quot;&gt;Machine Translation EN-JP Seq2Seq Tensorflow&lt;/a&gt;&lt;/p&gt;&lt;p&gt;P.S. Don&amp;#x27;t forget to &lt;em&gt;upvote&lt;/em&gt; if you like it üòä.&lt;/p&gt;&lt;h1 id=&quot;english---japanese-machine-translation&quot;&gt;English - Japanese Machine Translation&lt;/h1&gt;&lt;p&gt;So in this notebook we&amp;#x27;re going to build English to Japanese machine translation, Japanese text contains lots of unique words because they have 3 type of it:&lt;/p&gt;&lt;ol&gt;&lt;li&gt;Kanji&lt;/li&gt;&lt;li&gt;Katakana&lt;/li&gt;&lt;li&gt;Hiragana&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;that&amp;#x27;s the interesting part of it and so it&amp;#x27;ll be little complicated to process. So let&amp;#x27;s get started.&lt;/p&gt;&lt;h2 id=&quot;install-some-tools&quot;&gt;Install some tools&lt;/h2&gt;&lt;ol&gt;&lt;li&gt;Sacreblue for calculate BLEU score&lt;/li&gt;&lt;li&gt;Googletrans =&amp;gt; Google Translate for testing some sentences later&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;Note : You can use NLTK for calculating BLEU score &lt;a href=&quot;https://www.nltk.org/_modules/nltk/translate/bleu_score.html&quot;&gt;documentation&lt;/a&gt;&lt;/p&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;!pip -q install sacrebleu
!pip -q install googletrans
!pip -q install tensorflow-addons --upgrade
&lt;/code&gt;&lt;/pre&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;import random, re, string, itertools, timeit, sacrebleu
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from tqdm.notebook import tqdm
from IPython.display import display, clear_output
from sklearn.model_selection import train_test_split

# Tensorflow &amp;amp; Keras
import tensorflow as tf
import tensorflow_addons as tfa
import tensorflow.keras.backend as K
from tensorflow.keras.layers import Input, Dense, LSTM, LSTMCell
from tensorflow.keras.layers import Embedding, Bidirectional
from tensorflow.keras.models import Model
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.losses import SparseCategoricalCrossentropy
from tensorflow.keras.optimizers import Adam

# Japanese Word Tokenizer
from janome.tokenizer import Tokenizer as janome_tokenizer

plt.style.use(&amp;#x27;seaborn-pastel&amp;#x27;)
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&quot;dataset&quot;&gt;Dataset&lt;/h2&gt;&lt;p&gt;Here we use 55463 en-jp corpus from &lt;a href=&quot;http://www.manythings.org/bilingual/&quot;&gt;ManyThings.org Bilingual Sentence Pairs&lt;/a&gt;&lt;/p&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;# Download Data &amp;amp; Unzip Data
!wget http://www.manythings.org/anki/jpn-eng.zip
!unzip jpn-eng.zip
&lt;/code&gt;&lt;/pre&gt;&lt;pre&gt;&lt;code&gt;--2020-08-19 01:52:31--  http://www.manythings.org/anki/jpn-eng.zip
Resolving www.manythings.org (www.manythings.org)... 104.24.109.196, 104.24.108.196, 172.67.173.198, ...
Connecting to www.manythings.org (www.manythings.org)|104.24.109.196|:80... connected.
HTTP request sent, awaiting response... 200 OK
Length: 2303148 (2.2M) [application/zip]
Saving to: ‚Äòjpn-eng.zip‚Äô

jpn-eng.zip         100%[===================&amp;gt;]   2.20M  9.70MB/s    in 0.2s

2020-08-19 01:52:32 (9.70 MB/s) - ‚Äòjpn-eng.zip‚Äô saved [2303148/2303148]

Archive:  jpn-eng.zip
  inflating: jpn.txt
  inflating: _about.txt
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Load data to memory.&lt;/p&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;data = []

f1 = open(&amp;#x27;./jpn.txt&amp;#x27;, &amp;#x27;r&amp;#x27;)
data += [x.rstrip().lower().split(&amp;#x27;\t&amp;#x27;)[:2] for x in tqdm(f1.readlines())]
f1.close()

print(f&amp;#x27;Loaded {len(data)} Sentences&amp;#x27;)
&lt;/code&gt;&lt;/pre&gt;&lt;pre&gt;&lt;code&gt;Loaded 53594 Sentences
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&quot;text-preprocessing&quot;&gt;Text Preprocessing&lt;/h2&gt;&lt;h3 id=&quot;handling-misspell-words--clearing-punctuation&quot;&gt;Handling misspell words &amp;amp; Clearing Punctuation&lt;/h3&gt;&lt;p&gt;we&amp;#x27;re gonna change the misspell words in english sentences and clearing punctuation from text.&lt;/p&gt;&lt;p&gt;&amp;quot;aren&amp;#x27;t my english bad?&amp;quot; -&amp;gt; &amp;quot;are not my english bad&amp;quot;&lt;/p&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;mispell_dict = {
    &amp;quot;aren&amp;#x27;t&amp;quot; : &amp;quot;are not&amp;quot;,
    &amp;quot;can&amp;#x27;t&amp;quot; : &amp;quot;cannot&amp;quot;,
    &amp;quot;couldn&amp;#x27;t&amp;quot; : &amp;quot;could not&amp;quot;,
    &amp;quot;didn&amp;#x27;t&amp;quot; : &amp;quot;did not&amp;quot;,
    &amp;quot;doesn&amp;#x27;t&amp;quot; : &amp;quot;does not&amp;quot;,
    &amp;quot;don&amp;#x27;t&amp;quot; : &amp;quot;do not&amp;quot;,
    &amp;quot;hadn&amp;#x27;t&amp;quot; : &amp;quot;had not&amp;quot;,
    &amp;quot;hasn&amp;#x27;t&amp;quot; : &amp;quot;has not&amp;quot;,
    &amp;quot;haven&amp;#x27;t&amp;quot; : &amp;quot;have not&amp;quot;,
    &amp;quot;he&amp;#x27;d&amp;quot; : &amp;quot;he would&amp;quot;,
    &amp;quot;he&amp;#x27;ll&amp;quot; : &amp;quot;he will&amp;quot;,
    &amp;quot;he&amp;#x27;s&amp;quot; : &amp;quot;he is&amp;quot;,
    &amp;quot;i&amp;#x27;d&amp;quot; : &amp;quot;i would&amp;quot;,
    &amp;quot;i&amp;#x27;d&amp;quot; : &amp;quot;i had&amp;quot;,
    &amp;quot;i&amp;#x27;ll&amp;quot; : &amp;quot;i will&amp;quot;,
    &amp;quot;i&amp;#x27;m&amp;quot; : &amp;quot;i am&amp;quot;,
    &amp;quot;isn&amp;#x27;t&amp;quot; : &amp;quot;is not&amp;quot;,
    &amp;quot;it&amp;#x27;s&amp;quot; : &amp;quot;it is&amp;quot;,
    &amp;quot;it&amp;#x27;ll&amp;quot;:&amp;quot;it will&amp;quot;,
    &amp;quot;i&amp;#x27;ve&amp;quot; : &amp;quot;i have&amp;quot;,
    &amp;quot;let&amp;#x27;s&amp;quot; : &amp;quot;let us&amp;quot;,
    &amp;quot;mightn&amp;#x27;t&amp;quot; : &amp;quot;might not&amp;quot;,
    &amp;quot;mustn&amp;#x27;t&amp;quot; : &amp;quot;must not&amp;quot;,
    &amp;quot;shan&amp;#x27;t&amp;quot; : &amp;quot;shall not&amp;quot;,
    &amp;quot;she&amp;#x27;d&amp;quot; : &amp;quot;she would&amp;quot;,
    &amp;quot;she&amp;#x27;ll&amp;quot; : &amp;quot;she will&amp;quot;,
    &amp;quot;she&amp;#x27;s&amp;quot; : &amp;quot;she is&amp;quot;,
    &amp;quot;shouldn&amp;#x27;t&amp;quot; : &amp;quot;should not&amp;quot;,
    &amp;quot;that&amp;#x27;s&amp;quot; : &amp;quot;that is&amp;quot;,
    &amp;quot;there&amp;#x27;s&amp;quot; : &amp;quot;there is&amp;quot;,
    &amp;quot;they&amp;#x27;d&amp;quot; : &amp;quot;they would&amp;quot;,
    &amp;quot;they&amp;#x27;ll&amp;quot; : &amp;quot;they will&amp;quot;,
    &amp;quot;they&amp;#x27;re&amp;quot; : &amp;quot;they are&amp;quot;,
    &amp;quot;they&amp;#x27;ve&amp;quot; : &amp;quot;they have&amp;quot;,
    &amp;quot;we&amp;#x27;d&amp;quot; : &amp;quot;we would&amp;quot;,
    &amp;quot;we&amp;#x27;re&amp;quot; : &amp;quot;we are&amp;quot;,
    &amp;quot;weren&amp;#x27;t&amp;quot; : &amp;quot;were not&amp;quot;,
    &amp;quot;we&amp;#x27;ve&amp;quot; : &amp;quot;we have&amp;quot;,
    &amp;quot;what&amp;#x27;ll&amp;quot; : &amp;quot;what will&amp;quot;,
    &amp;quot;what&amp;#x27;re&amp;quot; : &amp;quot;what are&amp;quot;,
    &amp;quot;what&amp;#x27;s&amp;quot; : &amp;quot;what is&amp;quot;,
    &amp;quot;what&amp;#x27;ve&amp;quot; : &amp;quot;what have&amp;quot;,
    &amp;quot;where&amp;#x27;s&amp;quot; : &amp;quot;where is&amp;quot;,
    &amp;quot;who&amp;#x27;d&amp;quot; : &amp;quot;who would&amp;quot;,
    &amp;quot;who&amp;#x27;ll&amp;quot; : &amp;quot;who will&amp;quot;,
    &amp;quot;who&amp;#x27;re&amp;quot; : &amp;quot;who are&amp;quot;,
    &amp;quot;who&amp;#x27;s&amp;quot; : &amp;quot;who is&amp;quot;,
    &amp;quot;who&amp;#x27;ve&amp;quot; : &amp;quot;who have&amp;quot;,
    &amp;quot;won&amp;#x27;t&amp;quot; : &amp;quot;will not&amp;quot;,
    &amp;quot;wouldn&amp;#x27;t&amp;quot; : &amp;quot;would not&amp;quot;,
    &amp;quot;you&amp;#x27;d&amp;quot; : &amp;quot;you would&amp;quot;,
    &amp;quot;you&amp;#x27;ll&amp;quot; : &amp;quot;you will&amp;quot;,
    &amp;quot;you&amp;#x27;re&amp;quot; : &amp;quot;you are&amp;quot;,
    &amp;quot;you&amp;#x27;ve&amp;quot; : &amp;quot;you have&amp;quot;,
    &amp;quot;&amp;#x27;re&amp;quot;: &amp;quot; are&amp;quot;,
    &amp;quot;wasn&amp;#x27;t&amp;quot;: &amp;quot;was not&amp;quot;,
    &amp;quot;we&amp;#x27;ll&amp;quot;:&amp;quot; will&amp;quot;,
    &amp;quot;didn&amp;#x27;t&amp;quot;: &amp;quot;did not&amp;quot;,
    &amp;quot;tryin&amp;#x27;&amp;quot;:&amp;quot;trying&amp;quot;
}

mispell_re = re.compile(&amp;#x27;(%s)&amp;#x27; % &amp;#x27;|&amp;#x27;.join(mispell_dict.keys()))

def preprocess(text) -&amp;gt; str:
    def replace(match):
        return mispell_dict[match.group(0)]

    text = mispell_re.sub(replace, text)
    return text
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Japanese words have their own punctuation like „Äêthis„Äë&lt;/p&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;# Adding Japanese Punctuation
string.punctuation += &amp;#x27;„ÄÅ„ÄÇ„Äê„Äë„Äå„Äç„Äé„Äè‚Ä¶„Éª„ÄΩÔºàÔºâ„ÄúÔºüÔºÅÔΩ°ÔºöÔΩ§ÔºõÔΩ•&amp;#x27;

CP = lambda x : x.translate(str.maketrans(&amp;#x27;&amp;#x27;, &amp;#x27;&amp;#x27;, string.punctuation))
&lt;/code&gt;&lt;/pre&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;data = [x for x in data if len(x) == 2]

eng_data = [CP(preprocess(x[0])) for x in data]
jpn_data = [CP(x[1]) for x in data]
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&quot;segmenting-japanese-sentences&quot;&gt;Segmenting Japanese Sentences&lt;/h3&gt;&lt;p&gt;Unlike english sentence we can tokenize it by splitting words with space just like this,&lt;/p&gt;&lt;pre&gt;&lt;code&gt;&amp;#x27;This is english or i think so&amp;#x27;.split()

Output:
[&amp;#x27;This&amp;#x27;, &amp;#x27;is&amp;#x27;, &amp;#x27;english&amp;#x27;, &amp;#x27;or&amp;#x27;, &amp;#x27;i&amp;#x27;, &amp;#x27;think&amp;#x27;, &amp;#x27;so&amp;#x27;]
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;but in Japanese we can&amp;#x27;t do it that way. Here we gonna use Janome Tokenizer to segmenting Japanese sentence and adding space to it so Keras Tokenizer can handle it.&lt;/p&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;# Initialize Janome Tokenizer
token_jp = janome_tokenizer()
&lt;/code&gt;&lt;/pre&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;sample_text = &amp;#x27;„Åì„Åì„ÅßÁßÅ„ÅØËã±Ë™û„ÅßË©±„Åó„Å¶„ÅÑ„Çã&amp;#x27;
&amp;#x27; &amp;#x27;.join([word for word in token_jp.tokenize(sample_text, wakati=True) \
          if word != &amp;#x27; &amp;#x27;])
&lt;/code&gt;&lt;/pre&gt;&lt;pre&gt;&lt;code&gt;&amp;#x27;„Åì„Åì „Åß ÁßÅ „ÅØ Ëã±Ë™û „Åß Ë©±„Åó „Å¶ „ÅÑ„Çã&amp;#x27;
&lt;/code&gt;&lt;/pre&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;# Apply to Japanese Sentences
jpn_data = [&amp;#x27; &amp;#x27;.join([word for word in token_jp.tokenize(x, wakati=True) \
                      if word != &amp;#x27; &amp;#x27;]) for x in tqdm(jpn_data)]
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;For evaluating our model let&amp;#x27;s split our data.&lt;/p&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;eng_train, eng_test, jpn_train, jpn_test = \
train_test_split(eng_data, jpn_data, test_size = 0.04, random_state = 42)

print(f&amp;quot;Splitting to {len(eng_train)} Train data and \
{len(eng_test)} Test data&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;&lt;pre&gt;&lt;code&gt;Splitting to 51450 Train data and 2144 Test data
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&quot;add-bos-and-eos&quot;&gt;Add BOS and EOS&lt;/h3&gt;&lt;p&gt;We put BOS &amp;quot;Begin of Sequence&amp;quot; and EOS ‚ÄúEnd of Sequence&amp;quot; to help our decoder recognize begin and end of a sequence.&lt;/p&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;eng_train = [&amp;#x27;bos &amp;#x27;+ x + &amp;#x27; eos&amp;#x27; for x in eng_train + [&amp;#x27;unk unk unk&amp;#x27;]]
jpn_train = [&amp;#x27;bos &amp;#x27;+ x + &amp;#x27; eos&amp;#x27; for x in jpn_train + [&amp;#x27;unk unk unk&amp;#x27;]]

eng_val = [&amp;#x27;bos &amp;#x27;+ x + &amp;#x27; eos&amp;#x27; for x in eng_test]
jpn_val = [&amp;#x27;bos &amp;#x27;+ x + &amp;#x27; eos&amp;#x27; for x in jpn_test]
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&quot;word-tokenizing&quot;&gt;Word Tokenizing&lt;/h2&gt;&lt;p&gt;Here we use Tokenizer API from Keras to make vocabulary and tokenizing our data&lt;/p&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;# English Tokenizer
en_tokenizer = Tokenizer(filters=&amp;#x27;&amp;#x27;)
en_tokenizer.fit_on_texts(eng_train)

# Japannese Tokenizer
jp_tokenizer = Tokenizer(filters=&amp;#x27;&amp;#x27;)
jp_tokenizer.fit_on_texts(jpn_train)
&lt;/code&gt;&lt;/pre&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;print(f&amp;#x27;English vocab size   :&amp;#x27;, len(en_tokenizer.word_index) - 3)
print(f&amp;#x27;Japanese vocab size  :&amp;#x27;, len(jp_tokenizer.word_index) - 3)
&lt;/code&gt;&lt;/pre&gt;&lt;pre&gt;&lt;code&gt;English vocab size   : 9646
Japanese vocab size  : 14403
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&quot;word-cloud&quot;&gt;Word Cloud&lt;/h2&gt;&lt;p&gt;What comes when doing NLP? It&amp;#x27;s Word Cloud. Let&amp;#x27;s do it for our vocab.&lt;/p&gt;&lt;p&gt;Font : &lt;a href=&quot;https://www.google.com/get/noto/&quot;&gt;Google Noto Fonts&lt;/a&gt; -&amp;gt; Noto Sans CJK JP&lt;/p&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;!wget https://noto-website-2.storage.googleapis.com/pkgs/NotoSansCJKjp-hinted.zip
!wget https://raw.githubusercontent.com/Hyuto/NMT-TF-Seq2seq-EN-JP/master/Japan.jpg
!wget https://raw.githubusercontent.com/Hyuto/NMT-TF-Seq2seq-EN-JP/master/English.png
!mkdir font
!unzip NotoSansCJKjp-hinted.zip -d ./font
&lt;/code&gt;&lt;/pre&gt;&lt;pre&gt;&lt;code&gt;--2020-08-19 01:53:52--  https://noto-website-2.storage.googleapis.com/pkgs/NotoSansCJKjp-hinted.zip
Resolving noto-website-2.storage.googleapis.com (noto-website-2.storage.googleapis.com)... 172.217.204.128, 2607:f8b0:400c:c15::80
Connecting to noto-website-2.storage.googleapis.com (noto-website-2.storage.googleapis.com)|172.217.204.128|:443... connected.
HTTP request sent, awaiting response... 200 OK
Length: 121096772 (115M) [application/zip]
Saving to: ‚ÄòNotoSansCJKjp-hinted.zip‚Äô

NotoSansCJKjp-hinte 100%[===================&amp;gt;] 115.49M  53.3MB/s    in 2.2s

2020-08-19 01:53:54 (53.3 MB/s) - ‚ÄòNotoSansCJKjp-hinted.zip‚Äô saved [121096772/121096772]

--2020-08-19 01:53:55--  https://raw.githubusercontent.com/Hyuto/NMT-TF-Seq2seq-EN-JP/master/Japan.jpg
Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.200.133
Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.200.133|:443... connected.
HTTP request sent, awaiting response... 200 OK
Length: 3976 (3.9K) [image/jpeg]
Saving to: ‚ÄòJapan.jpg‚Äô

Japan.jpg           100%[===================&amp;gt;]   3.88K  --.-KB/s    in 0s

2020-08-19 01:53:55 (46.7 MB/s) - ‚ÄòJapan.jpg‚Äô saved [3976/3976]

--2020-08-19 01:53:56--  https://raw.githubusercontent.com/Hyuto/NMT-TF-Seq2seq-EN-JP/master/English.png
Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.200.133
Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.200.133|:443... connected.
HTTP request sent, awaiting response... 200 OK
Length: 303684 (297K) [image/png]
Saving to: ‚ÄòEnglish.png‚Äô

English.png         100%[===================&amp;gt;] 296.57K  --.-KB/s    in 0.1s

2020-08-19 01:53:56 (2.18 MB/s) - ‚ÄòEnglish.png‚Äô saved [303684/303684]

Archive:  NotoSansCJKjp-hinted.zip
  inflating: ./font/LICENSE_OFL.txt
  inflating: ./font/NotoSansCJKjp-Black.otf
  inflating: ./font/NotoSansCJKjp-Bold.otf
  inflating: ./font/NotoSansCJKjp-DemiLight.otf
  inflating: ./font/NotoSansCJKjp-Light.otf
  inflating: ./font/NotoSansCJKjp-Medium.otf
  inflating: ./font/NotoSansCJKjp-Regular.otf
  inflating: ./font/NotoSansCJKjp-Thin.otf
  inflating: ./font/NotoSansMonoCJKjp-Bold.otf
  inflating: ./font/NotoSansMonoCJKjp-Regular.otf
  inflating: ./font/README
&lt;/code&gt;&lt;/pre&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;from wordcloud import WordCloud, ImageColorGenerator
from PIL import Image

def get_words(arr):
    keys = list(arr.keys())
    count = list(arr.values())
    return &amp;#x27; &amp;#x27;.join([x for _,x in sorted(zip(count, keys), reverse = True)][2:])

def transform(arr):
    for i in range(len(arr)):
        for j in range(len(arr[i])):
            if not any(arr[i][j]):
                arr[i][j] = np.array([225, 225, 225, 225])
    return arr

font_path = &amp;#x27;./font/NotoSansCJKjp-Light.otf&amp;#x27;


mask = &amp;#x27;./English.png&amp;#x27;
mask = np.array(Image.open(mask))
mask = transform(mask)
image_colors = ImageColorGenerator(mask)
words = get_words(en_tokenizer.word_counts).title()
wc = WordCloud(background_color=&amp;quot;white&amp;quot;, max_words=2000, random_state=42,
               width=mask.shape[1], height=mask.shape[0])
wc = wc.generate(words)
fig1, ax1 = plt.subplots(figsize=(20,15))
ax1.imshow(wc.recolor(color_func=image_colors), interpolation=&amp;#x27;bilinear&amp;#x27;)
ax1.axis(&amp;quot;off&amp;quot;)

mask = &amp;#x27;./Japan.jpg&amp;#x27;
mask = np.array(Image.open(mask))
image_colors = ImageColorGenerator(mask)
words = get_words(jp_tokenizer.word_counts).title()
wc = WordCloud(collocations=False, background_color=&amp;quot;white&amp;quot;, mode=&amp;quot;RGBA&amp;quot;,
               max_words=6000, font_path=font_path, contour_width=1,
               scale=5, max_font_size = 50, relative_scaling=0.5,
               random_state=42, width=mask.shape[1], height=mask.shape[0])
wc = wc.generate(words)
fig2, ax2 = plt.subplots(figsize=(20,15))
ax2.imshow(wc.recolor(color_func=image_colors), interpolation=&amp;#x27;bilinear&amp;#x27;)
ax2.axis(&amp;quot;off&amp;quot;)

fig1.savefig(&amp;#x27;WC_English.png&amp;#x27;)
fig2.savefig(&amp;#x27;WC_Japanese.png&amp;#x27;)
plt.close(fig1)
plt.close(fig2)

!rm -rf ./font
&lt;/code&gt;&lt;/pre&gt;&lt;div class=&quot;tabbed&quot;&gt;
  &lt;span class=&quot;gatsby-resp-image-wrapper&quot; style=&quot;position:relative;display:block;margin-left:auto;margin-right:auto;max-width:630px&quot;&gt;
      &lt;a class=&quot;gatsby-resp-image-link&quot; href=&quot;/static/925d8b9db09014ceb7735de6722223fc/07a9c/WC_English.png&quot; style=&quot;display:block&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;
    &lt;span class=&quot;gatsby-resp-image-background-image&quot; style=&quot;padding-bottom:75.31645569620254%;position:relative;bottom:0;left:0;background-image:url(&amp;#x27;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAPCAYAAADkmO9VAAAACXBIWXMAAAsSAAALEgHS3X78AAACg0lEQVQ4y3WUe1PaUBDF/f5fojPtdOpM31qtDxQReQfQJAQSEhIICAQIT5WQhF8HrA62sH/cvbsze2bvPbtnj7+2XC75z7akdtlL/d5msFj4zD2PMAzx/YCQJU+TGaNml8Dz8D0P7/GR+fQB31sQLBYEfvBav/J7q+MlMRpPqVlNalYL+75Lr9Ul8/WcXCSNKWu0iiWMeA5XNbAydyjnSSYd5xVsDbjZYdsZYDXuKWsWomYhRFLkLrPkErfoQonCp2MK+7+pZ0TUsySp71fULYcw3NFhtzek1uhyq1hkZZPEWZqb4wSxwziVwyiZ9weopzd0FAMzXyZ/LWK2x+vv2QroDkbUzBaKZiOW60iKxfX+KZfvftLNSpg3ecq/rhjIGj3FQDmK0Tdbb5+8Ceg4LpJUparbFAsVBNEgmy2TjwrYxQpWPE/16IpZUWYmV3goyPTkCsFuUmbcNx3seoe61SZ9kSN+lECNCXRiWTo5kV6pSldSsW417EKZe6mynoqtgEN3REfRGZhNnHYfJZpDu0jzpFvoHw+QTxKMW11aJZ22VqcqlKnlxN1j8+AOcSSVodFgvGI8r9BMFQlVHf3DD5InWe4EjXHbwRCr2GqdiqjhBxuksAH4NJkyUHT6moXbaNOIC6T3zximb+l8OyEVKRCLyYjXRUoFDUPSMXSbxS6WZ30XRzMZmk06RpPI5yhnX2IYgkI9mka+SHOXlJHzJWqqQdOyMavmesO2Ak6cAbZSxVAMZMXk/EYikdcQFYuWM+Dh6ZHpbMZoPGYynTCdTXHd4RZSnqO1X+2vt1gwn88JA58g8J8LlkvCIFjf1w0s3wrKG3F4FZblboX5V402md1k+Q9clGHGV8oF9gAAAABJRU5ErkJggg==&amp;#x27;);background-size:cover;display:block&quot;&gt;&lt;/span&gt;
  &lt;img class=&quot;gatsby-resp-image-image&quot; alt=&quot;English WC&quot; title=&quot;English WC&quot; src=&quot;/static/925d8b9db09014ceb7735de6722223fc/f058b/WC_English.png&quot; srcSet=&quot;/static/925d8b9db09014ceb7735de6722223fc/c26ae/WC_English.png 158w,/static/925d8b9db09014ceb7735de6722223fc/6bdcf/WC_English.png 315w,/static/925d8b9db09014ceb7735de6722223fc/f058b/WC_English.png 630w,/static/925d8b9db09014ceb7735de6722223fc/40601/WC_English.png 945w,/static/925d8b9db09014ceb7735de6722223fc/78612/WC_English.png 1260w,/static/925d8b9db09014ceb7735de6722223fc/07a9c/WC_English.png 1440w&quot; sizes=&quot;(max-width: 630px) 100vw, 630px&quot; style=&quot;width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0&quot; loading=&quot;lazy&quot; decoding=&quot;async&quot;/&gt;
  &lt;/a&gt;
    &lt;/span&gt;
  &lt;span class=&quot;gatsby-resp-image-wrapper&quot; style=&quot;position:relative;display:block;margin-left:auto;margin-right:auto;max-width:630px&quot;&gt;
      &lt;a class=&quot;gatsby-resp-image-link&quot; href=&quot;/static/97526e703f2d47eceebc07ee03968a2e/07a9c/WC_Japanese.png&quot; style=&quot;display:block&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;
    &lt;span class=&quot;gatsby-resp-image-background-image&quot; style=&quot;padding-bottom:75.31645569620254%;position:relative;bottom:0;left:0;background-image:url(&amp;#x27;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAPCAYAAADkmO9VAAAACXBIWXMAAAsSAAALEgHS3X78AAAA70lEQVQ4y62UDQuCMBiE/f+/LaikKCvsO/qiaE6duu1iA0Pi3VzQCzIQ9+yOuxmBGK31Z22f7nvfRKFAEk4cEPlgXahL8bfqfoVKkRtd9p3AdoPMS4jzHeX2ZNcmy0m1/ZalRMM4sukKbDjDazBFNllCHK+QpQi33H6gRIVicwSLE2TxwoL5fA02mqO6Pcmg/MC6QZHuweIFeLIBn6UWzMYJ6sfrd4UmDHG5W5Cxa5QZYJEeoKqabIQT+FHZSAswMJ6ska92NhiqTsEpa6lsOPWTQRals6+9ln1do8ofrtBz9XTI1etQSWDfTyLCn+cNu5Sfjc744QQAAAAASUVORK5CYII=&amp;#x27;);background-size:cover;display:block&quot;&gt;&lt;/span&gt;
  &lt;img class=&quot;gatsby-resp-image-image&quot; alt=&quot;Japanese WC&quot; title=&quot;Japanese WC&quot; src=&quot;/static/97526e703f2d47eceebc07ee03968a2e/f058b/WC_Japanese.png&quot; srcSet=&quot;/static/97526e703f2d47eceebc07ee03968a2e/c26ae/WC_Japanese.png 158w,/static/97526e703f2d47eceebc07ee03968a2e/6bdcf/WC_Japanese.png 315w,/static/97526e703f2d47eceebc07ee03968a2e/f058b/WC_Japanese.png 630w,/static/97526e703f2d47eceebc07ee03968a2e/40601/WC_Japanese.png 945w,/static/97526e703f2d47eceebc07ee03968a2e/78612/WC_Japanese.png 1260w,/static/97526e703f2d47eceebc07ee03968a2e/07a9c/WC_Japanese.png 1440w&quot; sizes=&quot;(max-width: 630px) 100vw, 630px&quot; style=&quot;width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0&quot; loading=&quot;lazy&quot; decoding=&quot;async&quot;/&gt;
  &lt;/a&gt;
    &lt;/span&gt;&lt;/div&gt;&lt;p&gt;now let&amp;#x27;s transform our train sentences to sequences.&lt;/p&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;def Sequences(texts, tokenizer):
    res = []
    for text in texts:
        seq = []
        for w in text.split():
            try:
                seq.append(tokenizer.word_index[w])
            except:
                seq.append(tokenizer.word_index[&amp;#x27;unk&amp;#x27;])
        res.append(seq)
    return res
&lt;/code&gt;&lt;/pre&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;# Transform Sentences to Sequences
data_en = en_tokenizer.texts_to_sequences(eng_train)
data_jp = jp_tokenizer.texts_to_sequences(jpn_train)

val_en = Sequences(eng_val, en_tokenizer)
val_jp = Sequences(jpn_val, jp_tokenizer)
&lt;/code&gt;&lt;/pre&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;plt.figure(figsize = (8,8))
sns.distplot([len(x) for x in data_en], label=&amp;#x27;English&amp;#x27;)
sns.distplot([len(x) for x in data_jp], label=&amp;#x27;Japanese&amp;#x27;)
plt.title(&amp;#x27;Distribution of Sentences Length&amp;#x27;)
plt.legend()
plt.show()
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;span class=&quot;gatsby-resp-image-wrapper&quot; style=&quot;position:relative;display:block;margin-left:auto;margin-right:auto;max-width:490px&quot;&gt;
      &lt;a class=&quot;gatsby-resp-image-link&quot; href=&quot;/static/8ce63a24a5b8ffbe5487aa1074fa8a20/41d3c/output_32_0.png&quot; style=&quot;display:block&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;
    &lt;span class=&quot;gatsby-resp-image-background-image&quot; style=&quot;padding-bottom:98.10126582278481%;position:relative;bottom:0;left:0;background-image:url(&amp;#x27;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAUCAYAAACNiR0NAAAACXBIWXMAAAsSAAALEgHS3X78AAACwklEQVQ4y6WUW08TQRTH91UTL/XViES8pRD7RELDEwnhkY/gix/AJx8MGm+BGCWR1jbGxAh4ibEQiAoJIBcxxkBSwYhEpCz0Srud3XZ3u213ZnbnmG5boSGGFs/m7Ewyk9/+/2d2DudwODiv13PY5XKf8HoeH3e7+mwDg4PHOjs765ubmx1Op7Opo6OjoaWlpam1tdXe3t5+zul0Nra1tZ33+XxHHz3os3lcHtvAQL/N6/Ue4QCAEwThpWEYlFKKKKWSYRgSxiSt67qSz+dVjLFcnu8aZdMwJCWHU/EkQps8ryOE5iygLMujAADEBCAG1BR5E4OkpCEWjQJC6DsXCAQ4NZN5C8BAp0DylJkAzGTMyv3CBAYmAFBLECH+ssLhAjBPgKZyjBUWC+/ibG8wK4sbrIcxy5eu698sIEKiZVnOM4q0/YHFdfZ3rABGIhFOVTO+wjejCtCwvAOsNiqAsViMU0rAtSTQVaEMZFVD/2n5N2J0KWYVu8JWzcB0unAoACsJRr+GypwDKuR5vmQZwL+N6VxY+3+FqXTaUriU0OloKMpyJtl9grUDkwhZNVyVsnQkFGYyze0AoUZgMBjkFFW1LP9UEB2J8mxdUQ6uMJtROVVVhgoXblEU6LvtDeYXJQAwSz/3/tA9liVJHKUmwLwYouOJDfZJiANh1ddxD1BVlCFsmDAvBsmEsMEm4mGGaNq6stV0CMYYLQH9XGwzyOnZ3LBKMUwmeTYt8jApbMGKtg0AtfUyjPFysduIqTdbmggf4r+UycS6NpUIaFPxrcwPJaJJuqpliZ7BFGuYEo0YRCO0MjElKjUMls1lv3Acxx3qutF15vn0WMOT2ff1V3u7G+8Oui+89s/UXe7pbrxyv9f+bGHh1DXvHfvtpz32j2uf63pfuC7ect9rGl+eOt0//urs9Yc3L40tTpyZmZ89+Qdd/6OoUfpEpAAAAABJRU5ErkJggg==&amp;#x27;);background-size:cover;display:block&quot;&gt;&lt;/span&gt;
  &lt;img class=&quot;gatsby-resp-image-image&quot; alt=&quot;png&quot; title=&quot;png&quot; src=&quot;/static/8ce63a24a5b8ffbe5487aa1074fa8a20/41d3c/output_32_0.png&quot; srcSet=&quot;/static/8ce63a24a5b8ffbe5487aa1074fa8a20/c26ae/output_32_0.png 158w,/static/8ce63a24a5b8ffbe5487aa1074fa8a20/6bdcf/output_32_0.png 315w,/static/8ce63a24a5b8ffbe5487aa1074fa8a20/41d3c/output_32_0.png 490w&quot; sizes=&quot;(max-width: 490px) 100vw, 490px&quot; style=&quot;width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0&quot; loading=&quot;lazy&quot; decoding=&quot;async&quot;/&gt;
  &lt;/a&gt;
    &lt;/span&gt;&lt;/p&gt;&lt;p&gt;based on the distplot English sentences contains about 20 - 40 words while Japanese have more wider range.&lt;/p&gt;&lt;p&gt;Let&amp;#x27;s check their max length&lt;/p&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;max_en = max([len(x) for x in data_en] + [len(x) for x in val_en])
max_jp = max([len(x) for x in data_jp] + [len(x) for x in val_jp])

print(f&amp;#x27;Maximum length of English sequences is  {max_en}&amp;#x27;)
print(f&amp;#x27;Maximum length of Japanese sequences is {max_jp}&amp;#x27;)
&lt;/code&gt;&lt;/pre&gt;&lt;pre&gt;&lt;code&gt;Maximum length of English sequences is  49
Maximum length of Japanese sequences is 54
&lt;/code&gt;&lt;/pre&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;# Padding Sequences
data_en = pad_sequences(data_en, padding=&amp;#x27;post&amp;#x27;, maxlen = max_en)
data_jp = pad_sequences(data_jp, padding=&amp;#x27;post&amp;#x27;, maxlen = max_jp)

val_en = pad_sequences(val_en, padding=&amp;#x27;post&amp;#x27;, maxlen = max_en)
val_jp = pad_sequences(val_jp, padding=&amp;#x27;post&amp;#x27;, maxlen = max_jp)
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&quot;build--train-model&quot;&gt;Build &amp;amp; Train Model&lt;/h2&gt;&lt;p&gt;Now it&amp;#x27;s the time brace yourself.&lt;/p&gt;&lt;p&gt;We&amp;#x27;ll build model based on Seq2seq approaches with Attention optimization.&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;Seq2Seq is a method of encoder-decoder based machine translation that maps an input of sequence to an output of sequence with a tag and attention value. The idea is to use 2 RNN that will work together with a special token and trying to predict the next state sequence from the previous sequence.&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;&lt;span class=&quot;gatsby-resp-image-wrapper&quot; style=&quot;position:relative;display:block;margin-left:auto;margin-right:auto;max-width:630px&quot;&gt;
      &lt;a class=&quot;gatsby-resp-image-link&quot; href=&quot;/static/a5a04f4dc28fd14e1a9c5e883fda9bf5/5a190/seq2seq.png&quot; style=&quot;display:block&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;
    &lt;span class=&quot;gatsby-resp-image-background-image&quot; style=&quot;padding-bottom:50.632911392405056%;position:relative;bottom:0;left:0;background-image:url(&amp;#x27;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAKCAIAAAA7N+mxAAAACXBIWXMAAAsTAAALEwEAmpwYAAABeElEQVQoz22R2W7bMBBF/f/fE+SlD0WA5K1tHFuKbS22ZWqjJNJcRdLm0ipKUBvtATHALBd3wFmEe5xzWZZRRkMIPvi56P38fJJkjPGrdeZiQwiL8A/GGGvtbeViOCdgFDBPl+xcNtWW4PL/4nsmc85AWz6t3x6T7bf18gHWzwTHzrnF/dhNvEk73P1cv5awXm3X2SmP0vf0mM/OU59JUrRpj9uyO3S4rvqi7k8dagDMjdFEqAPE7YD3LWoGfGj6lohJ7P0k7s4wLn5lRZI28b5KdyDagTgDyaZcEYorRHZIRvkxxeOmqN7rYY+4s/bTWY2kKlejKE/Fsu9SAKIe7jg51lXsvdXmAvtByBEOSEjZDUgq9Xdto3qBXzWPweEF7F+yzXfU/tAs4ujNXrX3nlIy73jL54dZ61qIrAuYsKu1Uiou9OXqhdTz8b337gP7xSQ2xgAAKKWMETXK8xkLwTljlJJRCsqotVZrzRj7iFRKKYRQSv051W8r8je8hZRFFgAAAABJRU5ErkJggg==&amp;#x27;);background-size:cover;display:block&quot;&gt;&lt;/span&gt;
  &lt;img class=&quot;gatsby-resp-image-image&quot; alt=&quot;Seq2Seq&quot; title=&quot;Seq2Seq&quot; src=&quot;/static/a5a04f4dc28fd14e1a9c5e883fda9bf5/f058b/seq2seq.png&quot; srcSet=&quot;/static/a5a04f4dc28fd14e1a9c5e883fda9bf5/c26ae/seq2seq.png 158w,/static/a5a04f4dc28fd14e1a9c5e883fda9bf5/6bdcf/seq2seq.png 315w,/static/a5a04f4dc28fd14e1a9c5e883fda9bf5/f058b/seq2seq.png 630w,/static/a5a04f4dc28fd14e1a9c5e883fda9bf5/5a190/seq2seq.png 800w&quot; sizes=&quot;(max-width: 630px) 100vw, 630px&quot; style=&quot;width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0&quot; loading=&quot;lazy&quot; decoding=&quot;async&quot;/&gt;
  &lt;/a&gt;
    &lt;/span&gt;&lt;/p&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;# Config
epochs = 7
BATCH_SIZE = 64
BUFFER_SIZE = len(data_jp)
steps_per_epoch = BUFFER_SIZE//BATCH_SIZE
val_steps_per_epoch = len(val_jp) // BATCH_SIZE
embedding_dims = 256
rnn_units = 1024
dense_units = 1024
Dtype = tf.float32
&lt;/code&gt;&lt;/pre&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;def max_len(tensor):
    &amp;quot;&amp;quot;&amp;quot;
    Get max len in Sequences
    &amp;quot;&amp;quot;&amp;quot;
    return max( len(t) for t in tensor)
&lt;/code&gt;&lt;/pre&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;# Max Len
Tx = max_len(data_en)
Ty = max_len(data_jp)

# Vocab
input_vocab_size = len(en_tokenizer.word_index) + 1   # English
output_vocab_size = len(jp_tokenizer.word_index) + 1  # Japanese

# Changing to TF data
dataset = (tf.data.Dataset.from_tensor_slices((data_en, data_jp))
           .shuffle(BUFFER_SIZE)
           .batch(BATCH_SIZE, drop_remainder=True)
          )

val_dataset = (tf.data.Dataset.from_tensor_slices((val_en, val_jp))
               .batch(BATCH_SIZE)
              )
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Let&amp;#x27;s define our based Seq2Seq Model&lt;/p&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;# ENCODER
class EncoderNetwork(tf.keras.Model):
    def __init__(self,input_vocab_size,embedding_dims, rnn_units ):
        super().__init__()
        self.encoder_embedding = Embedding(input_dim=input_vocab_size,
                                           output_dim=embedding_dims)
        self.encoder_rnnlayer = LSTM(rnn_units,return_sequences=True,
                                     return_state=True )

# DECODER
class DecoderNetwork(tf.keras.Model):
    def __init__(self,output_vocab_size, embedding_dims, rnn_units):
        super().__init__()
        self.decoder_embedding = Embedding(input_dim=output_vocab_size,
                                           output_dim=embedding_dims)
        self.dense_layer = Dense(output_vocab_size)
        self.decoder_rnncell = LSTMCell(rnn_units)
        # Sampler
        self.sampler = tfa.seq2seq.sampler.TrainingSampler()
        # Create attention mechanism with memory = None
        self.attention_mechanism = \
            self.build_attention_mechanism(dense_units,None,BATCH_SIZE*[Tx])
        self.rnn_cell = self.build_rnn_cell(BATCH_SIZE)
        self.decoder = tfa.seq2seq.BasicDecoder(self.rnn_cell,
                                                sampler= self.sampler,
                                                output_layer = self.dense_layer
                                               )

    def build_attention_mechanism(self, units, memory, MSL):
        &amp;quot;&amp;quot;&amp;quot;
        MSL : Memory Sequence Length
        &amp;quot;&amp;quot;&amp;quot;
        #return tfa.seq2seq.LuongAttention(units, memory = memory,
        #                                  memory_sequence_length = MSL)
        return tfa.seq2seq.BahdanauAttention(units, memory = memory,
                                             memory_sequence_length = MSL)

    # wrap decodernn cell
    def build_rnn_cell(self, batch_size):
        return tfa.seq2seq.AttentionWrapper(self.decoder_rnncell,
                                            self.attention_mechanism,
                                            attention_layer_size=dense_units)

    def build_decoder_initial_state(self, batch_size, encoder_state, Dtype):
        decoder_initial_state = self.rnn_cell.get_initial_state(batch_size = batch_size,
                                                                dtype = Dtype)
        decoder_initial_state = decoder_initial_state.clone(cell_state = encoder_state)
        return decoder_initial_state
&lt;/code&gt;&lt;/pre&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;# Build Model
encoderNetwork = EncoderNetwork(input_vocab_size, embedding_dims, rnn_units)
decoderNetwork = DecoderNetwork(output_vocab_size, embedding_dims, rnn_units)

# Optimizer
optimizer = tf.keras.optimizers.Adam()
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Make custom training loop&lt;/p&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;def loss_function(y_pred, y):
    #shape of y [batch_size, ty]
    #shape of y_pred [batch_size, Ty, output_vocab_size]
    sparsecategoricalcrossentropy = SparseCategoricalCrossentropy(from_logits=True,
                                                                  reduction=&amp;#x27;none&amp;#x27;)
    loss = sparsecategoricalcrossentropy(y_true=y, y_pred=y_pred)
    mask = tf.logical_not(tf.math.equal(y,0))   #output 0 for y=0 else output 1
    mask = tf.cast(mask, dtype=loss.dtype)
    loss = mask * loss
    loss = tf.reduce_mean(loss)
    return loss

@tf.function
def train_step(input_batch, output_batch, encoder_initial_cell_state):
    # initialize loss = 0
    loss = 0
    with tf.GradientTape() as tape:
        encoder_emb_inp = encoderNetwork.encoder_embedding(input_batch)
        a, a_tx, c_tx = encoderNetwork.encoder_rnnlayer(encoder_emb_inp,
                                                        initial_state = encoder_initial_cell_state)

        # [last step activations,last memory_state] of
        # encoder passed as input to decoder Network

        # Prepare correct Decoder input &amp;amp; output sequence data
        decoder_input = output_batch[:,:-1] # ignore eos
        # compare logits with timestepped +1 version of decoder_input
        decoder_output = output_batch[:,1:] #ignore bos

        # Decoder Embeddings
        decoder_emb_inp = decoderNetwork.decoder_embedding(decoder_input)

        # Setting up decoder memory from encoder output
        # and Zero State for AttentionWrapperState
        decoderNetwork.attention_mechanism.setup_memory(a)
        decoder_initial_state = decoderNetwork.build_decoder_initial_state(BATCH_SIZE,
                                                                           encoder_state=[a_tx, c_tx],
                                                                           Dtype=tf.float32)

        # BasicDecoderOutput
        outputs, _, _ = decoderNetwork.decoder(decoder_emb_inp,initial_state=decoder_initial_state,
                                               sequence_length=BATCH_SIZE*[Ty-1])

        logits = outputs.rnn_output

        # Calculate loss
        loss = loss_function(logits, decoder_output)

    # Returns the list of all layer variables / weights.
    variables = encoderNetwork.trainable_variables + decoderNetwork.trainable_variables
    # differentiate loss wrt variables
    gradients = tape.gradient(loss, variables)

    # grads_and_vars ‚Äì List of(gradient, variable) pairs.
    grads_and_vars = zip(gradients,variables)
    optimizer.apply_gradients(grads_and_vars)
    return loss

@tf.function
def evaluate(input_batch, output_batch, encoder_initial_cell_state):
    loss = 0
    encoder_emb_inp = encoderNetwork.encoder_embedding(input_batch)
    a, a_tx, c_tx = encoderNetwork.encoder_rnnlayer(encoder_emb_inp,
                                                    initial_state =encoder_initial_cell_state)
    decoder_input = output_batch[:,:-1]
    decoder_output = output_batch[:,1:]
    decoder_emb_inp = decoderNetwork.decoder_embedding(decoder_input)
    decoderNetwork.attention_mechanism.setup_memory(a)
    decoder_initial_state = decoderNetwork.build_decoder_initial_state(BATCH_SIZE,
                                                                       encoder_state=[a_tx, c_tx],
                                                                       Dtype=tf.float32)
    outputs, _, _ = decoderNetwork.decoder(decoder_emb_inp,initial_state=decoder_initial_state,
                                           sequence_length=BATCH_SIZE*[Ty-1])
    logits = outputs.rnn_output
    loss = loss_function(logits, decoder_output)
    return loss
&lt;/code&gt;&lt;/pre&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;# RNN LSTM hidden and memory state initializer
def initialize_initial_state():
    return [tf.zeros((BATCH_SIZE, rnn_units)), tf.zeros((BATCH_SIZE, rnn_units))]
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&quot;calculating-bleu-score&quot;&gt;Calculating BLEU Score&lt;/h2&gt;&lt;blockquote&gt;&lt;p&gt;BLEU (bilingual evaluation understudy) is an algorithm for evaluating the quality of text which has been machine-translated from one natural language to another. Quality is considered to be the correspondence between a machine&amp;#x27;s output and that of a human: &amp;quot;the closer a machine translation is to a professional human translation, the better it is&amp;quot;. - Wikipedia&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;BLEU is a metric for evaluating a generated sentence to a reference sentence.
A perfect match results in a score of 1.0, whereas a perfect mismatch results in a score of 0.0.&lt;/p&gt;&lt;p&gt;So now we&amp;#x27;re going to define our translation function &amp;amp; calculate the BLEU score for test data at the end of every epoch while training. Note that test data is sentences that our tokenizer didn&amp;#x27;t train with, so there must be some words that our tokenizer didn&amp;#x27;t know.
I&amp;#x27;m currently working to fix this issue. Based on keras Tokenizer API it have &lt;code&gt;oov_token&lt;/code&gt; for handling this but i&amp;#x27;m not sure.&lt;/p&gt;&lt;p&gt;For now i&amp;#x27;m handling this by adding &lt;code&gt;unk&lt;/code&gt; in train dataset so the tokenizer can read it, and then when coming to translation if there is word that our tokenizer don&amp;#x27;t know i&amp;#x27;ll set it by index of &lt;code&gt;unk&lt;/code&gt; not very eficient but it works.&lt;/p&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;# Translate
def Translate(input_raw):
    input_raw = CP(preprocess(input_raw))
    input_lines = [&amp;#x27;bos &amp;#x27;+ input_raw + &amp;#x27;&amp;#x27;]

    input_sequences, unique = [], []
    for line in input_lines:
        temp = []
        for w in line.split(&amp;#x27; &amp;#x27;):
            try:
                temp.append(en_tokenizer.word_index[w])
            except: # Avoid Error
                unique.append(w)
                temp.append(en_tokenizer.word_index[&amp;#x27;unk&amp;#x27;])
        input_sequences.append(temp)

    input_sequences = pad_sequences(input_sequences, maxlen=Tx, padding=&amp;#x27;post&amp;#x27;)
    inp = tf.convert_to_tensor(input_sequences)
    inference_batch_size = input_sequences.shape[0]
    encoder_initial_cell_state = [tf.zeros((inference_batch_size, rnn_units)),
                                  tf.zeros((inference_batch_size, rnn_units))]
    encoder_emb_inp = encoderNetwork.encoder_embedding(inp)
    a, a_tx, c_tx = encoderNetwork.encoder_rnnlayer(encoder_emb_inp,
                                                    initial_state = encoder_initial_cell_state)

    start_tokens = tf.fill([inference_batch_size], jp_tokenizer.word_index[&amp;#x27;bos&amp;#x27;])

    end_token = jp_tokenizer.word_index[&amp;#x27;eos&amp;#x27;]

    greedy_sampler = tfa.seq2seq.GreedyEmbeddingSampler()

    decoder_input = tf.expand_dims([jp_tokenizer.word_index[&amp;#x27;bos&amp;#x27;]] * inference_batch_size,1)
    decoder_emb_inp = decoderNetwork.decoder_embedding(decoder_input)

    decoder_instance = tfa.seq2seq.BasicDecoder(cell = decoderNetwork.rnn_cell,
                                                sampler = greedy_sampler,
                                                output_layer = decoderNetwork.dense_layer)
    decoderNetwork.attention_mechanism.setup_memory(a)

    decoder_initial_state = decoderNetwork.build_decoder_initial_state(
        inference_batch_size, encoder_state=[a_tx, c_tx], Dtype=tf.float32)

    maximum_iterations = tf.round(tf.reduce_max(Tx) * 2)

    decoder_embedding_matrix = decoderNetwork.decoder_embedding.variables[0]
    (first_finished, first_inputs,first_state) = decoder_instance.initialize(
        decoder_embedding_matrix, start_tokens = start_tokens,
        end_token = end_token, initial_state = decoder_initial_state)

    inputs = first_inputs
    state = first_state
    predictions = np.empty((inference_batch_size,0), dtype = np.int32)
    for j in range(maximum_iterations):
        outputs, next_state, next_inputs, finished = decoder_instance.step(j, inputs,state)
        inputs = next_inputs
        state = next_state
        outputs = np.expand_dims(outputs.sample_id,axis = -1)
        predictions = np.append(predictions, outputs, axis = -1)

    res = &amp;#x27;&amp;#x27;
    for i in range(len(predictions)):
        line = predictions[i,:]
        seq = list(itertools.takewhile(lambda index: index !=2, line))
        res += &amp;quot; &amp;quot;.join( [jp_tokenizer.index_word[w] for w in seq])
    res = res.split()

    # Return back Unique words
    for i in range(len(res)):
        if res[i] == &amp;#x27;unk&amp;#x27; and unique != []:
            res[i] = unique.pop(0)

    return &amp;#x27; &amp;#x27;.join(res)

# Calculate BLEU
def BLEU(X, y):
    # Prediction
    pred = [Translate(w) for w in tqdm(X)]
    # Calculate BLEU
    score = sacrebleu.corpus_bleu(pred, [y]).score / 100
    return score, pred
&lt;/code&gt;&lt;/pre&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;# Custom Train Progress
class Progress:
    def __init__(self):
        self.fig = plt.figure(figsize = (8,6))
        self.ax = self.fig.add_subplot(1, 1, 1)
        self.loss, self.val_loss, self.BLEU = [], [], []
        self.epoch_loss = 0

    def get_val_loss(self):
        return [x[1] for x in self.val_loss]

    # Plot
    def dynamic_plot(self):
        self.ax.cla()
        self.ax.plot(range(len(self.loss)), self.loss, label=&amp;#x27;loss&amp;#x27;)
        if len(self.val_loss) &amp;gt;= 1:
            x = [l[0] for l in self.val_loss]
            y = [l[1] for l in self.val_loss]
            self.ax.plot(x, y, color = &amp;#x27;r&amp;#x27;, label=&amp;#x27;val_loss&amp;#x27;)
            self.ax.plot(x, self.BLEU, color = &amp;#x27;purple&amp;#x27;, label=&amp;#x27;BLEU&amp;#x27;)
        self.ax.set_ylim(0,)
        self.ax.legend(loc = 1)
        display(self.fig)

    # Train step progress
    def train_progress(self, epoch, step, steps_per_epoch, start):
        self.dynamic_plot()
        print(f&amp;#x27;Working on Epoch {epoch}&amp;#x27;)
        print(&amp;#x27;[&amp;#x27; + (&amp;#x27;=&amp;#x27; * int((step + 1) / steps_per_epoch * 60)).ljust(61, &amp;#x27; &amp;#x27;)
              + f&amp;#x27;]  {step + 1}/{steps_per_epoch} - loss : {round(self.epoch_loss / step, 4)}&amp;#x27;)
        print(f&amp;#x27;Time per Step {round(timeit.default_timer() - start, 2)} s&amp;#x27;)

    def summary(self):
        loss = np.array_split(np.array(self.loss), len(self.val_loss))
        loss = [np.mean(x) for x in loss]
        val_loss = [x[1] for x in self.val_loss]
        df = pd.DataFrame({&amp;#x27;Epochs&amp;#x27; : range(1, len(val_loss) + 1), &amp;#x27;loss&amp;#x27; : loss,
                           &amp;#x27;val loss&amp;#x27; : val_loss, &amp;#x27;BLEU&amp;#x27; : self.BLEU})

        self.dynamic_plot()
        clear_output(wait = True)
        display(df)
&lt;/code&gt;&lt;/pre&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;# Initialize Train Progress
TP = Progress()
best_prediction = []

for i in range(1, epochs + 1):

    encoder_initial_cell_state = initialize_initial_state()
    total_loss = 0.0
    # Train Loss
    TP.epoch_loss = 0

    # Train
    for (batch , (input_batch, output_batch)) in enumerate(dataset.take(steps_per_epoch)):
        start = timeit.default_timer()
        batch_loss = train_step(input_batch, output_batch, encoder_initial_cell_state)
        total_loss += batch_loss
        TP.loss.append(batch_loss.numpy())
        TP.epoch_loss += batch_loss.numpy()

        if (batch+1) % 30 == 0:
            TP.train_progress(i, batch, steps_per_epoch, start)
            clear_output(wait = True)

    # Validitate
    encoderNetwork.trainable = False  # Freeze our model layer to make sure
    decoderNetwork.trainable = False  # it didn&amp;#x27;t learn anything from val_data

    # Valid loss
    val_loss = 0
    for (batch, (input_batch, output_batch)) in enumerate(val_dataset.take(val_steps_per_epoch)):
        batch_loss = evaluate(input_batch, output_batch, encoder_initial_cell_state)
        val_loss += batch_loss.numpy()
    val_loss /= val_steps_per_epoch

    TP.val_loss.append((i * steps_per_epoch - 1, val_loss))

    # Bleu Score
    bleu_score, pred = BLEU(eng_test, jpn_test)
    TP.BLEU.append(bleu_score)

    encoderNetwork.trainable = True  # Unfreeze layer for next epoch
    decoderNetwork.trainable = True

    # Save best model
    if bleu_score == max(TP.BLEU) and val_loss == min(TP.get_val_loss()):
        best_prediction = pred
        encoderNetwork.save_weights(&amp;#x27;encoderNetwork&amp;#x27;)
        decoderNetwork.save_weights(&amp;#x27;decoderNetwork&amp;#x27;)

TP.summary()
&lt;/code&gt;&lt;/pre&gt;&lt;table&gt;&lt;thead&gt;&lt;tr&gt;&lt;th align=&quot;center&quot;&gt;Epochs&lt;/th&gt;&lt;th align=&quot;center&quot;&gt;loss&lt;/th&gt;&lt;th align=&quot;center&quot;&gt;val loss&lt;/th&gt;&lt;th align=&quot;center&quot;&gt;BLEU&lt;/th&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td align=&quot;center&quot;&gt;1&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;0.755007&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;0.617767&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;0.033657&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;center&quot;&gt;2&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;0.507803&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;0.467358&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;0.100527&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;center&quot;&gt;3&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;0.366943&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;0.404535&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;0.169416&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;center&quot;&gt;4&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;0.278531&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;0.382699&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;0.206622&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;center&quot;&gt;5&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;0.220114&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;0.379538&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;0.223136&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;center&quot;&gt;6&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;0.180076&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;0.384533&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;0.240543&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;center&quot;&gt;7&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;0.152284&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;0.390775&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;0.256055&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;&lt;span class=&quot;gatsby-resp-image-wrapper&quot; style=&quot;position:relative;display:block;margin-left:auto;margin-right:auto;max-width:490px&quot;&gt;
      &lt;a class=&quot;gatsby-resp-image-link&quot; href=&quot;/static/4968cd1de5dc4c8cb3cbadd3b3801588/41d3c/output_49_1.png&quot; style=&quot;display:block&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;
    &lt;span class=&quot;gatsby-resp-image-background-image&quot; style=&quot;padding-bottom:72.78481012658227%;position:relative;bottom:0;left:0;background-image:url(&amp;#x27;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAPCAYAAADkmO9VAAAACXBIWXMAAAsSAAALEgHS3X78AAACXklEQVQ4y52RT08TQRjG9zt48SuYgISEePBuIn6HBoMaE60XPXKRqjF69WI8ATcPRk0jCUYM8k+FQOgChZZu6F/ows7u0m53d3Zn3sdsWy0NJlIneTIzyby/93mfUTKZzFXDMN6apjlpmuZUL2KMTR0Y1lShWJrQdf29pmnXlFxu7y4X+K9FAKRdR7lYRKlcBmPsoVIsFmIuFyACJ6KwF0kpQ243wqOq7hmGEQHvK9ns7qjLKQIKgIjonO46D6NDEN0ZY3GlWMiPuFz2BPz9KNqbFYQgDEOYphlX8vvaTceXkKeA/4KeAQKBEKLlMK9lRku2hJAQRB2HPYzeDazp+ZHUgUTJbjmUktqdW/Gcw203sJBL3zqqEybWIPIW6PRYv4V2g865ncvfRi6WKjFAIlWFeL0c0o90HbYTdrmQ1FKTgTPqBqqqeicaM6rT66B3qy7ezJhY2zmBrjvwTxwgSgOyrRamE01zBWEoYJosrlQqlZiUf365WVGugZIqp8nFBn1YdymVOaGVbYusQ4saVUZulRFsRjAZwYpkcDh1GLYdV9Lp9O3213blLwTgB4BWCTG9EeCzyjGT4vi07uNLysf3HR9fVR/b+z5WM77QDjm8hvVA0TQtFgQBhBBewAPumq5XNxzuGA73bdeTns8R+Bwi8Fwv4DwUvMbJK9nSNxzBNUZe5lg6xzUOix3fUxKJxMVkMnllc2uzb2lhaWDs0dj1+dn5ge3cTv+T50+H55aXBtXsXt/YeOLGys/Fwd0ttf/ls/HhjYXpocAsXXr14vGw+u3jkFHKXp6dW7jwC/SSU1VUgwWcAAAAAElFTkSuQmCC&amp;#x27;);background-size:cover;display:block&quot;&gt;&lt;/span&gt;
  &lt;img class=&quot;gatsby-resp-image-image&quot; alt=&quot;png&quot; title=&quot;png&quot; src=&quot;/static/4968cd1de5dc4c8cb3cbadd3b3801588/41d3c/output_49_1.png&quot; srcSet=&quot;/static/4968cd1de5dc4c8cb3cbadd3b3801588/c26ae/output_49_1.png 158w,/static/4968cd1de5dc4c8cb3cbadd3b3801588/6bdcf/output_49_1.png 315w,/static/4968cd1de5dc4c8cb3cbadd3b3801588/41d3c/output_49_1.png 490w&quot; sizes=&quot;(max-width: 490px) 100vw, 490px&quot; style=&quot;width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0&quot; loading=&quot;lazy&quot; decoding=&quot;async&quot;/&gt;
  &lt;/a&gt;
    &lt;/span&gt;&lt;/p&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;# Load best weights
encoderNetwork.load_weights(&amp;#x27;encoderNetwork&amp;#x27;)
decoderNetwork.load_weights(&amp;#x27;decoderNetwork&amp;#x27;)
&lt;/code&gt;&lt;/pre&gt;&lt;pre&gt;&lt;code&gt;&amp;lt;tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7fb3601083d0&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Let&amp;#x27;s check the best prediction of our model.&lt;/p&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;for i in range(7,16):
    print(&amp;quot;English Sentence:&amp;quot;)
    print(eng_test[i])
    print(&amp;quot;\nJapanese Translation:&amp;quot;)
    print(best_prediction[i])
    print(&amp;quot;\nJapanese Reference:&amp;quot;)
    print(jpn_test[i])
    print(&amp;#x27;&amp;#x27;.ljust(60, &amp;#x27;-&amp;#x27;))
&lt;/code&gt;&lt;/pre&gt;&lt;pre&gt;&lt;code&gt;English Sentence:
in september there are just a few people here and there on the beach

Japanese Translation:
Ôºô ÊôÇ „Å´ „ÅØ „Åì„Åì „Å´ ‰ªñ „ÅÆ ‰∫∫ „Åå ‰Ωè„Çì „Åß „ÅÑ „Å™„ÅÑ ‰∫∫ „Åå „Åù„ÅÆ „Éì„Éº„ÉÅ „Åß ‰ªñ „ÅÆ ‰∫∫ÈÅî „Åå ÊÆã„Çä „ÅÆ „ÇÇ„Å® „Çí „ÅÇ„Å°„Çâ „Å´ Âª∫Ë®≠ Ë¶ã „Åü

Japanese Reference:
ÔºôÊúà „ÅÆ Êµ∑ „ÅØ ‰∫∫ „Åå „Åæ„Å∞„Çâ „Å† „Å≠
------------------------------------------------------------
English Sentence:
while you are young you should read a lot

Japanese Translation:
Ëã•„ÅÑ È†É „ÅØ Ë™≠„Åø ÁµÇ„Çè„Å£ „Åü„Çâ „ÅÑ„ÅÑ „Çà

Japanese Reference:
Ëã•„ÅÑ „ÅÜ„Å° „Å´ „Åü„Åè„Åï„Çì „ÅÆ Êú¨ „Çí Ë™≠„ÇÄ „Åπ„Åç „Å†
------------------------------------------------------------
English Sentence:
here i come

Japanese Translation:
„Åì„Åì „Å´ Êù• „Åü „ÅÆ

Japanese Reference:
„ÅÑ„Åæ Ë°å„Åç „Åæ„Åô
------------------------------------------------------------
English Sentence:
once you have decided when you will be coming let me know

Japanese Translation:
Êù•„Çã „Åã Âêõ „Å´ „ÅØ ÈÄ£Áµ° „Çí Ë®Ä„Å£ „Å¶ „Åç „Åü „Çà

Japanese Reference:
„ÅÑ„Å§ Êù•„Çã „Åã Ê±∫„Åæ„Å£ „Åü„Çâ Êïô„Åà „Å¶
------------------------------------------------------------
English Sentence:
he jumped on the train

Japanese Translation:
ÂΩº „ÅØ ÈõªËªä „Å´ Êóó „Çí È£õ„Å≥Ë∂ä„Åà „Åü

Japanese Reference:
ÂΩº „ÅØ ÈõªËªä „Å´ È£õ„Å≥‰πó„Å£ „Åü
------------------------------------------------------------
English Sentence:
he passed away yesterday

Japanese Translation:
ÂΩº „ÅØ Êò®Êó• ‰∫°„Åè„Å™„Å£ „Åü

Japanese Reference:
ÂΩº „ÅØ Êò®Êó• „Åä ‰∫°„Åè„Å™„Çä „Å´ „Å™„Çä „Åæ„Åó „Åü
------------------------------------------------------------
English Sentence:
i had no other choice

Japanese Translation:
‰ªñ „Å´ ÈÅ∏ÊäûËÇ¢ „Åå „Å™„Åã„Å£ „Åü

Japanese Reference:
‰ªñ „Å´ Êâã „Åå „Å™„Åã„Å£ „Åü „ÅÆ „Å†
------------------------------------------------------------
English Sentence:
are you good at bowling

Japanese Translation:
„Éú„Éº„É™„É≥„Ç∞ „ÅØ ÂæóÊÑè „Åß„Åô „Åã

Japanese Reference:
„Éú„Ç¶„É™„É≥„Ç∞ „ÅØ ÂæóÊÑè
------------------------------------------------------------
English Sentence:
it is strange that you do not know anything about that matter

Japanese Translation:
„Åù„Çå „Å´„Å§„ÅÑ„Å¶ ‰Ωï „ÇÇ Áü•„Çâ „Å™„ÅÑ „Åì„Å® „Åå „Å™„ÅÑ „Å®„ÅÑ„ÅÜ „Åì„Å® „ÅØ Â§â „Å†

Japanese Reference:
„ÅÇ„Å™„Åü „Åå „Åù„ÅÆ „Åì„Å® „Å´„Å§„ÅÑ„Å¶ ‰Ωï „ÇÇ Áü•„Çâ „Å™„ÅÑ „ÅÆ „ÅØ Â§â „Å†
------------------------------------------------------------
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&quot;test-with-some-raw-input&quot;&gt;Test with Some Raw Input&lt;/h2&gt;&lt;p&gt;Yeay now let&amp;#x27;s play with &lt;strong&gt;our&lt;/strong&gt; Machine Translation with some raw input. We&amp;#x27;ll cross check the prediction from MT with Google Translate API to translate it back to english and see how bad &lt;strong&gt;our&lt;/strong&gt; MT is :).&lt;/p&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;from googletrans import Translator
# Google Translate
translator = Translator()
&lt;/code&gt;&lt;/pre&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;raw_input = [&amp;#x27;i love you&amp;#x27;, &amp;#x27;i am sorry&amp;#x27;, &amp;#x27;hello&amp;#x27;, &amp;#x27;thank you&amp;#x27;,
             &amp;#x27;is there something i can help?&amp;#x27;]

for i in range(len(raw_input)):
    prediction = Translate(raw_input[i])
    print(&amp;quot;English Sentence:&amp;quot;)
    print(raw_input[i])
    print(&amp;quot;\nJapanese Translation:&amp;quot;)
    print(prediction)
    print(&amp;quot;\nEnglish Translation from prediction [GoogleTranslate]:&amp;quot;)
    print(translator.translate(prediction).text)
    print(&amp;#x27;&amp;#x27;.ljust(60, &amp;#x27;-&amp;#x27;))
&lt;/code&gt;&lt;/pre&gt;&lt;pre&gt;&lt;code&gt;English Sentence:
i love you

Japanese Translation:
ÊÑõ„Åó „Å¶„Çã „Çà

English Translation from prediction [GoogleTranslate]:
I love you
------------------------------------------------------------
English Sentence:
i am sorry

Japanese Translation:
„Åô„Åø„Åæ„Åõ„Çì

English Translation from prediction [GoogleTranslate]:
Excuse me
------------------------------------------------------------
English Sentence:
hello

Japanese Translation:
„ÇÇ„Åó„ÇÇ„Åó

English Translation from prediction [GoogleTranslate]:
Hello
------------------------------------------------------------
English Sentence:
thank you

Japanese Translation:
„ÅÇ„Çä„Åå„Å®„ÅÜ „Åî„Åñ„ÅÑ „Åæ„Åô

English Translation from prediction [GoogleTranslate]:
Thank you
------------------------------------------------------------
English Sentence:
is there something i can help?

Japanese Translation:
‰Ωï „Åã Êâã‰ºù„Åà„Çã „ÇÇ„ÅÆ „Åå „ÅÇ„Çã „ÅÆ

English Translation from prediction [GoogleTranslate]:
There is something to help
------------------------------------------------------------
&lt;/code&gt;&lt;/pre&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;import pickle

with open(&amp;#x27;en_tokenizer.pickle&amp;#x27;, &amp;#x27;wb&amp;#x27;) as handle:
    pickle.dump(en_tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)
handle.close()

with open(&amp;#x27;jp_tokenizer.pickle&amp;#x27;, &amp;#x27;wb&amp;#x27;) as handle:
    pickle.dump(jp_tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)
handle.close()
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&quot;reference&quot;&gt;Reference&lt;/h2&gt;&lt;ol&gt;&lt;li&gt;TensorFlow Addons Networks : Sequence-to-Sequence NMT with Attention Mechanism &lt;a href=&quot;https://www.tensorflow.org/addons/tutorials/networks_seq2seq_nmt&quot;&gt;Link&lt;/a&gt;&lt;/li&gt;&lt;li&gt;seq2seq (Sequence to Sequence) Model for Deep Learning with PyTorch &lt;a href=&quot;https://www.guru99.com/seq2seq-model.html&quot;&gt;Link&lt;/a&gt;&lt;/li&gt;&lt;/ol&gt;</content:encoded></item></channel></rss>